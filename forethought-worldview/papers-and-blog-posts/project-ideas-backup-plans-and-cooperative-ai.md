---
title: "Project Ideas: Backup Plans & Cooperative AI"
url: https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai
---

# Project Ideas: Backup Plans & Cooperative AI

[Lukas Finnveden](https://www.forethought.org/people/lukas-finnveden)

3rd January 2024Last update: 20th May 2025


## Introduction

In this final article in the series, I include two categories of projects (which are related, and each of which I have less to say about than the previous areas):

1. [Backup plans for misaligned AI](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#backup-plans-for-misaligned-ai): If we can't build aligned AI, and if we fail to coordinate well enough to avoid putting misaligned AI systems in positions of power, we might have some strong preferences about the dispositions of those misaligned AI systems. This section is about nudging those into somewhat better dispositions (in worlds where we can't align AI systems well enough to stay in control). A favorite direction is to [study generalization & AI personalities to find easily-influenceable properties](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#studying-generalization--ai-personalities-to-find-easily-influenceable-properties-ml).

2. [Cooperative AI](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#cooperative-ai): Difficulties with cooperation have been a big source of lost value and unnecessary risk in the past. AI offers dramatic changes in how bargaining could work. This section is about projects that could make AI (and AI-assisted humans) more likely to handle cooperation well. One of my favorite projects here is actually the same as the project I mentioned for "backup plans", just above. (There's significant overlap between the two.)


## Backup plans for misaligned AI

When humanity builds powerful AI systems, I hope that those systems will be safe and aligned. (And I'm excited about efforts to make that happen.)

But it's possible that alignment will be very difficult and that there won't be any successful coordination effort to avoid building powerful misaligned AI. If misaligned AI will be built and seize power (or at least have the option of doing so), then there are nevertheless certain types of misaligned systems that I would prefer over others. This section is about affecting _that_.

This decomposes into two questions:

- If humanity fails to align their systems and misaligned AIs seize power: What properties would we prefer for those AIs to have?

- If humanity fails to align their systems and misaligned AIs seize power: What are realistic ways in which we might have been able to affect which misaligned AI systems we get? (Despite how our methods were insufficient to make the AIs fully safe & aligned.)


The first is addressed in [What properties would we prefer misaligned AIs to have?](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#what-properties-would-we-prefer-misaligned-ais-to-have-philosophicalconceptual-forecasting) The second is addressed in [Studying generalization & AI personalities to find easily-influenceable properties](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#studying-generalization--ai-personalities-to-find-easily-influenceable-properties-ml). (If you're more skeptical about the second of these than the first, you should feel free to read that section first.)

### What properties would we prefer misaligned AIs to have? \[Philosophical/conceptual\] \[Forecasting\]

There are a few different plausible categories here. All of them could use more analysis on which directions would be good and which would be bad.

#### Making misaligned AI have better interactions with other actors

This overlaps significantly with cooperative AI. The idea is that there are certain dispositions that AIs could have that would lead them to have better interactions with other actors who are comparably powerful.

When I say "better" interactions, I mean that the interactions will leave the other actors better off (by their own lights). Especially insofar as this happens via positive-sum interactions that don't significantly disadvantage the AI system we're influencing.

Here are some examples of who these "other actors" could be:

- Humans.


  - There might be some point in time when misaligned AIs have escaped human control, and have a credible shot at taking full control, but when humanity[1](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-1) still has a fighting chance. If so, it would be great if humans and AIs could find a cooperative solution that would leave both of them better-off than conflict.


- Aliens in our universe, or distant aliens who we can only interact with acausally (e.g. via [evidential cooperation in large worlds](https://longtermrisk.org/ecl) ECL).


  - One reason to care about these interactions is that some (possibly small) fraction of aliens would have values that overlap significantly with our own values.

  - Evidential cooperation in large worlds (ECL) could provide another reason to care about the values of distant aliens, as I've written about [here](https://lukasfinnveden.substack.com/p/how-ecl-changes-the-value-of-interventions).


- Other misaligned AIs on Earth, insofar as multiple groups of misaligned AIs acquired significant power around the same time.


  - I think the case for caring about those AIs' values is weaker than the case for caring about the earlier listed types of actors. But it's possible that some of those AI systems' values could overlap with ours, or that some of them would partially care about humanity, or that ECL gives us reasons to care about their values.


So what are these "dispositions" that could lead AIs to have better interactions with other comparably powerful actors? Some candidates are:

- What type of decision theory the AIs use. For example, if the AIs use EDT rather than CDT, it might be rational for them to act more cooperatively due to ECL.

- AIs not having any spiteful preferences, i.e. preferences such that they would actively value and attempt to frustrate others' preferences.[2](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-2) (Note that sadism in humans provides some precedent for this coming about naturally.)


  - See [this post](https://www.lesswrong.com/posts/92xKPvTHDhoAiRBv9/making-ais-less-likely-to-be-spiteful#:~:text=We%20define%20spite%20as%20a,well%20as%20risks%20from%20malevolence.) for a proposed operationalization and some analysis.


- AI having diminishing marginal returns to resources rather than increasing or linear returns.[3](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-3)

- AIs having [porous values](https://nickbostrom.com/papers/porosity.pdf).


#### AIs that we may have moral or decision-theoretic reasons to empower

The main difference between this section and the above section is that the motivations in this section are one step closer to being about empowering the AIs for "their own sake" rather than for the sake of someone they interact with. Though it still includes pragmatic/decision-theoretic reasons for why it's good to satisfy certain AI systems' values.

One direction to think about is the scheme that Paul Christiano suggests in [When is unaligned AI morally valuable?](https://ai-alignment.com/sympathizing-with-ai-e11a4bf5ef6e)

- It involves simulating AI systems that are _seemingly_ in a situation similar to our own: in the process of deciding whether to hand over significant power to an alien intelligence of their own design.

- If those AI systems behave "cooperatively" in the sense of empowering an alien intelligence that itself acted "cooperatively", then perhaps there's a moral and/or decision-theoretic argument for us empowering those AIs.


  - The pragmatic decision-theoretic argument would be that _we_ might be in such a simulation and that _our_ behaving "cooperatively" would lead to _us_ being empowered.

  - The moral case would be that there is a certain symmetry between the AI's situation and our own, and so [the Golden Rule](https://en.wikipedia.org/wiki/Golden_Rule) might recommend empowering those AI systems.


- Overall, this seems incredibly complicated. But perhaps further analysis could reveal whether there is something to this idea.


Another direction is the ideas that I talk about in [ECL with AI](https://lukasfinnveden.substack.com/p/ecl-with-ai). Basically:

- ECL might give us reason to benefit and empower value systems held by distant ECL-sympathetic AIs.

- Thus, if we can make our AIs have values closer to distant ECL-sympathetic AIs and/or be more competent by the lights of distant ECL-sympathetic AIs, then we might have reason to do that.


Another direction is to think about object-level things that humans value, as well as the process that produced our values, and try to get AI systems more inclined to value similar things. I'm somewhat skeptical of this path since [human values seem complex](https://www.lesswrong.com/tag/complexity-of-value), and so I'm not sure what schemes could plausibly make AIs share a significant fraction of human values _without_ us also having the capability of making the AIs [corrigible](https://ai-alignment.com/corrigibility-3039e668638) or otherwise safe.[4](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-4) But it doesn't seem unreasonable to think about it more.

(To reiterate what I said above: I think that all of these schemes would be significantly worse than successfully building AI systems that are aligned and corrigible to human intentions.)

#### Making misaligned AI positively inclined toward us

A final way in which we might want to shape the preferences of misaligned AIs is to make them more likely to care enough about humans to give us a small utopia, instead of killing us. (Even if most of the universe gets used for the AI's own ends.)

For an AI that cares about all the resources in the universe (in an mostly impartial way), it would be extremely cheap to do this. Our solar system is a negligible fraction of all the resources in the accessible universe. And a surveillance system that prevents humans from competing with the AIs could probably be built cheaply and without interfering too much with human happiness. (For some discussion of this, see my report [AGI and lock-in](https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in) _,_ especially [section 8.2](https://docs.google.com/document/d/1mkLFhxixWdT5peJHq4rfFzq4QbHyfZtANH1nou68q88/edit#heading=h.h48tmkhka4ca).)

I think it's reasonably likely that this would happen as a result of trade with distant civilizations. Taking that into account, there are 3 broad directions, here:

- Firstly, trade will be more likely to save us if we succeed at [Making misaligned AI have better interactions with other actors](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#making-misaligned-ai-have-better-interactions-with-other-actors), as discussed above.

- Secondly, trade will be more likely to save us if it's really cheap for the AI to treat us well. Since the resource cost of treating us well is small by default, this might just mean decreasing the probability that the AI either actively wants to harm us or that it has preferences that especially interfere with ours (via e.g. caring a lot about what happens on Earth in just the next few years).

- Finally, if trade falls through, it might help for the AI to have some intrinsic concern for humans getting what they want (by their own lights).


Insofar as we want the AIs to have some intrinsic concern for us (or at least not to be actively antagonistic towards us), we can also distinguish between interventions that:

- Directly modify the AIs' dispositions and preferences.

- Intervenes on _what humanity does_ in a way that makes the AI more likely to care about us insofar as it has some sense of justice or reciprocity.


  - For example, if we successfully carry out many of the interventions suggested in the post on [sentience and rights of digital minds](https://www.forethought.org/research/project-ideas-sentience-and-rights-of-digital-minds): AIs that have absorbed a sense of justice could reasonably be more positively inclined towards us than if we had been entirely indifferent to AI welfare.


For some discussion about whether it's plausible that AIs could have some intrinsic concern for humans getting what they want (by their own lights), which addresses issues around the "complexity of human values", I recommend [this comment](https://www.lesswrong.com/posts/2NncxDQ3KBDCxiJiP/cosmopolitan-values-don-t-come-free?commentId=ofPTrG6wsq7CxuTXk) and subsequent thread.

### Studying generalization & AI personalities to find easily-influenceable properties \[ML\]

Here is a research direction that hasn't been very explored to date: Study how language models' generalization behavior / "personalities" seem to be shaped by their training data, by prompts, by different training strategies, etc. Then, use that knowledge to choose training data, prompts, and training strategies that induce the kind of properties that we want our AIs to have.

If done well, this could be highly useful for alignment. In particular: We might be able to find training set-ups which often seem to lead to corrigible behavior.

But notably, this research direction could fail to work for alignment while still being practically able to affect other properties of language models.[5](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-5) For example, maybe corrigibility is a really unnatural and hard-to-get property (perhaps for reasons suggested in item 23 of Yudkowsky's [list of lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), and formally analyzed [here](https://www.lesswrong.com/posts/WCX3EwnWAx7eyucqH/a-certain-formalization-of-corrigibility-is-vnm-incoherent#comments)). That wouldn't necessarily imply that it was similarly hard to modify the other properties discussed above (decision theories, spitefulness, desire for humans to do well by their own lights). So this research direction looks more exciting insofar as we could influence AI personalities in many different valuable ways. (Though more like 3x as exciting than 100x as exciting, unless you have particular views where "corrigibility" is either significantly less likely or less desirable than the other properties.)

**What about fine-tuning?**

A "baseline" strategy for making AIs behave as you want is to finetune them to exhibit that behavior in situations that you can easily present them with. But if this work is to be useful, it needs to generalize to strange, future situations where humans no longer have total control over their AI systems. We can't easily present AIs with situations from that same distribution, and so it's not clear whether fine-tuning will generalize that far.[6](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-6)

So while "finetune the model" seems like an excellent direction to explore, for this type of research, you'll still want to do the work of empirically evaluating when fine-tuning will and won't generalize to other settings. By varying various properties of the fine-tuning dataset, or other things, like whether you're doing supervised learning or RL.

Also, insofar as you can find models that satisfy your evaluations _without_ needing to do a lot of "local" search (like fine-tuning / gradient descent), it seems somewhat more likely that the properties you evaluated for will generalize far. Because if you make large changes in e.g. architecture or pre-training data, it's more likely that your measurements are picking up on deeper changes in the models. Whereas if you use gradient descent, it is somewhat more likely that gradient descent implements a "shallow" fix  that only applies to the sort of cases that you can test.[7](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-7)

Of course, the above argument only works insofar as you're searching for properties you could plausibly get without doing a lot of search. For example, you'd never get something as complex as "human values" without highly targeted search or design. But properties like "corrigibility", "(lack of) spitefulness", and "some desire for humans to do well by-their-own-lights" all seem like properties that could plausibly be common under some training schemes.

Ideally, this research direction would lead to a scientific understanding of training that would let us (in advance) identify & pick training processes that robustly lead to the properties that we want. But insofar as we're looking for properties that appear reasonably often "by default", one possible backup plan may be to train several models under somewhat different conditions, evaluate all of them for properties that we care about, and deploy the one that does best. (To be clear: this would be a real hail-mary effort that would always carry a large probability of failing, e.g. due to the models knowing what we were trying to evaluate them for and faking it.)

**Previous work**

An example of previous, related research is Perez et al.'s [Discovering Language Model Behaviors with Model-Written Evaluations](https://arxiv.org/abs/2212.09251).

Ways in which this work is relevant for the path-to-impact outlined here:

- The paper does not focus on "capability evaluations" (i.e. analyzing whether models are _capable_ of providing certain outputs, given the right fine-tuning or prompting). Instead, it measures language models' inclinations along dimensions they haven't been intentionally finetuned for.

- It measures how these inclinations vary depending on some high-level changes to the training process. In particular, it looks at model size and the presence vs. absence of RLHF training.

- It measures how these inclinations vary depending on some features of the prompting. In particular, it studies models' inclinations towards "sycophancy" by examining whether models' responses are sensitive to facts the user shared about themselves.

- For each property it wants to test for, it generates many questions that get at that question, thereby reducing noise and the risk of spurious results.


Further directions that could make this type of research more useful for this path-to-impact.

- Considering a greater number of training conditions. For example:


  - Testing for differences between fine-tuning via supervised learning vs. fine-tuning via RL.

  - Testing the differential impacts of different finetuning datasets (rather than just one "RLHF" setting, with more/fewer training steps).


    - Potentially using [influence functions](https://www.anthropic.com/index/influence-functions) or (more simplistically) leaving particular data points out from fine-tuning and seeing how the results change.


- Varying the context that the LLM is presented with. Is it asked a question about what's right or wrong, is it asked to advise us, or is it prompted to itself take an action? (This context can be varied both during evaluation and during training.)

- More systematic study of framing effects. Are the models' answers better predicted by the content of the questions or by the way they are presented?

- Using more precisely described scenarios so that it's easier to vary individual details and see what matters for the AIs' decisions. E.g. present actual pay-off matrices in difficult dilemmas.

- Study how various closely adjacent concepts go together or come apart by presenting dilemmas where they would recommend different actions. For example, contrast being "nice" vs. "cooperative" vs. "high-integrity", etc. What are the natural dimensions of variation within the AIs' personality?

- Making use of analysis concerning [What properties would we prefer misaligned AIs to have?](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#what-properties-would-we-prefer-misaligned-ais-to-have-philosophicalconceptual-forecasting), and targeting evaluations & training datasets to answer the most important questions.


  - For example: Designing multi-agent training & evaluation data sets that study when models may or may not develop spiteful preferences. Perhaps comparing models only trained on zero-sum games vs. models also trained on cooperative or mixed-motive games.


- Studying how far various properties generalize from the training distribution, by intentionally making the test distribution different in various ways.


(Thanks to Paul Christiano for discussion.)

### Theoretical reasoning about generalization \[ML\] \[Philosophical/conceptual\]

Rather than doing empirical ML research, you could also do theoretical reasoning about what sort of generalization properties and personality traits are more or less likely to be induced by different kinds of training.

For example, it seems a-priori plausible that spiteful preferences are more likely to arise if you (only) train AI systems on zero-sum games.

There has also been some theoretical work on what kind of decision-theoretic behavior is induced by different training algorithms, for example [Bell, Linsefors, Oesterheld & Skalse (2021)](https://proceedings.neurips.cc/paper/2021/file/b9ed18a301c9f3d183938c451fa183df-Paper.pdf) and [Oesterheld (2021)](https://link.springer.com/article/10.1007/s11229-019-02148-2).

I think we'll ultimately want empirical work to support any theoretical hypotheses, here. But theoretical work seems great for generating ideas of what's important to test.

## Cooperative AI

This is an area other people have written about.

- It's the focus of the [Cooperative AI Foundation](https://www.cooperativeai.com/foundation)

- It's a major focus area of the [Center on Long-Term Risk](https://longtermrisk.org/) (because it seems especially important for s-risk reduction).


  - You can see their research agenda on the topic [here](https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf).


- There's relevant research at the [Foundations of Cooperative AI Lab](https://www.cs.cmu.edu/~focal/index.html) at CMU.

- It's a significant motivation behind [encultured.ai](https://www.encultured.ai/).


Partly due to this, I will write about it in less detail than I've written about the other topics. But I will mention a few projects I'd be especially excited about.

The first thing to mention is that some of my favorite cooperative AI projects are variants of the just-previously mentioned topics: [Studying generalization & AI personalities to find easily-influenceable properties](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#studying-generalization--ai-personalities-to-find-easily-influenceable-properties-ml) and figuring out [What properties would we prefer misaligned AIs to have?](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#what-properties-would-we-prefer-misaligned-ais-to-have-philosophicalconceptual-forecasting) Positively influencing cooperation-relevant properties like (lack of) spitefulness seems great. I won't go over those projects again, but I think they're great cooperative AI projects, so don't be deceived by their lack of representation here.

Similarly, some of the topics under [Governance during explosive technological growth](https://www.forethought.org/research/project-ideas-governance-during-explosive-technological-growth) are also related to cooperative AI. In particular, the question of [How to handle brinkmanship/threats?](https://www.forethought.org/research/project-ideas-governance-during-explosive-technological-growth#how-to-handle-brinkmanshipthreats) is very tightly related.

Another couple of promising projects are:

### Implementing surrogate goals / safe Pareto improvements \[ML\] \[Philosophical/conceptual\] \[Governance\]

[Safe Pareto improvements](https://www.andrew.cmu.edu/user/coesterh/SPI.pdf) are an idea for how certain bargaining strategies can guarantee a (weak) Pareto-improvement for all players via preserving certain invariants about what equilibrium is selected while replacing certain outcomes with other, less-harmful outcomes. [Surrogate goals](https://s-risks.org/using-surrogate-goals-to-deflect-threats/) are a special case of this, which involves genuinely adopting a new goal in a way that will mostly not affect your behavior, but which will encourage people who want to threaten you to make threats against the surrogate goal rather than your original values. If bargaining breaks down and the threatener ends up trying to harm you, it is better that they act to thwart the surrogate goal than to harm your original values. See [here](https://longtermrisk.org/spi) for resources on surrogate goals & safe Pareto improvements.

I think there are some promising empirical projects that can be done here:

- Empirical experiments of implementing surrogate goals in contemporary language models.

- Empirical experiments of implementing surrogate goals in contemporary language models _that the models try to keep around_ during self-modification / when designing future systems.


Conceptual/theory projects:

- Better understanding of conditions where surrogate goals / safe Pareto improvements are credible. (Including credibly sticking around for a long time.) Especially when humans are still in the loop.

- What are the conditions under which classically rational agents would use safe Pareto improvements?


### AI-assisted negotiation \[ML\] \[Philosophical/conceptual\]

One use-case for AI that might be especially nice to differentially accelerate is "AI that helps with negotiation". Certainly, it would be of great value if AI could increase the frequency and speed at which different parties could come to mutually beneficial disagreements. Especially given the tricky [governance issues that might come with explosive growth](https://www.forethought.org/research/project-ideas-governance-during-explosive-technological-growth), which may need to be dealt with _quickly_.

(This is also related to [Technical proposals for aggregating preferences](https://www.forethought.org/research/project-ideas-governance-during-explosive-technological-growth#technical-proposals-for-aggregating-preferences), mentioned in that post.)

I'm honestly unsure about what kind of bottlenecks there are here, and to what degree AI could help alleviate them.

Here's one possibility. By virtue of AI being cheaper and faster than humans, perhaps negotiations that were mediated by AI systems could find mutually agreeable solutions in much more complex situations. Such as situations with a greater number of interested parties or a greater option space. (This would be compatible with humans being the ones to finally read, potentially opine on, and approve the outcome of the negotiations.)

More speculatively: Perhaps negotiations via AI could also go through more candidate solutions faster because anything an AI said would have the plausible deniability of being an error. Such that you'd lose less bargaining power if your AI signaled a willingness to consider a proposal that superficially looked bad for you.[8](https://www.forethought.org/research/project-ideas-backup-plans-and-cooperative-ai#user-content-fn-8)

### Implications of acausal decision theory \[Philosophical/conceptual\]

One big area is: the implications of acausal decision theory for our priorities. This is something that I previously wrote about in [Implications of ECL](https://lukasfinnveden.substack.com/p/implications-of-ecl) (there focusing specifically on [evidential cooperation in large worlds](https://longtermrisk.org/ecl)).

But to highlight one particular thing: One potential risk that's highlighted by acausal decision theories is _the risk of learning too much information_. This is discussed in Daniel Kokotajlo's [The Commitment Races problem](https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem), and some related but somewhat distinct risks are discussed in my post [When does EDT seek evidence about correlations?](https://lukasfinnveden.substack.com/p/when-does-edt-seek-evidence-about) I'm interested in further results about how big of a problem this could be in practice. If we get an intelligence explosion anytime soon, then our knowledge about distant civilizations could expand quickly. Before that happens, it could be wise to understand what sort of information we should be happy to learn as soon as possible vs. what information we should take certain precautions about.

Updateless Decision Theory, as first described [here](https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory), takes some steps towards solving that problem but is far from having succeeded. See e.g. [UDT shows that decision theory is more confusing than ever](https://www.alignmentforum.org/posts/wXbSAKu2AcohaK2Gt/udt-shows-that-decision-theory-is-more-puzzling-than-ever) for a description of remaining puzzles. (And e.g. [open-minded updatelessness](https://www.lesswrong.com/posts/uPWDwFJnxLaDiyv4M/open-minded-updatelessness) for a candidate direction to improve upon it).

### End

That's all I have on this topic! As a reminder: it's very incomplete. But if you're interested in working on projects like this, please feel free to get in touch.

[**Project Ideas for Making Transformative AI Go Well, Other Than by Working on Alignment**](https://www.forethought.org/research/project-ideas-for-making-transformative-ai-go-well-other-than-by-working-on-alignment) Article Series

Part 4 of 4

This series contains lists of projects that it could be very valuable for someone to tackle. The projects would be especially valuable if transformative AI is coming in the next 10 years or so,
but they are not primarily about controlling AI or aligning AI to human intentions.

Most of the projects would be valuable even if we were guaranteed to get aligned AI. Some of the projects would be especially valuable if we were inevitably going to get _mis_ aligned AI.

[Project Ideas: Sentience and Rights of Digital Minds](https://www.forethought.org/research/project-ideas-sentience-and-rights-of-digital-minds)