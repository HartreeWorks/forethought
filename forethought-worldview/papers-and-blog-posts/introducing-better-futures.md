---
title: "Introducing Better Futures"
url: https://www.forethought.org/research/introducing-better-futures
---

# Introducing Better Futures

[William MacAskill](https://www.forethought.org/people/william-macaskill)

3rd August 2025


## 1\. The basic case

Suppose we want the future to go better. What should we do?

One prevailing approach is to try to avoid roughly _zero-value_ futures: reducing the risks of human extinction or of misaligned AI takeover.

This essay series will explore an alternative point of view: _making good futures even better_. On this view, it’s not enough to avoid near-term catastrophe, because the future could still fall far short of what’s possible. From this perspective, a near-term priority — or maybe even _the_ priority — is to help achieve a truly _great_ future.

That is, we can make the future go better in one of two ways:

1. _Surviving_: Making sure humanity avoids near-term catastrophes (like extinction or permanent disempowerment).[1](https://www.forethought.org/research/introducing-better-futures#user-content-fn-1)

2. _Flourishing_: Improving the quality of the future we get if we avoid such catastrophes.


This essay series will argue that work on _Flourishing_ is in the same ballpark of priority as work on _Surviving_. The basic case for this appeals to the _scale_, _neglectedness_ and _tractability_ of the two problems, where I think that _Flourishing_ has greater scale and neglectedness, but probably lower tractability. This section informally states the argument; the supplement (“ [The Basic Case for Better Futures](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures)”) makes the case with more depth and precision.

### Scale

First, scale. As long as we’re closer to the ceiling on _Survival_ than we are on _Flourishing_ — if there is more room for improvement on the latter — then _Flourishing_ has greater scale.

To illustrate, suppose you think that our chances of survival this century are reasonably high (greater than 80%) but that, if we survive, we should expect a future that falls far short of how good it could be (less than 10% as good as the best feasible futures). These are close to my views; the view about _Surviving_ seems widely-held,[2](https://www.forethought.org/research/introducing-better-futures#user-content-fn-2) and Fin Moorhouse and I will argue in essays 2 and 3 for something like that view on _Flourishing_. If so, there’s more room to improve the future by working on _Flourishing_ than by working on _Surviving_.

![Chart comparing value loss from survival vs flourishing risks: small red area shows 20% extinction risk, large blue area shows 72% value loss from non-flourishing, demonstrating flourishing has 36x greater scale](https://images.ctfassets.net/4owxfjx3z3if/41DrT8Ehvh1OBpXkWvX82K/c2e8dccdd840f41a66013cf8e55e3c22/surviving-vs-flourishing-value-comparison.png?w=3840&q=75&fm=webp)

Comparing the scale of surviving and flourishing


On these numbers, if we completely solved the problem of not- _Surviving_, we would be 20 percentage points more likely to get a future that's 10% as good as it could be. Multiplying these together, the difference we’d make amounts to 2% of the value of the best feasible future.

In contrast, if we completely solved the problem of non- _Flourishing_, then we’d have an 80% chance of getting to a 100%-valuable future. The difference we’d make amounts to 72% of the value of the best feasible future — 36 times greater than if we’d solved the problem of not- _Surviving_. Indeed, increasing the value of the future given survival from 10% to just 12.5% would be as good as wholly eliminating the chance that we don't survive.[3](https://www.forethought.org/research/introducing-better-futures#user-content-fn-3)

And the upside from work on _Flourishing_ could plausibly be much greater still than these illustrative numbers suggest. If _Surviving_ is as high as 99% and _Flourishing_ as low as 1%, then the problem of non- _Flourishing_ is almost 10,000 times as great in scale as the risk of not- _Surviving_. So, for priority-setting, the value of forming better estimates of these numbers is high.[4](https://www.forethought.org/research/introducing-better-futures#user-content-fn-4)

| **Surviving (probability of avoiding a ~zero-value future)** | **Flourishing (% value of the future if we avoid a ~zero-value future)** | **Relative scale of non-Flourishing to not-Surviving** |
| --- | --- | --- |
| 0.8 | 0.1 | 36 |
| 0.95 | 0.05 | 361 |
| 0.99 | 0.01 | 9801 |

Comparing the value of fully solving non- _Flourishing_ with fully solving not- _Surviving_, given different default estimates of _Surviving_ and _Flourishing_.

A further argument about scale comes from considering _which_ worlds are saved by working on _Survival_, or improved by working on _Flourishing_. Conditional on successfully preventing an extinction-level catastrophe, you should expect _Flourishing_ to be (perhaps much) lower than otherwise, because a world that needs saving is more likely to be uncoordinated, poorly directed, or vulnerable in the long run. So the value of increasing _Survival_ is lower than it would first appear. On the other hand, there is little reason to believe that worlds where you successfully increase _Flourishing_ are ones in which the chance of _Surviving_ is especially low. So this consideration differentially increases the value of work on _Flourishing_.[5](https://www.forethought.org/research/introducing-better-futures#user-content-fn-5)

### Neglectedness

Second, neglectedness. Most people in the world today, on both their self-interest and their moral views, care much more about avoiding near-term catastrophe (including risks to the lives of themselves and their family), than they do about long-term flourishing. So we should expect at least some aspects of _Flourishing_ to be much more neglected, by the wider world, than risks to _Survival_.[6](https://www.forethought.org/research/introducing-better-futures#user-content-fn-6) Work on _Flourishing_ currently seems more neglected among those motivated by longtermism, too.

This neglect arises in part because the risks of failure in _Flourishing_ are often much more subtle than the risk of near-term catastrophe. The future could even be truly wonderful, compared to the current world, yet still fall radically short of what’s possible. Ask someone to picture utopia, and they might describe a society like ours, but free from its most glaring flaws, and abundant with those things we currently want. But the difference in value between the world today and that common-sense utopia might be very small compared to the difference between that common-sense utopia and the best futures we could feasibly achieve.

![Value spectrum showing existential catastrophe at zero, present day and common-sense utopia clustered near the low end, with vast unexplored space between utopia and near-best futures at value 1](https://images.ctfassets.net/4owxfjx3z3if/5vLIUnOs6bULRnxNrOkdCW/0d8a4775e7a98d2a422555a1852885eb/future-value-spectrum-diagram.png?w=3840&q=75&fm=webp)

Comparing the value of possible futures. The “present-day” future means a future which extends the most relevant features of the world today, for as long as the common-sense utopia lasts, and considering human lives only.


### Tractability

The tractability of work to improve _Flourishing_ is less clear; essays [4](https://www.forethought.org/research/persistent-path-dependence) and [5](https://www.forethought.org/research/how-to-make-the-future-better) will discuss this more. I see this as the strongest argument against the better futures perspective, and the reason why I don’t feel confident that work on _Flourishing_ is higher-priority than work on _Surviving_, rather than merely in the same ballpark.

But at the very least I think we should _try to find out_ how tractable work to improve _Flourishing_ is. Some promising areas include: reducing the risk of human concentration of power; ensuring that advanced AI is not merely corrigible but also loaded with good, reflective values; and improving the quality of decisions that structure the post-AGI world, including around space governance and the rights of digital beings.

## 2\. The series

In the rest of the series, I argue:

- **We are unlikely to get a flourishing future by default** even if we avoid catastrophe, because a flourishing future is a narrow target ( [essay 2](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile)) and it’s unlikely that future people will hone in on that target ( [essay 3](https://www.forethought.org/research/convergence-and-compromise#2-will-most-people-aim-at-the-good))[7](https://www.forethought.org/research/introducing-better-futures#user-content-fn-7)

- **It’s possible to have persistent positive impact on how well the long-run future goes other than by avoiding catastrophe** ( [essay 4](https://www.forethought.org/research/persistent-path-dependence))

- **There are concrete things we could do to this end, today** ( [essay 5](https://www.forethought.org/research/how-to-make-the-future-better#2-keeping-our-options-open))


There’s a lot I _don’t_ cover, too, just because of limitations of space and time. For an overview, see this footnote.[8](https://www.forethought.org/research/introducing-better-futures#user-content-fn-8)

## 3\. What Better Futures is not

Before we dive in, I want to clarify some possible misconceptions.

First, this series doesn’t require accepting consequentialism, which is the view that the moral rightness of an act depends wholly on the value of the outcomes it produces. It’s true that my focus is on how to bring about good outcomes, which is the consequentialist _part_ of morality. But I don’t claim you should always maximize the good, no matter the self-sacrifice, and no matter what means are involved. There are lots of other relevant moral considerations that should be weighed when taking action, including non-longtermist considerations like special obligations to those in the present (which generally favour interventions to increase _Survival_). But long-term consequences are important, too, and that’s what I focus on.[9](https://www.forethought.org/research/introducing-better-futures#user-content-fn-9),[10](https://www.forethought.org/research/introducing-better-futures#user-content-fn-10)

Second, this series doesn’t require accepting moral realism, which I’ll define as the view that there are objective facts about value, true independently from what anyone happens to think.[11](https://www.forethought.org/research/introducing-better-futures#user-content-fn-11) Whether or not you think there are objective moral facts, you can still care about how the future goes, and worry that the future will not be in line with your own values, or the values you’d have upon careful reflection. I’m aware that this series often uses realist-flavoured language, which is simpler and reflects how I personally tend to think about ethics. But we can usually just translate between realism and antirealism: where the realist speaks of the “correct” moral view, the antirealist could think about “the preferences I’d have given some ideal reflective process”.[12](https://www.forethought.org/research/introducing-better-futures#user-content-fn-12)

Third, this series isn’t in opposition to work on preventing downsides, like “s-risks” —  risks of astronomical amounts of suffering, which also affect “ _Flourishing_” rather than “ _Survival_”. We should take such risks seriously: depending on your values and your takes on tractability, they might be the top priority, and their importance comes up repeatedly in the next two essays. The focus of this series, though, is generally on making good futures even better, rather than avoiding net-negative futures.[13](https://www.forethought.org/research/introducing-better-futures#user-content-fn-13)

Fourth, the better futures perspective doesn’t mean endorsing some narrow conception of an ideal future, as past utopian visions have often done. Given how much moral progress we should hope to make in the future, and how much we’ll learn about what’s even empirically possible, we should act on the assumption that we have almost no idea what the best feasible futures would look like. Committing today to some particular vision would be a great mistake.

A central concept in my thinking about better futures is that of _viatopia_, which is a state of the world where society can guide itself towards near-best outcomes, whatever they may be.[14](https://www.forethought.org/research/introducing-better-futures#user-content-fn-14) We can describe viatopia even if we have little conception of what the desired end state is. Plausibly, viatopia is a state of society where existential risk is very low, where many different moral points of view can flourish, where many possible futures are still open to us, and where major decisions are made via thoughtful, reflective processes. From my point of view, the key priority in the world today is to get us closer to viatopia, not to some particular narrow end-state. I don’t discuss this concept further in this series, but I hope to write more about it in the future.

With that, let’s jump in.

[**Better Futures**](https://www.forethought.org/research/better-futures) Article Series

Part 1 of 6

Suppose we want the future to go better. What should we do?

One approach is to avoid near-term catastrophes, like human extinction. This essay series explores a different, complementary, approach: improving on futures where we survive, to achieve a truly great future.

[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)