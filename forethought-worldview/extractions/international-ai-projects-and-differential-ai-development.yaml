paper:
  slug: "international-ai-projects-and-differential-ai-development"
  title: "International AI projects and differential AI development"

premises_taken_as_given:
  - claim: "AI will be transformative and poses existential-scale risks"
    confidence: "near-certain"
    evidence: "The paper builds entirely on the assumption that advanced AI poses major risks requiring international governance; this is never argued for."
  - claim: "Once AI can fully automate AI R&D, progress follows a super-exponential curve (intelligence explosion)"
    confidence: "strong"
    evidence: "Stated as fact with a link to a separate Forethought paper on software intelligence explosions, not argued for here."
  - claim: "An international AI project of some kind is desirable"
    confidence: "strong"
    evidence: "The paper's entire purpose is to improve existing proposals for international AI projects; the desirability of such a project is the starting point, not the conclusion."
  - claim: "AI-enabled concentration of power (including coups) is a serious risk"
    confidence: "strong"
    evidence: "Referenced as a known risk with a link to a dedicated Forethought paper; treated as established rather than defended."
  - claim: "Large training runs are concentrated among a small number of actors, making compute-threshold governance feasible"
    confidence: "near-certain"
    evidence: "Used as a key premise for enforceability—'only a handful of actors'—without extended argument."

distinctive_claims:
  - claim: "Proposals for international AI projects should incorporate differential AI development: limiting dangerous capabilities, permitting beneficial ones, and actively encouraging capabilities that help manage AI risks"
    centrality: "thesis"
    key_argument: "Existing proposals apply blanket restrictions above compute thresholds, but this is both unnecessarily restrictive and misses the opportunity to accelerate AI capabilities that improve humanity's ability to navigate the intelligence explosion."
  - claim: "The ability to automate AI R&D (ML research/engineering, chip design) is the single most dangerous AI capability and should be the primary target of international restrictions"
    centrality: "load-bearing"
    key_argument: "AI R&D automation is what drives super-exponential progress, the transition from controllable to uncontrollable AI, rapid tech change with no response time, and decisive power concentration."
  - claim: "Narrow superintelligence in 'wisdom' domains (forecasting, policy analysis, ethical deliberation, trade-making, education) before general superintelligence would be highly desirable"
    centrality: "load-bearing"
    key_argument: "These capabilities augment human decision-makers so they can keep pace with the early intelligence explosion, analogous to narrow superintelligence in Chess/Go."
  - claim: "Restrictions on AI R&D automation are enforceable even without precise capability specifications, by analogy to financial fraud law"
    centrality: "supporting"
    key_argument: "The small number of actors capable of large training runs makes intense oversight feasible; companies have weak incentives to violate because detection risk is high, penalties severe, and profits remain available from non-R&D AI."
  - claim: "It can be beneficial for adversaries to also have access to certain AI capabilities (e.g. trade-identifying AI)"
    centrality: "supporting"
    key_argument: "Some capabilities are positive-sum—e.g. AI that identifies mutually beneficial treaties helps both parties."

positions_rejected:
  - position: "Blanket prohibition on all frontier AI development above a compute threshold outside an international project"
    why_rejected: "Unnecessarily restricts beneficial and helpful AI capabilities; is less acceptable to industry; increases incentives for outside actors to race; and misses the opportunity to differentially accelerate capabilities that reduce risk."
  - position: "Treating all frontier AI capabilities as equally dangerous"
    why_rejected: "Some capabilities (e.g. cancer screening AI) are broadly beneficial and others (e.g. forecasting AI) actively help manage AI risks; only capabilities enabling AI R&D automation and catastrophic tech production warrant restriction."
  - position: "That the difficulty of precisely specifying dangerous capabilities makes restriction infeasible"
    why_rejected: "Analogized to financial fraud law—imprecise specification is manageable when the number of regulated actors is small and oversight can be intense."

methodological_commitments:
  - "Policy proposal design: the paper is structured around generating and evaluating a specific governance proposal rather than empirical analysis or formal modelling"
  - "Differential technology development framework: explicitly adopts the lens of selectively accelerating vs. restricting different AI capabilities based on their risk/benefit profiles"
  - "Reasoning by analogy: uses analogies to financial fraud enforcement and narrow superintelligence in games to support feasibility and desirability claims"
  - "Consequentialist expected-value reasoning: explicitly weighs increased risk from dangerous capabilities against benefits from helpful ones ('it could well be worth increasing the risk from dangerous capabilities a little, to increase the benefits from helpful ones')"
  - "Building iteratively on prior Forethought proposals: frames itself as correcting a gap in prior work"

cross_references:
  - "what-an-international-project-to-develop-agi-should-look-like"
  - "a-global-convention-to-govern-the-intelligence-explosion"
  - "intelsat-as-a-model-for-international-agi-governance"
  - "ai-tools-for-existential-security"
  - "appendices-to-ai-tools-for-existential-security"
  - "will-ai-r-and-d-automation-cause-a-software-intelligence-explosion"
  - "preparing-for-the-intelligence-explosion"
  - "ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power"
  - "three-types-of-intelligence-explosion"
  - "agi-and-world-government"