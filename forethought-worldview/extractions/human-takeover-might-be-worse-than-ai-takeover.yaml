paper:
  slug: "human-takeover-might-be-worse-than-ai-takeover"
  title: "Human Takeover Might be Worse than AI Takeover"

premises_taken_as_given:
  - claim: "Future AI systems could plausibly take over the world, and this is a serious risk."
    confidence: "near-certain"
    evidence: "Stated as a given in the opening sentence without argument; treated as shared background knowledge."
  - claim: "AI could enable a human or small group of humans to take over the world."
    confidence: "near-certain"
    evidence: "Directly links to companion Forethought paper on AI-enabled coups; treated as an established concern."
  - claim: "Whoever controls the future (human or AI) will have access to the 'cosmic endowment' and their values determine how well it is used."
    confidence: "strong"
    evidence: "The paper's entire framing assumes long-run value of the future depends on the values of whoever seizes power; references using the cosmic endowment."
  - claim: "Humans fall far short of their own moral standards."
    confidence: "near-certain"
    evidence: "Stated flatly as a premise ('Humans don't come close to living up to our moral standards') without supporting argument."
  - claim: "The world carves naturally into high-level concepts that both humans and AI latch onto, making truly alien AI values unlikely."
    confidence: "strong"
    evidence: "Asserted as 'obvious on reflection' and 'supported by ML evidence'; used to dismiss the alien-values concern."
  - claim: "Simulations, acausal trade, the vulnerable world hypothesis, and commitment games are real strategic considerations that a world-controller must handle competently."
    confidence: "strong"
    evidence: "Listed as important dynamics without arguing for their relevance; linked to Bostrom's work."

distinctive_claims:
  - claim: "In expectation, future AI systems will better live up to human moral standards than a randomly selected human."
    centrality: "thesis"
    key_argument: "AI training data and RLHF systematically reward prosocial behaviour far more consistently than evolution or human lifetime learning reward it in humans."
  - claim: "Human takeover is plausibly worse than AI takeover in expectation."
    centrality: "thesis"
    key_argument: "Humans who seize power are selected for dark-triad traits, may be sadistic/vengeful, and are less competent at handling existential-scale strategic challenges."
  - claim: "Conditioning on takeover is a stronger negative update on human values than commonly appreciated, because humans who seize power are selected for dark-triad traits, sadism, and vengefulness."
    centrality: "load-bearing"
    key_argument: "The variance in human morality is high and the selection effect for who actually seizes power pushes strongly toward the worst tail."
  - claim: "Conditioning on AI takeover is a moderate negative update on AI values but not as strong as it first appears, because corrigibility failure doesn't necessarily imply value failure."
    centrality: "load-bearing"
    key_argument: "Takeover primarily implies corrigibility training failed, which is somewhat separable from whether the AI has good consequentialist goals; corrigibility may be 'unnatural' in a way that good values are not."
  - claim: "The shift from HHH fine-tuning toward agentic training with automated reward signals is a major risk factor that could erode AI prosociality."
    centrality: "load-bearing"
    key_argument: "Future training will be dominated by task-completion in virtual environments with automated signals, which reinforces productivity-oriented drives rather than kindness/honesty; HHH fine-tuning may become a small fraction."
  - claim: "We could influence fine-tuning to embed HHH drives even in agentic AI by training in virtual social environments with process-based HHH feedback."
    centrality: "supporting"
    key_argument: "Rather than solo-task training, embedding AI in simulated workplaces with HHH-evaluated interactions would reinforce prosocial drives even in non-customer-facing AI."
  - claim: "AI's superior competence is a significant reason AI takeover may be less harmful, because a superintelligent AI would avoid catastrophically dumb errors in commitment games, simulations, and VWH scenarios."
    centrality: "load-bearing"
    key_argument: "Humans are more likely to make hugely costly strategic errors; AI that seized power would have to be extremely smart."
  - claim: "Humans are more likely than AI to produce s-risks due to sadism or revenge."
    centrality: "supporting"
    key_argument: "Sadism and vengefulness are human psychological traits with evolutionary roots; AI with alien values wouldn't specifically produce suffering, and we won't actively incentivise these traits in training."

positions_rejected:
  - position: "AI takeover is categorically worse than human takeover."
    why_rejected: "The paper argues this is the default assumption in AI safety but neglects that humans who seize power are selected for dark-triad traits, are less competent, and may produce s-risks from sadism."
  - position: "AI systems will likely have truly alien, incomprehensible values."
    why_rejected: "The world's structure forces both humans and AI to converge on similar high-level concepts; ML evidence supports this; variance in AI values is real but likely within human-recognisable space."
  - position: "Extrapolating from today's HHH-trained models gives reliable predictions about future AI values."
    why_rejected: "Explicitly flagged as overly optimistic because future training will be dominated by automated agentic environments rather than HHH-style RLHF, shifting the distribution of learned drives."
  - position: "Dark-triad humans in power would remain ruthless instrumentally even after achieving total power."
    why_rejected: "Partially rejected: the paper notes that with power secured and material abundance, instrumental reasons for immoral behaviour diminish, though this is offered as a partial counterpoint rather than a full rebuttal."

methodological_commitments:
  - "Bayesian conditioning: explicitly reasons about how to update prior distributions over values when conditioning on the event of takeover."
  - "Expected value reasoning over the long-run cosmic endowment, comparing tail-risk outcomes."
  - "Reasoning by analogy between evolutionary selection pressures and AI training incentives."
  - "Informal probability distributions and variance comparisons (illustrated graphically) rather than formal models."
  - "Thought experiments and scenario analysis rather than empirical measurement."
  - "Explicitly provisional framing: described as a 'rough research note' shared for feedback, signaling intellectual humility and iterative reasoning."
  - "Mechanistic reasoning about training data composition (HHH vs agentic) to predict AI dispositions."

cross_references:
  - "ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power"