paper:
  slug: "supplement-the-basic-case-for-better-futures"
  title: "Supplement: The Basic Case for Better Futures"

premises_taken_as_given:
  - claim: "Longtermism is correct: almost all expected value from our actions lies in the far future."
    confidence: "near-certain"
    evidence: "Longtermism is cited as a foundational premise (referencing Greaves & MacAskill) and used without argument to simplify the model by zeroing out near-term and medium-term value components."
  - claim: "Expected value maximisation is the correct decision framework for comparing interventions."
    confidence: "near-certain"
    evidence: "The entire SF model is built on EV decomposition, and positive affine transformations of value are invoked to simplify expressions without further justification."
  - claim: "The scale-neglectedness-tractability framework is a valid method for comparing cause areas."
    confidence: "near-certain"
    evidence: "The framework is adopted wholesale (citing Cotton-Barratt) as the organising structure for the argument without defending it against alternatives."
  - claim: "An intelligence explosion (rapid transition to superintelligence) is a live and important possibility this century."
    confidence: "strong"
    evidence: "The paper repeatedly references 'the intelligence explosion' as a planning scenario, discussing which challenges arise 'deeper into' it, and cross-references 'Preparing for the Intelligence Explosion'."
  - claim: "AI takeover and engineered pandemics represent the primary sources of near-term existential risk."
    confidence: "strong"
    evidence: "These are the two risk categories used as running examples throughout the Survival discussion, treated as the canonical x-risks without extensive argument."
  - claim: "The current expected value of Flourishing (F) is positive."
    confidence: "strong"
    evidence: "Explicitly stated as an assumption ('we will assume that F is currently positive') to ensure raising S is valuable, but acknowledged as an assumption rather than argued for at length."
  - claim: "We are probably far from the ceiling on Flourishing (F)."
    confidence: "strong"
    evidence: "Asserted by reference to companion essays 'No Easy Eutopia' and 'Convergence and Compromise', with illustrative estimates of F between 1% and 50%."

distinctive_claims:
  - claim: "The expected value of the future can be usefully decomposed into S (probability of surviving existential catastrophe this century) × F (expected value conditional on survival), and this decomposition reveals that work on F is systematically under-prioritised."
    centrality: "thesis"
    key_argument: "The multiplicative SF model, combined with the observation that we are much further from the ceiling on F than on S, means the marginal value of improving F exceeds that of improving S in scale, often by orders of magnitude."
  - claim: "The problem of non-flourishing has roughly 1.9x–9,801x the scale of the problem of non-survival, with the authors' own views leaning toward ~100x."
    centrality: "thesis"
    key_argument: "Quantitative tables using both absolute and proportional change formulations show that when S is high and F is low, gains from improving F dominate gains from improving S."
  - claim: "Flourishing-focused work is approximately 10x–100x more neglected than Survival-focused work, and this roughly cancels out Flourishing's 10x–100x lower tractability."
    centrality: "load-bearing"
    key_argument: "Latent self-interested demand to avoid extinction is enormous and will be progressively mobilised, whereas no comparable demand exists for issues like extrasolar resource governance or improving AI alignment targets; but tractability is lower because Flourishing often requires coordination, regulation, and faces active opposition."
  - claim: "Conditional on successfully averting an existential catastrophe, the expected value of Flourishing is lower than unconditional E[F], because worlds where you needed to avert catastrophe are probably 'badly-run' worlds."
    centrality: "load-bearing"
    key_argument: "Bayesian updating: if you successfully prevented a catastrophe, you're probably in a high-risk world, which is also likely low-Flourishing, making S-interventions less valuable than naively calculated (illustrated with a two-state model yielding a 5x discount)."
  - claim: "The existence of potential alien civilisations both raises S (since extinction of humans doesn't preclude other civilisations settling our light cone) and lowers F (since the marginal contribution of Earth-originating life is smaller), further favouring attention to Flourishing."
    centrality: "supporting"
    key_argument: "Formalised: if aliens would use fraction A of resources in our light cone, the best feasible future attributable to us shrinks, lowering F; meanwhile S rises because human extinction no longer means zero value."
  - claim: "The 'baby problem' strategy used in AI safety (working on tractable present-day versions of future challenges) can and should be applied to Flourishing areas like AI epistemics, the alignment target, and space governance."
    centrality: "load-bearing"
    key_argument: "Historical analogy: AI safety and biorisk were once seen as intractable but became tractable through foundational research, field-building, and organisational creation; the same dynamic should apply to Flourishing work."
  - claim: "Maximising only S (i.e. adopting something like Bostrom's Maxipok principle as a complete strategy) is very unlikely to be the best course of action."
    centrality: "load-bearing"
    key_argument: "The 'tails come apart' principle: when maximising a product of two imperfectly correlated variables, maximising one factor alone is suboptimal."
  - claim: "Personal fit should often be the determining factor in choosing between Survival and Flourishing work, and the EA-adjacent community has unusually good fit for Flourishing work."
    centrality: "supporting"
    key_argument: "Flourishing work benefits from generalist research skills, comfort with pre-paradigm fields, taking abstract ethics seriously, and understanding intelligence explosion dynamics—traits concentrated in EA."

positions_rejected:
  - position: "Exclusive focus on existential risk reduction (Maxipok-style) is the optimal longtermist strategy."
    why_rejected: "The SF model shows that when S is high and F is low, marginal improvements to F dominate; the tails-come-apart argument shows that optimising one factor of a product is suboptimal."
  - position: "Flourishing work can be dismissed due to low tractability."
    why_rejected: "General arguments (tractability varies less than scale/neglectedness; unexplored areas have uncertain but non-trivially-positive expected tractability; track record of AI safety becoming tractable) and specific baby-problem strategies suggest tractability is sufficient, especially given the enormous scale advantage."
  - position: "The relevant decomposition of the future's value should be 'human-controlled vs. not-human-controlled' futures."
    why_rejected: "The dividing line between human-controlled and non-controlled is very blurry; EV(future | loss of human control) is not approximately zero; and some people should be trying to increase EV(future | loss of human control)."
  - position: "Conditional on successfully raising S, the world is in roughly the same state as unconditionally expected."
    why_rejected: "Bayesian reasoning shows that successfully averting catastrophe is evidence you're in a high-risk, badly-run world, so E[F | you raised S] < unconditional E[F], discounting the value of S-work."
  - position: "Better futures work should wait until AI timelines are clearer or until after superintelligence arrives."
    why_rejected: "Some windows of opportunity close early; early work sets precedents; field-building now pays off later; and these arguments are strongest in worlds where AGI is 3+ years away, which retains substantial probability."

methodological_commitments:
  - "Expected value decomposition and multiplicative factor modelling (the SF model) as the primary analytical framework."
  - "Scale-neglectedness-tractability (SNT) framework for comparing cause areas, using both absolute and proportional change formulations."
  - "Bayesian updating and conditional expectation reasoning (e.g. 'which world gets saved' analysis)."
  - "Quantitative estimation with explicit uncertainty ranges (e.g. S ∈ [0.65, 0.99], F ∈ [0.01, 0.5], neglectedness ratios, tractability ratios)."
  - "Historical analogy as evidence (AI safety's trajectory from intractable to tractable as a model for Flourishing work)."
  - "Thought experiments with illustrative numerical examples (two-state world model, alien civilisation scenarios)."
  - "Reasoning by analogy with established EA/longtermist concepts (tails come apart, logarithmic returns, value of information)."

cross_references:
  - "no-easy-eutopia"
  - "convergence-and-compromise"
  - "preparing-for-the-intelligence-explosion"
  - "how-to-make-the-future-better"