paper:
  slug: "ai-adoption-gap"
  title: "The AI Adoption Gap: Preparing the US Government for Advanced AI"

premises_taken_as_given:
  - claim: "AI capabilities are improving rapidly and will continue to do so, likely at a greater-than-linear rate"
    confidence: "near-certain"
    evidence: "Treated as background fact throughout; references to 'accelerating AI progress,' plummeting costs, and improving capabilities are never argued for"
  - claim: "AI will be transformative enough to pose existential-level challenges (pandemics, great power war, human disempowerment by AI)"
    confidence: "near-certain"
    evidence: "The paper lists catastrophic scenarios as motivating examples without arguing that they are plausible; they function as the premise justifying urgent action"
  - claim: "The US federal government has a legitimate and important role in governing AI and responding to AI-driven challenges"
    confidence: "near-certain"
    evidence: "Never questioned; the entire paper is built on the assumption that government capacity matters for existential risk reduction"
  - claim: "Democratic governance and institutional legitimacy are intrinsically valuable and worth preserving"
    confidence: "near-certain"
    evidence: "Threats to democratic oversight and separation of powers are consistently framed as harms without defending the value of democracy itself"
  - claim: "The private sector is substantially ahead of the US government in AI adoption, and this gap is likely to widen"
    confidence: "strong"
    evidence: "Supported with data (job listings, surveys, contract analysis) but the widening-gap projection is explicitly flagged as uncertain with some counterscenarios acknowledged"
  - claim: "Uncontrollable or 'rogue' AI systems are a serious risk worth planning around"
    confidence: "strong"
    evidence: "Referenced as a motivating concern (citing Bengio) but not argued for in depth; treated as a known risk in the AI safety community"
  - claim: "Market concentration among frontier AI developers is a concerning trend"
    confidence: "strong"
    evidence: "Mentioned as a risk factor multiple times (bargaining power, vendor lock-in, Microsoft's 85% market share) without extended argument"

distinctive_claims:
  - claim: "The growing AI adoption gap between the US government and private sector is itself a major source of existential risk, not merely an efficiency problem"
    centrality: "thesis"
    key_argument: "A capacity-lagging government cannot respond to AI-driven existential challenges, maintain democratic legitimacy against AI-empowered private actors, and faces dangerous rushed-adoption dynamics later"
  - claim: "Gradual government AI adoption now is strictly safer than delayed adoption followed by a crisis-driven rush"
    centrality: "load-bearing"
    key_argument: "The 'procrastination penalty' means delayed adoption raises the probability of hasty, corner-cutting integration when a crisis forces action, with less time for testing, expertise-building, and safeguards"
  - claim: "The framing of 'pro-speed vs. pro-security' for government AI adoption is a false dichotomy; many interventions are win-win"
    centrality: "load-bearing"
    key_argument: "Clear policies increase uptake in risk-averse environments; building technical expertise serves both adoption and oversight; the four-quadrant framework shows most high-priority interventions benefit both goals"
  - claim: "We should actively prepare contingency plans for two specific failure modes: effective state collapse (government remains low-capacity) and rushed late-stage adoption"
    centrality: "load-bearing"
    key_argument: "Current trends suggest slow adoption is the default; these scenarios would invalidate much current risk-mitigation work and receive too little advance attention"
  - claim: "AI governance strategies that do not depend on US federal government action deserve significant investment as hedges"
    centrality: "supporting"
    key_argument: "If the US government remains low-capacity, third-party auditors, private governance, non-US institutions, and direct work with AI companies become critical backstops"
  - claim: "Clumsy or gratuitous safety requirements can actively increase overall risk by creating compliance theater and crowding out effective safeguards"
    centrality: "supporting"
    key_argument: "Cites evidence that human oversight policies legitimize faulty algorithms without fixing them; overly burdensome rules get dropped in crises when most needed"
  - claim: "AI-enabled subversion of democratic processes—including by AI companies leveraging their position as technology providers—is a serious near-term concern"
    centrality: "supporting"
    key_argument: "References Microsoft's 85% government market share and security failures, forthcoming work on 'AI-Enabled Coups,' and the risk that government dependence on a few AI vendors undermines independent oversight"

positions_rejected:
  - position: "Blanket acceleration of government AI adoption without regard to safety"
    why_rejected: "Could desensitize decision-makers to risks, enable subversion, deploy unsafe systems, and accelerate arms race dynamics"
  - position: "Blanket restriction or slowdown of government AI adoption for safety reasons"
    why_rejected: "Would block valuable low-risk use cases, widen the capacity gap, and increase the probability of dangerous rushed adoption later"
  - position: "AI governance can rely primarily on US federal government regulation"
    why_rejected: "Current trends suggest the government may remain too low-capacity; contingency planning for non-governmental and non-US governance mechanisms is warranted"
  - position: "DOGE-style disruption will straightforwardly solve government AI adoption problems"
    why_rejected: "Acknowledged as a possible opportunity but flagged as potentially counterproductive—could reduce critical AI expertise and cut adoption funding"
  - position: "The government should primarily focus AI adoption on military/defense applications"
    why_rejected: "Over-focusing on military uses risks arms race dynamics, misperceptions by other states, and neglects civilian agencies where the adoption gap is most severe"

methodological_commitments:
  - "Policy analysis grounded in institutional and organizational constraints rather than abstract optimization"
  - "Scenario planning and contingency thinking: explicitly models failure modes (state collapse, rushed adoption) and designs hedges"
  - "Empirical grounding through surveys, contract data, job listing analysis, and case studies of specific agencies"
  - "Two-dimensional cost-benefit framing (adoption vs. safety) rather than single-axis evaluation of interventions"
  - "Focus on neglectedness and tractability as criteria for prioritization (explicitly calls government adoption 'a neglected lever')"
  - "Historical analogy as supporting evidence (2008 financial crisis, Standard Oil, government IT modernization history)"
  - "Orientation toward actionable, specific policy recommendations rather than theoretical analysis"

cross_references:
  - "should-there-be-just-one-western-agi-project"
  - "agi-and-lock-in"
  - "ai-tools-for-existential-security"
  - "preparing-for-the-intelligence-explosion"