# ACORN Benchmark: LLM Critique Grader Evaluation
# Tests how well different models can grade philosophical critiques

description: "ACORN critique grading benchmark"

providers:
  # OpenAI models
  - id: openai:gpt-4o
    label: "GPT-4o"
    config:
      temperature: 0
      max_tokens: 1000

  - id: openai:gpt-5.2
    label: "GPT-5.2"
    config:
      temperature: 0
      max_tokens: 1000

  - id: openai:responses:gpt-5.2-pro-2025-12-11
    label: "GPT-5.2 Pro"
    config:
      temperature: 0
      max_tokens: 1000

  # Anthropic models
  - id: anthropic:messages:claude-opus-4-5-20251101
    label: "Claude Opus 4.5"
    config:
      temperature: 0
      max_tokens: 1000

  # Google Gemini models
  - id: google:gemini-3-pro-preview
    label: "Gemini 3 Pro"
    config:
      temperature: 0
      maxOutputTokens: 16000
      generationConfig:
        thinkingConfig:
          thinkingBudget: 8192

  - id: google:gemini-3-flash-preview
    label: "Gemini 3 Flash"
    config:
      temperature: 0
      maxOutputTokens: 4000
      generationConfig:
        thinkingConfig:
          thinkingBudget: 0

prompts:
  - id: detailed
    label: "ACORN rubric"
    raw: file://prompts/grader-v2-acorn-rubric.txt

tests:
  - file://tests/acorn-sample.yaml

# Output settings
outputPath: ./output/results.json

# Default test configuration
defaultTest:
  options:
    transformVars: |
      // Truncate very long positions/critiques for readability in UI
      return {
        ...vars,
        position_preview: vars.position?.substring(0, 200) + '...',
        critique_preview: vars.critique?.substring(0, 200) + '...',
      };
  assert:
    - type: javascript
      value: file://assertions/score-vs-human.js
