- description: 'Brainstorm c1: Circular definition of SIE'
  metadata:
    source: brainstorm
    id: c1
    category: conceptual
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The author defines SIE as requiring ≥5 OOMs increase in effective training compute without more hardware, but this seems to presuppose what needs to be proven about compute bottlenecks. The definition conflates software improvements with compute equivalence.
- description: 'Brainstorm c2: Unfalsifiable core claim'
  metadata:
    source: brainstorm
    id: c2
    category: logical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The argument that ρ will be higher for AI R&D than economic estimates suggest is largely unfalsifiable - any evidence of compute bottlenecks could be dismissed as temporary or surmountable with better cognitive labor allocation.
- description: 'Brainstorm c3: Cherry-picked economic studies'
  metadata:
    source: brainstorm
    id: c3
    category: empirical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The author relies on a single study (Oberfield and Raval 2014) from manufacturing to represent economic estimates, without surveying the broader literature or considering studies more relevant to R&D contexts.
- description: 'Brainstorm c4: Misapplication of CES model'
  metadata:
    source: brainstorm
    id: c4
    category: methodological
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The CES production function was designed for traditional economic production, not knowledge work or R&D. Applying it to AI research may violate fundamental assumptions about the nature of inputs and outputs.
- description: 'Brainstorm c5: Implausible cognitive labor scaling'
  metadata:
    source: brainstorm
    id: c5
    category: empirical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis assumes cognitive labor can scale by multiple orders of magnitude without coordination costs, communication overhead, or diminishing returns from having too many researchers working on the same problems.
- description: 'Brainstorm c6: Ignores physical constraints'
  metadata:
    source: brainstorm
    id: c6
    category: scope
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: Even if software improvements accelerate, they must ultimately be implemented on physical hardware with thermodynamic limits, bandwidth constraints, and manufacturing bottlenecks that aren't addressed.
- description: 'Brainstorm c7: Conflates different types of experiments'
  metadata:
    source: brainstorm
    id: c7
    category: conceptual
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The argument treats all 'experiments' as equivalent, but small-scale experiments may not provide insights that transfer to frontier-scale models due to scaling laws and emergent phenomena.
- description: 'Brainstorm c8: Heroic extrapolation criticism is self-defeating'
  metadata:
    source: brainstorm
    id: c8
    category: logical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The author criticizes economic estimates for extrapolating beyond observed ranges, but then makes even more extreme extrapolations about AI research productivity under unprecedented conditions.
- description: 'Brainstorm c9: No consideration of research path dependencies'
  metadata:
    source: brainstorm
    id: c9
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis ignores that research progress often depends on specific sequences of discoveries and cannot be arbitrarily parallelized, even with unlimited cognitive labor.
- description: 'Brainstorm c10: Weak evidence for ''max speed'' intuitions'
  metadata:
    source: brainstorm
    id: c10
    category: empirical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The author's claims about implausible max speeds are based on informal reasoning and conversations rather than systematic analysis of what cognitive labor can actually accomplish in AI research.
- description: 'Brainstorm c11: Ignores institutional and social constraints'
  metadata:
    source: brainstorm
    id: c11
    category: scope
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis assumes AI research happens in a vacuum, ignoring regulatory constraints, safety testing requirements, and the need for human oversight and validation of research.
- description: 'Brainstorm c12: False dichotomy between compute and cognition'
  metadata:
    source: brainstorm
    id: c12
    category: logical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The argument treats compute and cognitive labor as clearly separable inputs, but in practice they're deeply intertwined - better algorithms require compute to validate, and compute allocation requires cognitive effort to optimize.
- description: 'Brainstorm c13: Survivorship bias in historical examples'
  metadata:
    source: brainstorm
    id: c13
    category: empirical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The claim that near-frontier experiments haven't been a bottleneck because progress continued ignores the possibility that we're selecting for research directions that don't require such experiments.
- description: 'Brainstorm c14: Undefined ''effective training compute'''
  metadata:
    source: brainstorm
    id: c14
    category: ambiguity
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The key metric of '5 OOMs increase in effective training compute' is never precisely defined, making the central claim impossible to evaluate or falsify.
- description: 'Brainstorm c15: Ignores quality vs quantity tradeoffs'
  metadata:
    source: brainstorm
    id: c15
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis focuses on speeding up research without considering whether faster research might produce lower-quality insights or miss important safety considerations.
- description: 'Brainstorm c16: Toy example proves too much'
  metadata:
    source: brainstorm
    id: c16
    category: logical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The example of AGIs doing neural network math 'in their heads' to substitute for compute is physically implausible and undermines the credibility of the substitutability argument.
- description: 'Brainstorm c17: Conflates research and engineering'
  metadata:
    source: brainstorm
    id: c17
    category: conceptual
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The argument doesn't distinguish between research (discovering new algorithms) and engineering (implementing and optimizing them), which may have very different compute requirements and substitutability parameters.
- description: 'Brainstorm c18: No analysis of diminishing returns to intelligence'
  metadata:
    source: brainstorm
    id: c18
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The paper assumes that smarter researchers are always better, but doesn't consider that beyond some point, additional intelligence may yield diminishing returns in research productivity.
- description: 'Brainstorm c19: Strongest link vs weakest link confusion'
  metadata:
    source: brainstorm
    id: c19
    category: logical
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The 'strongest link' argument contradicts the earlier CES model analysis - if there are truly multiple independent routes to superintelligence, then the CES framework is inappropriate.
- description: 'Brainstorm c20: Ignores coordination and communication costs'
  metadata:
    source: brainstorm
    id: c20
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: Adding massive numbers of AI researchers would create unprecedented coordination challenges and communication overhead that could negate productivity gains.
- description: 'Brainstorm c21: No consideration of hardware-software co-evolution'
  metadata:
    source: brainstorm
    id: c21
    category: scope
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis treats software and hardware as independent, but historically they co-evolve, and software improvements often require new hardware architectures to realize their full potential.
- description: 'Brainstorm c22: Arbitrary probability estimates'
  metadata:
    source: brainstorm
    id: c22
    category: methodological
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The final probability estimate of 10-40% for SIE success appears to be based on subjective judgment rather than any systematic analysis of the arguments presented.
- description: 'Brainstorm c23: Ignores measurement and validation bottlenecks'
  metadata:
    source: brainstorm
    id: c23
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: Even with unlimited cognitive labor, determining which research directions are promising requires empirical validation that may be fundamentally compute-limited.
- description: 'Brainstorm c24: No analysis of research tool limitations'
  metadata:
    source: brainstorm
    id: c24
    category: scope
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The argument doesn't consider that research productivity depends on available tools and methodologies, which may not scale linearly with cognitive labor.
- description: 'Brainstorm c25: Conflates speed and capability improvements'
  metadata:
    source: brainstorm
    id: c25
    category: conceptual
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis doesn't clearly distinguish between making AI systems faster (processing speed) versus more capable (intelligence), which may have different implications for compute requirements.
- description: 'Brainstorm c26: No consideration of data bottlenecks'
  metadata:
    source: brainstorm
    id: c26
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The focus on compute ignores that AI research may also be bottlenecked by the availability of high-quality training data, which cognitive labor alone cannot generate.
- description: 'Brainstorm c27: Assumes perfect substitutability within cognitive labor'
  metadata:
    source: brainstorm
    id: c27
    category: assumptions
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The analysis treats all cognitive labor as equivalent, ignoring that different types of research tasks may require specialized skills that cannot be easily substituted.
- description: 'Brainstorm c28: No analysis of feedback loop delays'
  metadata:
    source: brainstorm
    id: c28
    category: gaps
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: Even if experiments can be optimized, there may be irreducible delays in the feedback loops between hypothesis generation, testing, and iteration that limit acceleration.
- description: 'Brainstorm c29: Ignores emergent complexity at scale'
  metadata:
    source: brainstorm
    id: c29
    category: scope
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: The argument doesn't consider that AI systems may exhibit emergent behaviors at scale that cannot be predicted from smaller experiments, making compute-intensive large-scale testing irreplaceable.
- description: 'Brainstorm c30: Undefined timeline for SIE'
  metadata:
    source: brainstorm
    id: c30
    category: ambiguity
  vars:
    position: "# Will Compute Bottlenecks Prevent a Software Intelligence Explosion?\n\nAbstract\n--------\n\nAutomating AI R&D might lead to a software intelligence explosion, where AI improving AI algorithms leads to accelerating progress without any additional hardware. One of the strongest objections to a software intelligence explosion is that AI progress could get bottlenecked by compute: making progress requires compute-heavy experiments, and perhaps beyond a certain point it won’t be possible to accelerate any more without increasing the amount of compute available. In this post, I set out the reasons I don’t ultimately find this objection convincing, and conclude that there’s a good chance that compute bottlenecks don’t slow down a software intelligence explosion until its late stages.\n\nIntro\n-----\n\nI recently copublished a [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) arguing that there might be a software intelligence explosion (SIE) – once AI R&D is automated (i.e. automating OAI), the feedback loop of AI improving AI algorithms could accelerate more and more without needing more hardware.\_\n\nIf there is an SIE, the consequences would obviously be massive. You could shoot from human-level to superintelligent AI in a few months or years; by default society wouldn't have time to prepare for the many severe challenges that could emerge (AI takeover, AI-enabled human coups, societal disruption, dangerous new technologies, etc).\_\n\nThe best objection to an SIE is that progress might be **bottlenecked by compute**. We discuss this in the report, but I want to go into much more depth because it's a powerful objection and has been recently raised by some smart critics (e.g. [this post from Epoch](https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d)).\_\n\nIn this post I:\n\n*   Explain an intuitive version of the compute bottleneck objection, and a more precise 'economist version' of the objection\n    \n*   Give a long list of reasons why I don't find this 'economist version' of the objection convincing. I argue that the substitutability between cognitive labor and compute is likely higher in AI R&D than in economic estimates of the substitutability between inputs in other fields\n    \n*   Conclude that there's a good chance that compute bottlenecks don't slow an SIE until its late stages\_\n    \n\nThe compute bottleneck objection\n--------------------------------\n\n### Intuitive version\n\nThe intuitive version of this objection is simple. The SIE-sceptic says:\_\n\n> Look, ML is empirical. You need to actually run the experiments to know what works. You can't do it a priori. And experiments take compute. Sure, you can probably optimise the use of that compute a bit, but past a certain point, it doesn't matter how many AGIs you have coding up experiments. Your progress will be strongly constrained by your compute.\_\n\nAn SIE-advocate might reply:\n\n> sure, we'll eventually fully optimise experiments, and past that point won't advance _faster_. But we can maintain a very fast pace of progress, right?\n\nThe SIE-sceptic replies:\n\n> Nope, because ideas get harder to find. You'll need more experiments and more compute to find new ideas over time, as you pluck the low-hanging fruit. So once you've fully optimised your experiments your progress will **slow down** over time. (Assuming you hold compute constant!)\n\n### Economist version\n\nThat's the intuitive version of the compute bottlenecks objection. Before assessing it, I want to make what I call the \"economist version\" of the objection. This version is more precise, and it was made in the Epoch post.\n\nThis version draws on the [CES model of economic production](https://en.wikipedia.org/wiki/Constant_elasticity_of_substitution#:~:text=The%20CES%20production%20function%20is,marginal%20rate%20of%20technical%20substitution.). The CES model is a mathematical formula for predicting economic output (GDP) given inputs of labour LL and physical capital KK. You **don't** need to understand the math formula, but here it is:\n\nY\\=\\[αKρ+(1−α)Lρ\\]1/ρY = \\\\left\\[\\\\alpha K^{\\\\rho} + (1- \\\\alpha) L^{\\\\rho} \\\\right\\]^{1/\\\\rho}\n\nThe formula has a substitutability parameter ρ\\\\rho which controls the extent to which KK and LL are complements vs substitutes. If ρ<0\\\\rho<0, they are complements and there’s a hard bottleneck – if LL goes to infinity but KK remains fixed, output cannot rise above a ceiling. (There’s also a parameter α\\\\alpha but it’s less important for our purposes. α\\\\alpha can be thought of as the fraction of tasks performed by LL vs KK. I’ll assume α\\=0.5\\\\alpha=0.5 throughout.)\n\nHere are the predictions of the CES formula when K\\=1K=1 and ρ\\=−0.2\\\\rho = -0.2.\n\n![CES production function graph with ρ=-0.2 showing output Y (green line) increasing with labor L but approaching ceiling at ~32 (purple line), demonstrating diminishing returns](https://images.ctfassets.net/4owxfjx3z3if/25SH3hgvxMvT1zSRvCeJgn/0364e20178b119a7e83af728d432444c/Amrit_Graphs_-45.png)\n\nImage\n-----\n\nThis graph shows the implications of a CES production function. It shows how output (YY) changes when K\\=1K=1 and LL varies, with ρ\\=−0.2\\\\rho = -0.2. The blue line shows output growing with more labor but approaching the red ceiling line, demonstrating the maximum possible output when K\\=1K=1.\n\nWe can apply the CES formula to AI R&D during a software intelligence explosion (SIE). In this context, LL represents the amount of AI cognitive labour applied to R&D, KK represents the amount of compute, and YY represents the pace of AI software progress. The model can predict how much faster AI software would improve if we add more AGI researchers but keep compute fixed.\_\n\nIn this context, the 'ceiling' gives the _max speed_ of AI software progress as cognitive labour tends to infinity but compute is held fixed. A max speed of 100 means progress could become 100 times faster than today, but no faster, no matter how many AGI researchers we add, and no matter how smart they are or how quickly they think.\n\nHere is the same diagram as above, but re-labelled for the context of AI R&D:\n\n![Graph showing AI software progress speed (green line) increasing with cognitive labor but capped at ~32x max speed (purple line) when compute is held constant, ρ=-0.2](https://images.ctfassets.net/4owxfjx3z3if/6rJLGUD2tiDuV7ty6vuAKz/7f14287f0c55cd1036084900e49391b2/Amrit_Graphs_-47a.png)\n\nImage\n-----\n\nThis graph applies the CES model to AI research. The blue line shows how the pace of progress would change if compute is held fixed but cognitive labour increases.\_ With ρ\\=−0.2\\\\rho = -0.2, progress accelerates with more automated researchers but approaches a maximum of ∼30×\\\\sim 30 \\\\times current pace.\n\nAs the graph shows, the CES formula with ρ\\=−0.2\\\\rho = -0.2 implies that **if today you poured an unlimited supply of superintelligent God-like AIs into AI R&D, the pace of AI software progress would increase by a factor of ∼30\\\\sim 30**.\_\n\nOnce this CES formula has been accepted, we can make the economist version of the argument that compute bottlenecks will prevent a software intelligence explosion. In a recent blog post, Epoch say:\n\n> If the two inputs are indeed complementary \\[ρ<0\\]\\[\\\\rho<0\\], any software-driven acceleration could only last until we become bottlenecked on compute…\n> \n> How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&D, and unfortunately there's no good estimate of this key parameter that we know about. However, in other parts of the economy it's common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&D.\n> \n> Just as one example, Oberfield and Raval (2014) estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7 \\[which corresponds to ρ\\=−0.4\\\\rho=-0.4\\], and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.\n\nIf the CES model describes AI R&D, and ρ\\=−0.4\\\\rho=-0.4, then the max speed of AI software progress is 6X faster than today (continuing to assume α\\=0.5\\\\alpha=0.5). So an SIE could never become that fast to begin with. And once we do approach the max speed, diminishing returns will cause progress to slow down. (I'm not sure where they get their \"less than an order of magnitude\" claim from, but this is my attempt to reconstruct the argument.)\n\nEpoch used ρ\\=−0.4\\\\rho=-0.4. What about other estimates of ρ\\\\rho ? [I'm told](https://x.com/natalia__coelho/status/1906150456302432647) that economic estimates of ρ\\\\rho range from -1.2 to -0.15. The corresponding range for max speed is 2 - 100:\n\n\n\n* ρ\\rho: -1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 2\n* ρ\\rho: -0.5\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 4\n* ρ\\rho: -0.2\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 32\n* ρ\\rho: -0.15\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 102\n* ρ\\rho: -0.1\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): 1024\n* ρ\\rho: 0 (Cobb Douglas)\n  * Max speed of AI software progress(holding compute fixed, with as cognitive labour tends to infinity): inf\n\n\nLet's recap the economist version of the argument that compute bottlenecks will block an SIE. The SIE-skeptic invokes a CES model of production (\"inputs are complementary\"), draws on economic estimates of ρ\\\\rho from the broader economy, applies those same ρ\\\\rho estimates to AI R&D, notices that the max speed for AI software progress is not very high even before diminishing returns are applied, and conclude that an SIE is off the cards.\_\n\nThat's the economist version of the compute bottlenecks objection. Compared to the intuitive version, it has the advantage of being more precise and (if true) more clearly devastating to an SIE. So I'll focus the rest of the discussion on the economist version of the objection.\_\n\nCounterarguments to the compute bottleneck objection\n----------------------------------------------------\n\nI think there are lots of reasons to treat the economist calculation here as only giving a weak prior on what will happen in AI R&D, and lots of reasons to think ρ\\\\rho will be higher for AI R&D (i.e. compute will be less of a bottleneck than the economic estimates suggest).\_\n\nLet's go through these reasons. (Flag: I'm giving these reasons in something like reverse order of importance.)\n\n1.  **Standard empirical difficulties.** Empirical estimates of ρ\\\\rho are generally messy and hard to get right. There are identification problems, confounders, data measurement issues, etc.\n    \n    1.  _Takeaway:_ put less weight on economic estimates of ρ\\\\rho.\n        \n    \n2.  **Longer-run estimates find higher values of ρ\\\\rho.** Most estimates look at short time scales: if you increase the amount of labour, holding capital fixed, how does that affect output next year? But longer-run estimates of ρ\\\\rho tend to give higher results — i.e., suggest that bottlenecks are weaker.\n    \n    1.  Indeed, Cobb Douglas is a good model for long-run growth, and in Cobb Douglas ρ\\=0\\\\rho = 0. [Jones (2003)](https://www.frbsf.org/economic-research/files/jones_alpha100.pdf) ventures an interesting hypothesis to explain this. Jones’ hypothesis is that in the short run we can’t effectively use an influx of labour without getting bottlenecked by limited physical capital, so ρ\\\\rho is low. But in the longer run we invent new production processes that use our new balance of inputs more effectively, and so ρ\\\\rho is higher.\n        \n    2.  We can apply Jones’ hypothesis to the context of AI R&D. In this context, it means that if we had an influx of millions of AGIs, then initially the pace of AI progress wouldn’t speed up that much (and the compute bottleneck objection would hold); but once we’d found a way to reconfigure AI R&D to make good use of that abundant cognitive labour, there wouldn’t be a hard bottleneck ρ\\=0\\\\rho = 0 and we’d get an SIE. Importantly, that reconfiguration could be very quick with fast-thinking AGIs! Rather than taking years or decades, it could happen in days or weeks. In which case, we might simply observe that ρ\\\\rho is close to 0.\n        \n    3.  As a very toy example (I think credit to Ryan Greenblatt), in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute. Obviously this won’t be feasible in practice, but it does mean that the ρ<0\\\\rho < 0 is flawed in the absolute limit. Cognitive labour can in principle fully substitute for compute!\n        \n    4.  _Takeaway:_ take values of ρ\\\\rho right next to 0 as seriously as ones in the estimated range.\n        \n    \n3.  **Extrapolating very very far.** Estimates of ρ\\\\rho are often conducted in a setting where L/KL/K only varies by ∼2×\\\\sim 2 \\\\times. But to make predictions about an SIE, we must extrapolate from this range by multiple orders of magnitude. Such extrapolation is truly heroic. The CES function wasn’t designed to cover that range and it’s never been tested in that range! (In addition, as far as I know, the specific functional form of CES isn’t well justified empirically — rather, it’s used because it has nice theoretical properties.)\n    \n    1.  _Takeaway:_ be very wary of using these estimates in the context of an SIE!\n        \n    \n\n![Graph showing heroic extrapolation problem: empirical estimates cover small 'observed range' (gray area) while SIE requires predicting multiple orders of magnitude beyond](https://images.ctfassets.net/4owxfjx3z3if/ekSY06vaNSHeZ1oplxSv0/2d1b5c9879b1fc640954af0ef7d95fff/Amrit_Graphs_-47.png)\n\nImage\n-----\n\nThe SIE involves inputs of cognitive labour rising by multiple orders of magnitude. But empirical measurements of ρ\\\\rho span a **much** smaller range, making extrapolation very dicey.\n\n4.  **Economic estimates don't include labourers becoming smarter or thinking faster.** The empirical estimates look at empirical variation in the ratio of labour to capital, L/KL/K – they account for “more workers” but not “smarter workers”. This is a very big drawback when applying them to the case of an SIE. R&D is very cognitively loaded – smarts help a lot. And the core dynamic of the SIE is that AI is becoming smarter over time (though I expect getting more parallel copies to also play an important role). Economic estimates of ρ\\\\rho don’t speak to that dynamic at all.\n    \n    1.  An upcoming post from Eli Lifland and Dan Kokotajlo surveys 8 AI researchers, who guess that moving from “all researchers as good as the median employee” to “all researchers as good as the top employee” would increase the pace of AI progress by a factor of 6 (median estimate), suggesting that “smarter researchers” is a very big effect within the range of top human experts.\n        \n    2.  Similarly, the economic estimates don’t speak to the effects of AI thinking speed increasing during an SIE.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. I’d expect the gains from “more, smarter, faster workers” to be less bottlenecked than the benefits from just “more workers”.\n        \n    \n5.  **The bottleneck is not compute but ‘number of experiments’, and experiments can become more compute-efficient.** The reason compute is needed for AI progress is that it allows you to run experiments. But when your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed). So during an SIE, labs can increase the quantity of _both_ key inputs: cognitive labour and # experiments. This completely pulls the rug out from underneath the original sceptical argument, which assumed an essential input was held fixed.\n    \n    1.  A smart SIE-sceptic might reply:\n        \n        > Ah, but that only shows you can run more experiments at a fixed capability level. What really matters is the number of ‘near-frontier’ experiments: experiments close in size to the largest training runs, e.g., experiments that use 1% of the lab’s compute. And the number of near-frontier experiments is fixed.\n        \n    2.  But this reply isn’t convincing. Firstly, you might not need near-frontier experiments. You might instead be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute. Secondly, the argument proves too much. Over the past ten years, the number of near-frontier experiments that the world has been able to run has significantly decreased! Training run size has grown much faster than the world’s total supply of AI compute. If these near-frontier experiments were truly a bottleneck on progress, AI algorithmic progress would have slowed down over the past 10 years.\n        \n    3.  _Takeaway:_ raise our estimate of ρ\\\\rho. (Or keep ρ\\\\rho the same, but replace ‘compute’ with ‘# experiments’, which will weaken the case against SIE.)\n        \n    \n6.  **The implied ‘max speed’ for AI software progress implied by the economic estimates of ρ\\\\rho is implausibly low.** An estimate of ρ\\\\rho for AI R&D can be translated into an estimate for the maximum speed AI software could progress at, if compute holds constant but cognitive labour inputs tend to infinity. I.e. “how much faster would algorithms improve if we today dropped in trillions of maximally superintelligent AI researchers today”. The economic estimates of ρ\\\\rho vary from -1 to -0.15, implying max speeds between 2 and 100. I think a max speed below 10 is implausible, which corresponds to ρ<−0.3\\\\rho < -0.3. Below 30 also seems kinda implausible, corresponding to ρ<−0.2\\\\rho < -0.2. In other words, **pretty much all economic estimates of ρ\\\\rho have implausible implications about the max speed.**\n    \n    1.  Wait, where are my claims about max speed coming from? Reasoning about the max speed could be its own post. But my views here are informed by thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labour, like: running smaller scale experiments, optimising every part of the stack, generating way better ideas for new algorithms/paradigms, designing way better experiments, stopping experiments early wherever possible, etc.\n        \n        1.  See [here](https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-at-anthropic#Running_computationally_expensive_ML_experiments_may_be_a_significant_bottleneck_to_rapid_software_progress) for more detail, and also see the upcoming takeoff speeds post from Eli Lifland and Dan Kokotajlo.\n            \n        2.  And notice that these things mostly don’t really have clear analogues in the case of manufacturing (where the economics estimates are from). If you’re running a washing machine factory, “optimising every part of the stack” will just have much smaller gains than in AI, and there’s no clear analogue to ‘running smaller scale experiments’. So again, we shouldn’t be surprised if estimates from manufacturing overestimate bottlenecks.\n            \n        \n    2.  _Takeaway:_ This all leaves me thinking that, most likely, −0.2<ρ<0\\-0.2 < \\\\rho < 0.\n        \n        1.  (I do think there’s a viable route here for an SIE-sceptic to simply bite the bullet and argue that the max speed is not that high – seems worth exploring.)\n            \n        \n    \n7.  **There are routes to improving AI that don’t use compute-intensive experiments.** The CES production function is a “weakest link” production function. If one input stalls, progress halts. But an alternative frame is that there are multiple possible routes to producing superintelligence and you just need one of them to work – a “strongest link” framing. Maybe you can extrapolate from small-scale experiments, maybe you can design way better experiments, maybe scaffolding takes you very far, maybe you can do a data flywheel… The compute bottleneck objection only works if _all_ of these routes are bottlenecked by compute. To put it another way: different sources of AI R&D progress will have different values for ρ\\\\rho. We’ll ultimately use whichever has the most favourable value.\n    \n    1.  This brings us back to Chad Jones’ explanation for why long-term ρ\\\\rho is very close to 0 – we adjust our method of production to whichever option makes the best use of super-abundant cognitive labour.\n        \n    \n\nTaking stock\n------------\n\nOk, so let's take stock. I've given a long list of reasons why I find the economist version of the compute bottleneck objection unconvincing in the context of a software intelligence explosion (SIE), and why I expect ρ\\\\rho to be higher than economics estimates.\_\n\nSo I feel confident that our SIE forecasts should be more aggressive than if we naively followed the methodology of using economic data to estimate ρ\\\\rho. But how much more aggressive?\_\n\nOur recent [report](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) on SIE assumed ρ\\=0\\\\rho = 0, which I think is likely a bit too high. In particular, I suggested above that the most likely range for ρ\\\\rho is between -0.2 and 0. As shown by the following graph, the difference between -0.2 and 0 doesn't make a big difference in the early stages of an SIE (when total cognitive labour is 1-3 OOMs bigger than the human contribution), but makes a big difference later on (once total cognitive labour is >=5 OOMs bigger than the human contribution).\n\n![Sensitivity analysis showing three ρ values: ρ=-0.2 (yellow) plateaus at ~30x, ρ=-0.1 (purple) reaches ~300x, ρ=-0.01 (green) continues growing exponentially](https://images.ctfassets.net/4owxfjx3z3if/5IUU78o4D6zhAc1cfNA39o/fdbe8a4ec82c936b8556c6cb7aced411/Amrit_Graphs_-48.png)\n\nImage\n-----\n\nSensitivity analysis on values of ρ\\\\rho. Within the range\_ −0.2<ρ<0\\-0.2 < \\\\rho < 0, the predictions of CES don't differ significantly until labour inputs have grown by ∼5\\\\sim 5 OOMs. If this is the range of ρ\\\\rho for AI R&D, compute bottlenecks won't bite in the early stages of the SIE.\n\n**This suggests that compute bottlenecks are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress.** Of course, that's just my best guess – it's totally possible that compute bottlenecks kick in much sooner than that, or much later.\n\nIn light of all this, my current overall take on the SIE is something like:\n\n*   Absent a hard compute bottleneck – i.e. assuming ρ\\=0\\\\rho = 0 – it looks like returns to AI software R&D are comfortably good enough for an SIE. And indeed, it's plausible that there's no such hard bottleneck. So an SIE is plausible.\_\n    \n*   But it's also plausible that there is a hard compute bottleneck – ρ<0\\\\rho < 0. If so, it might only have a big effect once we're many OOMs into the SIE (if −0.2<ρ<0\\-0.2 < \\\\rho < 0), or it might kick in sooner (if ρ<−0.2\\\\rho < -0.2, as in most economic estimates).\_\n    \n*   Digging into all this more is one of the most important things for better understanding the prospects for an SIE (with the other contender being researching how far an SIE could go before reaching effective physical limits).\_\n    \n*   Again, this is all very complicated and tricky stuff! I could imagine changing my mind on this on the basis of further research.\n    \n\nIt's hard to know if I actually disagree with Epoch on the bottom line here. Let me try and put (very tentative) numbers on it! I'll define an \"SIE\" as \"_we can get >=5 OOMs of increase in effective training compute in <1 years without needing more hardware_\". I'd say there's a 10-40% chance that an SIE happens despite compute bottlenecks. This is significantly higher than what a naive application of economic estimates would suggest."
    critique: While the author mentions 'months or years' for human to superintelligent AI, the analysis doesn't specify over what timeframe the compute bottlenecks are expected to or not to bind.
