1. The central inference from “many moral views judge our world deeply flawed” to “mostly-great futures are unlikely by default” looks like a non sequitur. Disagreement and historical error show that moral failure is possible, but they do not by themselves establish that the space of high-value futures is narrow rather than broad or that progress is unlikely. The paper often slides from “can go badly in many ways” to “probably will go badly,” without a defensible probabilistic bridge.

2. The paper’s quantitative framework relies on strong decision-theoretic assumptions (completeness, continuity, independence/VNM representability) that many moral theories reject, especially under risk or moral uncertainty. Even if some nonconsequentialist views can be represented with utility, the independence axiom and the permissibility of sharp tradeoffs (e.g., 50–50 eutopia/extinction) are controversial and may distort “fussiness” by construction. This risks turning a substantive thesis (“eutopia is hard”) into an artifact of the chosen formalism.

3. Key definitions (e.g., “best feasible future” as a 99.99th percentile outcome, “near-best” as ≥90% of its value) are arbitrary and may predetermine the “narrow target” conclusion. If feasibility tails are heavy or the distribution is misspecified, percentile-based “best feasible” picks a reference point that can make most outcomes look far away by stipulation. The paper does not justify why these cutoffs track any philosophically or practically significant boundary rather than merely providing a rhetorically striking contrast.

4. The discussion repeatedly assumes a well-informed probability distribution over futures “absent serious coordinated optimization,” but it never specifies how such a distribution could be grounded or why it should be stable across worldview shifts and technological regimes. Without a model of default dynamics (institutions, incentives, selection effects, learning, path dependence), the claim that mostly-great futures are rare is underdetermined: different plausible priors could make the “default” highly convergent toward welfare-enhancing norms. The argument risks being a catalog of possibilities rather than evidence about likelihoods.

5. The “multiplicative product of factors” model is presented as an explanatory backbone, but the independence and uniformity assumptions are both highly contestable. Many purported “dimensions” (e.g., autonomy, wellbeing, rights, diversity) may be positively correlated under abundance, governance improvements, and moral learning, which would make high overall value less rare than the toy model suggests. Treating value as a product also bakes in a strong substitutability/fragility structure (one low factor collapses value) that many theories deny.

6. Several “moral catastrophe” examples conflate “controversial moral dispute” with “catastrophe-level value loss,” which risks question-begging against more pluralist or tolerance-friendly metaethics. Listing religious error, sexual norms, socialism, and environmentalism side-by-side treats each as comparably weighty evidence of fragility, but these views disagree radically about stakes and about whether the purported wrong is centrally value-determining. The paper uses diversity of moral condemnation as evidence of narrowness, but it could equally support the opposite conclusion: that many futures will be acceptable by at least some reasonable standards.

7. The scale arguments often presume that larger civilization size is straightforwardly available and morally required on “linear” or total views, while underplaying classic objections from population ethics and person-affecting intuitions. For many plausible positions, “bigger is better” is not obviously a moral mandate, and the paper treats resistance to scope-sensitivity as a cognitive bias rather than a substantive ethical stance. This framing risks stacking the deck by labeling dissenting intuitions as “misguided” rather than engaging the literature on average utilitarianism, person-affecting views, or variable value axioms.

8. The paper’s empirical/technological claims (e.g., likely near-term explosive space settlement, “capturing essentially all resources,” digital beings outnumbering biological beings) are speculative and often asserted with high confidence despite deep uncertainty in astrobiology, propulsion, governance, and AI timelines. If these assumptions fail, much of the “resource linearity” and “one part in 10^22 matters” reasoning becomes irrelevant or at least far less action-guiding. The argument would benefit from sensitivity analysis showing how conclusions change under slower expansion, limited controllable cosmos, or strong coordination constraints.

9. The “fat-tailed value-per-resource” premise is doing heavy lifting to claim that only very specific uses of resources yield most value, yet the analogy to wealth/citations is weak for moral value. Value might be smoother, more redundant, or more saturating than these social distributions, especially if many diverse lives are comparably good rather than dominated by rare peak experiences. Additionally, the move from “fat tails exist in some domains” to “future moral value-efficiency is fat-tailed enough to make mostly-great futures rare” is an unsupported extrapolation.

10. The treatment of bounded views and aggregation (separate vs joint goods/bads) risks oversimplifying the space of axiologies and may generate “fussiness” through stylized functional forms rather than necessity. For instance, the claim that separate aggregation implies that one bad part in 10^22 blocks “mostly-great” depends on a particular symmetry, scaling, and normalization choice; alternative bounded or thresholding frameworks (lexical priorities, pluralistic aggregation, sufficientarian constraints) could avoid both “scale tipping” and “near-zero tolerance for bads.” The paper portrays easygoingness as a “narrow slice,” but it does not convincingly rule out many non-utilitarian, pluralist, or satisficing theories that could make high-value futures comparatively robust.