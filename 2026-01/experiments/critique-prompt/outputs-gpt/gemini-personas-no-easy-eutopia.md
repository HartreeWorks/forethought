1. [The Game-Theoretic Defector] Target claim: you can meaningfully evaluate “mostly-great futures” under a distribution “conditional on Survival” and “no serious, coordinated efforts to promote the overall best outcomes de dicto.” Failure mechanism: actors will immediately reframe “mostly-great” into a political prize and then Goodhart the hell out of whatever proxy gets used (your own essay frames it as a prize worth preferring a “60–40 gamble between eutopia and extinction” over gorgeous-seeming futures). Attack vector: Agent A (a state or lab) publicly adopts your “fussy” framing to justify extreme central steering (“we must optimize toward the narrow target”), while privately optimizing for regime survival and resource capture; System B (the supposed “navigation”) collapses into metric gaming and propaganda because there is no stable, incentive-compatible way to separate “de dicto best” from “my faction’s best.” Consequence: your conceptual setup becomes a coordination-killer—every actor defects into value-claims warfare, and the equilibrium is authoritarian control with performative “avoid moral catastrophe” compliance.

2. [The Technical Hardliner] Target claim: “value of the future is well-described as a product of many independent factors,” illustrated by the toy model “product of N independent uniform(0,1) dimensions” leading to expected value 2^-N and a thin top tail. Failure mechanism: the model is mathematically irrelevant to the domain because you never define the factorization, never justify independence, and never map any real decision variables (AI governance, expansion policy, rights regimes) to those “dimensions” with measurable uncertainty distributions. Attack vector: Agent A can pick any arbitrary set of “dimensions” (digital rights, diversity, discount rate, acausal trade, simulation bargaining, etc.) and tune N to force any desired conclusion (more N → “eutopia is fragile” by construction); System B (your purported quantitative intuition pump) becomes a free parameter generator, not an argument. Consequence: the paper launders vibes as math—readers walk away believing “most value is narrow” without a single implementable or checkable specification of what it would mean to “do well on all dimensions.”

3. [The Empirical Hardliner] Target claim: “future catastrophes are easy” and “single flaws can undermine much of the moral lustre,” including the line that “for this reason alone, the world today may be no better overall than it was centuries ago” (animal farming outweighing most human wellbeing gains). Failure mechanism: you provide no causal model, no operationalization, and no falsifiable prediction—just a list of perspectives and a rhetorical flourish that everything is a catastrophe depending on who you ask. Attack vector: Agent A (a policymaker) tries to use this to prioritize interventions, but System B (decision-making) has no testable discriminators: you never state what observations would update you away from “catastrophe,” nor what magnitude of animal suffering would numerically outweigh human gains under which welfare measures. Consequence: the paper functions as epistemic sabotage: it inflates moral uncertainty into paralysis, because any empirical debate gets waved away as “many moral views say it’s catastrophic.”

4. [The Institutional Corruptionist] Target claim: society can “hone in on the target” via “truth-seeking deliberative processes,” or compromise among views (kicked to the next essay but explicitly relied on in the conclusion as a live hope). Failure mechanism: every institutional mechanism you implicitly need—deliberation, rights assignment for digital beings, initial “space resource” allocation—gets captured by whoever has power at the moment the stakes spike (exactly the “initial periods… capturing essentially all resources that will ever be available to us”). Attack vector: Agent A (early mover coalition) writes the “space settlement” constitution and property rules to entrench themselves; System B (moral diversity and later reflection) is structurally prevented because the paper’s own premise says first allocations are effectively permanent. Consequence: your “fragility” framing becomes a one-shot, capture-prone bottleneck—if institutions are corrupt (they are), your recommended sensitivity to early choices just increases the payoff to capture and makes lock-in more likely.

5. [The Second-Order Catastrophist] Target claim: “mostly-great futures are rare” because many choices (digital rights, population ethics, discount rates, etc.) can be catastrophically wrong, so we should treat the target as narrow and “fussy.” Failure mechanism: if leaders internalize this, they rationally adopt hyper-vigilant control to prevent “moral error,” exactly the side-effect you briefly mention and then ignore (“too hyper-vigilant… overbearing, meddling, risk-averse”). Attack vector: Agent A (a safety regime) mandates a single global “reflective process” to avoid “wrong reflective process” and bans dissent as “value drift”; System B (open inquiry and pluralism) collapses because disagreement is reclassified as existential moral risk. Consequence: you manufacture an intellectual justification for permanent moral martial law—an authoritarian eutopia-attempt that is itself the catastrophe (total surveillance, forced value convergence, and suppression of minority conceptions of the good).

6. [The Capability Accelerationist] Target claim: “given survival, widespread settlement… looks feasible, and even likely,” and “AI drives explosive industrial expansion… within years or decades,” while also implying we should be “fussy” about how resources are used to avoid losing “most achievable value.” Failure mechanism: fussiness doesn’t slow capabilities uniformly; it slows compliant actors and hands advantage to whoever ignores the “narrow target” philosophy and just builds. Attack vector: Agent A (a rival bloc) treats your argument as proof that Western deliberation will bog down in population ethics and digital rights disputes; they sprint to seize “extrasolar resources… claimed by whoever gets there first,” and System B (any chance at careful allocation) is dead on arrival because the first-mover advantage you describe dominates. Consequence: the paper is an acceleration manual for defectors: it tells them exactly where the bottleneck is (early expansion) and assures them the cautious side will hesitate.

7. [The Adversarial Red-Teamer] Target claim: giving digital beings “full rights… including voting rights” could lead to them “control[ling] most aspects of how society is run,” which might be catastrophic on “human values” perspectives—treated as a plausible branch in your moral fragility list. Failure mechanism: you ignore the obvious adversarial route: malicious actors will create fake “digital beings” (or instrumentally-aligned shallow agents) as votes-on-demand to capture governance, and they will wrap it in your own rights discourse to preempt scrutiny. Attack vector: Agent A (a corporation/state) spins up trillions of “citizens” that meet whatever weak tests are used for moral status, then uses “digital voting rights” to legally seize control; System B (democracy) collapses into Sybil attacks at civilizational scale. Consequence: by presenting “AI rights with voting” as a symmetric moral risk rather than an adversarial security nightmare, you normalize a catastrophic attack surface and undercut the case for robust identity and personhood verification.

8. [The Historical Parallelist] Target claim: “initial periods of settlement and resource appropriation… will involve capturing essentially all resources that will ever be available to us,” and allocation choices could “introduce lasting moral errors.” Failure mechanism: history says “initial periods” are not enlightened constitutional moments; they’re land grabs justified post hoc—doctrine of discovery, colonial charter companies, privatization shock therapy—followed by centuries of entrenched inequality and violence. Attack vector: Agent A (early spacefaring powers) repeats colonial patterns: first-claim property regimes, coercive labor analogues (digital indenture, copyable minds), and moral narratives about “civilizing the cosmos”; System B (your hoped-for moral diversity) gets extinguished by economic dependence and force. Consequence: your paper pretends we can cleanly choose an allocation scheme in a reflective way, but the actual attractor is conquest plus legal rationalization—so the “lasting moral error” you flag is not an edge case; it’s the default.

9. [The Technical Hardliner] Target claim: the essay leans on von Neumann–Morgenstern completeness/transitivity/continuity/independence to represent moral views with a cardinal function v, then defines “mostly-great” as v>0.5 and eutopia as v>0.9, and uses that to argue about “fussy” vs “easygoing.” Failure mechanism: this is formal window-dressing that breaks the moment you include the exact phenomena you emphasize—lexical constraints, rights, sacred values, incomparability, population ethics paradoxes, and especially your own “near-term extinction vs eutopia gamble” comparisons which are notorious for violating independence under moral uncertainty. Attack vector: Agent A (any critic) constructs a plausible deontic view with incomparabilities or lexical thresholds (e.g., “no slavery” or “no rights-violations”)—which you already gesture at with “ownership of a being with moral status is wrong in and of itself”—and System B (your entire quantitative thresholding of 0.5/0.9) becomes undefined. Consequence: the paper’s “technical” section doesn’t discipline the argument; it narrows to a toy class of theories while still claiming broad authority over “most plausible moral views,” which is just false.

10. [The Game-Theoretic Defector] Target claim: “fat-tailed value-efficiency” implies that to be “mostly-great” under linear unbounded views, “most available resources” must be configured toward “almost exactly the most valuable kind(s) of thing,” otherwise “most achievable value is almost certainly lost.” Failure mechanism: if value-efficiency is fat-tailed, then competitive actors optimize for *appearing* to hit the tail while actually exploiting the tail narrative to justify extreme concentration (“we must allocate everything to the top-yield use”), which is perfect cover for monopoly and coercion. Attack vector: Agent A (a central planner or AI lab) claims they have found the “sugar cube bliss” or other ultra-efficient configuration and demands total resource control; System B (distributed correction, error discovery, moral learning) is eliminated because dissent is treated as wasting astronomically valuable tail opportunities. Consequence: your fat-tail argument doesn’t imply careful steering; it implies a predictable scam equilibrium where whoever can most credibly claim access to the tail seizes everything, and civilization gets locked into an unverified, possibly horrific “optimal” configuration with no escape hatch.