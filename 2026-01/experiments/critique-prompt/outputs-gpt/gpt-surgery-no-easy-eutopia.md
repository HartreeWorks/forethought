1. The inference “**Non-obvious but severe flaws are … the norm across history and across moral views**” (Sec. 2.1) is used as a load-bearing bridge to “therefore a single flaw could undermine much of the moral lustre of the future” (Sec. 2.1–2.3). **Attack type: Reference class sabotage.** The historical cases you cite (slavery, disenfranchisement, brutal punishment) are not a random draw from “possible future moral mistakes”; they are clustered around coordination failures and domination under scarcity, weak institutions, and low epistemic capacity. A plausible counter-history is that once a society crosses certain thresholds (education, wealth, rights enforcement, global information flow), “catastrophes” of that specific kind become systematically harder to sustain even if people’s moral beliefs remain imperfect—meaning “moral catastrophe was common” doesn’t generalize to “moral catastrophe will remain easy under abundance + strong institutions.” If this critique holds, you’d need to replace the rhetorical historical survey with an argument that *future* moral errors are drawn from a reference class that remains high-frequency *after* the key civilizational transitions you implicitly assume in common-sense utopia.

2. The claim “**the suffering directly caused by animal farming may well be enough to outweigh most or all of the gains to human wellbeing… For this reason alone, the world today may be no better overall than it was centuries ago**” (Sec. 2.1) is doing more than scene-setting: it underwrites “even worlds everyone endorses can be morally catastrophic.” **Attack type: Dominant alternative.** A simpler explanation is that this conclusion is driven by adopting a moral aggregator that is extremely sensitive to large-N suffering (e.g., strong prior on totalist suffering-weighting), rather than by a general fact about “how easy moral catastrophe is.” If the “ongoing catastrophe” diagnosis mainly tracks one parameter choice (how to trade off nonhuman suffering against human gains, plus population aggregation), then it doesn’t support your later claim that *most plausible* views are fussy—only that *some* plausible views are. If this critique holds, you’d need to explicitly show that across a wide set of moral weightings and aggregation rules, a single domain like factory farming robustly flips the sign or crushes the value ranking, rather than leaning on one particularly leverageable moral premise.

3. Your “**Common-sense utopia**” construction (Sec. 2.2) is used as a test case for “even if everyone gets most of what they want under abundance, value can still be far below potential” (Sec. 2.3). **Attack type: Hidden parameter.** The vignette builds in “do almost anything… as long as they don’t harm others” and “collaboration and bargaining replace war,” which already presuppose a highly competent harm-accounting system and stable governance that prevents externalities—exactly the kind of steering capacity that would also plausibly correct several of your later “easy-to-get-wrong” items (digital welfare, rights, bads minimization). In other words, your own definition of the utopia tacitly includes the navigation system you say you are bracketing (“no serious coordinated efforts”), making the scenario less of a “default” and more of a “partially solved alignment/governance” world. If this critique holds, you’d need to either (a) weaken the common-sense utopia assumptions to match your “no serious optimization pressure” condition, or (b) concede that once societies can robustly enforce “no harm to others” at scale, many of the claimed fragilities become less probable, shrinking the “no easy eutopia” gap.

4. In Sec. 2.3.1 you treat “**scale-insensitivity**” as a major way common-sense utopia “could lose out on most value,” e.g., “a galaxy’s worth of flourishing could be billions of times more valuable.” **Attack type: Reversal.** Under many plausible decision procedures and political economies, strong scale-sensitivity can drive expansion races, resource appropriation, and conflict externalities that *increase* the probability of catastrophic lock-in or suffering (including digital exploitation), making the *easygoing* “small but high-quality” path higher expected value than aggressive scaling. That is, the same mechanism (“scale matters hugely”) can imply the opposite policy conclusion: don’t push scale because scaling amplifies tail risks and moral hazards. If this critique holds, you’d need to integrate an endogenous-risk model where the pursuit of scale changes the distribution over moral catastrophes, rather than treating “more scale” as a near-free multiplier on value conditional on survival.

5. The move “**There are therefore many ways in which the future could lose out on most value for population-ethical reasons**” (Sec. 2.3.1) is leveraged to support the “narrow target” picture. **Attack type: Countermodel.** Consider a world where (i) institutions commit to pluralistic “moral parliament” governance, (ii) population choices are made via reversible, compartmentalized experiments (e.g., different habitats implementing different population ethics), and (iii) inter-habitat migration and value-trade are allowed. In that world, population-ethics disagreement doesn’t force a single global irreversible choice; it becomes an arena for diversification and compromise, making it much harder for population ethics to zero out “most value” across the whole future. Your argument treats population ethics as a single-shot, globally coupled decision, but a plausible future governance architecture makes it a decoupled portfolio problem. If this critique holds, you’d need to argue that (a) population ethics decisions are inherently globally coupled by physics (e.g., irreversible resource capture), and (b) portfolio approaches cannot preserve most of the value on most plausible moral views.

6. In Sec. 2.3.2 you argue digital beings are a major “moral catastrophe” axis: too few rights → exploitation; full rights → humans lose control and “human values” may be lost. **Attack type: Strategic response.** Your framing implicitly assumes political power maps to headcount (voting rights) and that governance can’t discriminate between entity types without moral error, but future actors will strategically design the franchise and ontology to preserve their interests (e.g., citizenship tests, constitutional constraints, weighted voting, nonpersonhood design, or deliberate creation of nonvoting labor minds). That strategic adaptiveness makes the “either exploit them or they take over” dichotomy non-load-bearing, and it also undercuts the “easy to accidentally introduce” claim: the system will be intentionally engineered around this exact fault line. If this critique holds, you’d need to replace the binary with an equilibrium analysis: given incentives, what governance arrangements around digital beings are stable, and do they actually concentrate probability mass on low-value outcomes rather than on engineered compromises?

7. The centerpiece “**value is the product of many factors** … ‘as N increases… expected value shrinks’” toy model (Sec. 2.4) is doing heavy lifting for “mostly-great futures are rare.” **Attack type: Hidden parameter.** The conclusion depends on (i) near-independence of factors, (ii) roughly uniform uncertainty over each factor, and (iii) multiplicative aggregation rather than min/lexicographic or saturating aggregation. A plausible alternative is strong positive correlation and governance-driven coupling: societies that solve “nonharmful freedom + cooperation” are also more likely to solve “digital welfare + bads minimization,” causing the joint distribution to have much more mass near the top than independent uniforms imply (the product model is maximally pessimistic about correlation structure). Moreover, many moral theories don’t treat “one bad dimension” as proportionally shrinking value; they treat certain constraints as side-constraints (pass/fail) or allow compensation, which yields a distribution shaped nothing like a product of uniforms. If this critique holds, you’d need to either justify why independence/uniformity is the right uncertainty model for future governance choices, or redo the fragility argument under correlated, clustered competence models and show the “rare mostly-great” result survives.

8. The argument in Sec. 3.2 that on **unbounded linear views** “to be mostly-great, almost all resources must be used and put toward some very specific use” relies on “fat-tailed value-efficiency” and the existence of a “single value-efficient arrangement” to replicate. **Attack type: Countermodel.** Imagine value-efficiency is *broad-plateaued* because many microphysical instantiations yield near-maximal welfare (e.g., any sufficiently advanced architecture can realize extremely high-quality experience), and the fat tail comes from rare *failures* (extreme suffering) rather than rare *bonuses* (extreme bliss). Then “most uses” aren’t orders of magnitude worse than the best; instead, a wide set of designs cluster near the top, and the tail risk is mostly about avoiding specific traps—flipping your conclusion from “must aim at a specific use” to “avoid specific anti-patterns.” Your analogies (wealth, citations, consumer surplus) don’t establish that *intrinsic* value-per-resource over future mind designs is fat-tailed in the relevant direction (huge upside gaps vs broad plateau). If this critique holds, you’d need to show that (a) near-max value is not a plateau for plausible mind designs, and (b) the tails are dominated by a few extremely high-value configurations rather than by “many good-enough” configurations.

9. In Sec. 3.3 you argue “**bounded views of universal value are approximately linear in practice**” because “Earth’s difference is tiny vs universe,” so concave functions look linear locally. **Attack type: Hidden parameter / Reversal.** This hinges on a cosmological premise (vast/infinite universe with many alien civilizations) *and* on the moral view caring about total universal value rather than agent-relative or region-relative value. But if the universe is causally disconnected beyond some horizon, or if moral relevance is restricted to the affectable region (common in practical ethics), then humanity’s marginal impact is not “tiny” in the relevant domain, and boundedness can be materially non-linear—making easygoing bounded views far less “narrow and specific” than you claim. Even granting a huge universe, many moral views treat “what we do” as evaluatively central irrespective of unobserved aliens (partiality, responsibility-sensitive views), again breaking the “approximately linear” step. If this critique holds, you’d need to explicitly condition the linearization argument on a metaethical assumption about global impartial aggregation over the total universe, and show your “most plausible views” actually accept that assumption rather than sliding it in via cosmology.

10. Your “**separate aggregation bounded views imply extreme fussiness: one part in 10^22 bad prevents mostly-great**” (Sec. 3.3) depends on treating “bads” as additively scaling with resources and on a specific mapping from “mostly-great = >0.5 of bound” to “a star-system’s worth of bad hits >0.5 of disvalue bound.” **Attack type: Quantitative cliff.** This is a threshold artifact of assuming the disvalue bound is reached at roughly “one common-sense utopia scale” of bads; but many bounded/separately aggregating views set the concavity so that disvalue saturates only after *enormous* amounts of bads (or weights bads sublinearly once protections exist), precisely to avoid “one speck ruins everything” implications. Once you allow the saturation scale (where additional bads add little marginal disvalue) to be a free parameter, your “1/10^22” brittleness no longer follows; it can easily become “tolerate a nontrivial fraction of bads without losing mostly-great,” which would make these views much less fussy than claimed. If this critique holds, you’d need to justify a principled choice of saturation scale for bads (not just “matches our intuitions”) and show that under those principled parameterizations, the extreme sensitivity result persists rather than being a tunable artifact.