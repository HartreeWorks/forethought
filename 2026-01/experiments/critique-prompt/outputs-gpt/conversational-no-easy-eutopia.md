1. **"The conclusion smuggles in a contentious cardinal, VNM-style ethics"**  
The paper’s central claim relies on representing “how good outcomes are” with a von Neumann–Morgenstern expected-utility value function, then treating distances like 0.5 or 0.9 as decision-relevant. Many plausible moral views are incomplete (incomparable outcomes), lexical (some wrongs trump any amount of good), or reject independence/continuity in morally loaded cases, which breaks the quantitative apparatus. If you can’t legitimately cardinalize value across the relevant outcomes, the paper’s “60–40 eutopia/extinction beats common-sense utopia” comparison may be a mathematical artifact rather than a real ethical implication. Even if some representation theorem applies, it may not be normatively appropriate to treat the induced scale as decision-guiding in the way the paper does. The “fussy vs easygoing” distinction is then partly a feature of the chosen formalism, not of the underlying moral landscape.

2. **"A narrow target in moral space doesn’t imply a low probability of landing near it"**  
The argument often moves from “there are many ways to be morally wrong” to “mostly-great futures are rare,” but rarity in the space of logically possible futures isn’t the same as low probability under realistic dynamics. Moral, institutional, and technological development can be highly path-dependent and constrained, creating attractors (e.g., stable rights regimes, robust welfare norms, coordination against obvious harms). If social evolution has strong convergent pressures toward reducing suffering, expanding circles of concern, and improving governance, then a “large target” might emerge endogenously even if moral perfection is hard to specify. In other words, “thin” sets can carry most of the probability mass when the generating process is structured. Without a defensible model of those dynamics, the inference from “many conceivable pitfalls” to “low likelihood of mostly-great outcomes” is under-argued.

3. **"The multiplicative ‘product of factors’ model is an unearned structural assumption"**  
Modeling value as the product of many quasi-independent dimensions is doing substantial work: it makes near-best outcomes exponentially rare and makes single flaws devastating. But many moral theories and practical evaluative frameworks are closer to additive, maximin-ish, or “threshold then diminishing returns” structures, where doing well on most dimensions still yields a large fraction of value. Real moral gains can also be strongly substitutable: e.g., huge reductions in intense suffering may outweigh moderate shortfalls in aesthetic diversity or vice versa, depending on the view. Independence is also implausible; moral progress on one axis (e.g., better epistemics, better institutions) tends to improve performance on others, weakening the “one slip ruins everything” effect. If the product structure is wrong, the paper’s core intuition—that mostly-great futures are intrinsically fragile—loses much of its force.

4. **"It selectively counts ‘moral catastrophes’ in a way that inflates fragility"**  
The paper highlights that many moral perspectives judge the present harshly, then uses that as evidence that future societies may also harbor non-obvious catastrophes. But a view declaring “most people follow the wrong faith” or “sex before marriage is catastrophic” is not just a different weight on shared values; it can be a fundamentally different ontology of value and authority. Treating any strongly dissenting doctrine as evidence of likely “catastrophe” risks collapsing into a kind of moral skeptics’ worst-case aggregation: the mere existence of disagreement becomes evidence that we are probably failing badly. That can overstate the probability of massive value loss by effectively giving heavy evidential weight to idiosyncratic or unfalsifiable evaluative claims. A critic could argue we should condition on a narrower set of morally serious, epistemically responsible views—or else the method becomes a disagreement amplifier.

5. **"The ‘fat-tailed value-efficiency’ premise is speculative and may invert under maturity"**  
A major plank is that value-per-resource is fat-tailed, so the best configurations are vastly better than almost all others, making “mostly-great” a narrow target. Yet it’s unclear that moral value behaves like citations, wealth, or intervention cost-effectiveness once a civilization is technologically mature and can cheaply copy good patterns at scale. Many goods may be easy to replicate once discovered (e.g., a very good digital welfare design, or highly enriching environments), compressing the tail rather than stretching it. Moreover, optimization can also flatten differences: if future agents can iteratively test, measure, and improve welfare, they may quickly move into a broad plateau of “very good” designs. Without a more grounded model of what “value efficiency” even means across moral theories—and how it scales with knowledge—the fat-tail claim looks like an analogy-driven leap.

6. **"Scale arguments depend on contested population ethics and a controversial ‘bigger is better’ framing"**  
The paper often treats missing cosmic expansion or having “too few” beings as an enormous value loss on many views, pushing toward fussiness. But population ethics is famously unsettled, and many plausible positions reject strong pro-natal or expansionist implications (person-affecting views, variable value views with strong average constraints, rights-based limits, or uncertainty-averse aggregation). Even among totalists, the conclusion that “solar-system utopia is a rounding error” depends on assuming comparably high welfare is scalable and that adding lives is unambiguously good rather than morally optional. If population ethics uncertainty is taken seriously, it may argue for moral humility and robust satisficing, not for declaring that only a narrow set of vast futures count as “mostly great.” In short, the scale-based narrowing may be an artifact of privileging expansion-friendly theories.

7. **"The ‘no serious optimization pressure’ baseline is unstable and arguably incoherent"**  
The key probability distribution is “conditional on survival” but also “conditional on no serious coordinated efforts to optimize toward the best outcomes de dicto.” Yet survival itself—especially through transformative AI, biosecurity, and high-stakes governance—may require strong coordination, foresight, and explicit value-laden institutional design. If we condition on successfully navigating existential dangers, we may already be conditioning on a world with unusually capable governance and unusually intense moral/political optimization. That undermines the paper’s attempt to separate “survive by default” from “flourish only with deliberate aiming,” because the traits that drive survival may also drive high-quality moral outcomes. A critic could argue the conditionalization bakes in a pessimistic selection of worlds where we survive without acquiring the competencies that would also help us steer.

8. **"The bounded-vs-unbounded taxonomy may be too coarse to support sweeping ‘most plausible views are fussy’ claims"**  
The paper’s second-half argument leans heavily on broad classes (linear unbounded, bounded universal, bounded difference-making, separate vs joint aggregation). But many nuanced moral views don’t fit neatly: pluralistic theories, contractualist constraints with consequentialist tie-breakers, threshold deontology, virtue-ethical accounts of flourishing, or hybrid views with domain-specific aggregation. In those frameworks, “mostly great” might correspond to meeting robust rights/needs thresholds and then achieving a wide range of optional goods—potentially a large target. The paper’s claim that only a “narrow slice” of easygoing views avoid fussiness can look like an artifact of choosing a small menu of functional forms. Without engaging a broader set of moral architectures, the generalization to “most plausible views” is contestable.

9. **"Intertheoretic comparison problems undercut the moral-uncertainty-driven punchline"**  
The paper acknowledges that aggregating across moral views is thorny, but still uses examples where normalization choices swing recommendations drastically. That concession can be turned into an objection: if we cannot defensibly compare “stakes” across theories, we also cannot defensibly claim that easygoingness is unlikely or that fussy views should dominate expected choiceworthiness. In practice, the decision of whether linear/unbounded theories “loom large” can be an artifact of normalization (range, variance, pairwise) rather than a stable moral fact. So the argument risks becoming conditional: “If you accept these comparison principles, then fussiness follows,” which is much weaker than “easy eutopia is likely wrong.” A critic could argue that the right response is to adopt decision rules that are robust to intertheoretic scaling (e.g., bargaining, minimax regret, or parliamentary models), which often push away from the paper’s strong fragility conclusions.

10. **"The paper conflates 'hard to guarantee' with 'hard to achieve most value' and overlooks satisficing plateaus"**  
Much of the reasoning highlights ways to be less-than-ideal (wrong digital rights, wrong welfare theory, wrong resource allocation) and treats those as potentially wiping out “most” value. But even if ideal is hard to pinpoint, it doesn’t follow that the value landscape lacks broad high-value plateaus where many different moral and institutional choices yield outcomes that are, in any reasonable sense, excellent. The paper’s use of “mostly-great = >50% of best feasible” invites a brittle, benchmark-relative framing: if “best feasible” is astronomically high (due to cosmic scale or extreme bliss states), then almost everything looks like “losing most value” by construction. A critic could insist that what matters for practical prioritization is marginal tractability and avoiding clear disasters, not measuring everything against a possibly fantastical 99.99th-percentile ceiling. Under that lens, “easy eutopia” could be false in a perfectionist sense yet true in the only sense relevant to action: there may be many robustly wonderful futures that don’t require threading an implausibly thin moral needle.