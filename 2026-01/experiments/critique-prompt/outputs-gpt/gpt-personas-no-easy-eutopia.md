1. [The Empirical Hardliner] Target claim: your headline inference that “eutopia is fragile” because “single moral errors… erase a major fraction of the goodness of the world,” supported by the “ongoing moral catastrophes” list and historical analogy. Failure mechanism (measurement/identification failure): you never specify an observable criterion that distinguishes “we’re in a moral catastrophe” from “we’re merely below the best feasible future,” because across your own table (religious views, conservative morality, pro-life, environmentalism, communism), the label is driven by theory-relative thresholds rather than an identifiable welfare functional. That makes the key empirical premise—“large value is easy to accidentally forfeit”—non-discriminatory: any society can be called a catastrophe by some plausible-seeming view, so your evidence cannot update the probability that the future will be <50% of value conditional on “Common-sense utopia”-like material conditions. Consequence: the argumentative move from “many views condemn the present” to “mostly-great futures are rare by default” collapses into a semantic artifact of letting “catastrophe” track disagreement, not a claim about the underlying distribution of attainable outcomes.

2. [The Moral Parliament Dissenter] Target claim: the paper’s quantitative framing—“it’s rational to prefer a 60–40 gamble between eutopia and extinction over a guarantee of futures that seem wonderful”—using a VNM-representable value function anchored at v(extinction)=0 and v(best feasible)=1. Failure mechanism (normative incoherence / value aggregation contradiction): you smuggle in a “stakes calibration” across moral theories precisely where your own Section 3.5 admits there is no non-arbitrary intertheoretic scale; yet your intuitive shock test depends on that calibration being meaningful (otherwise “60–40” has no determinate normative bite). If a reader’s moral uncertainty includes incomplete/lexicalist views (which your setup explicitly excludes), then your “prefer the gamble” claim becomes undefined, not counterintuitive—so the rhetorical pressure you use to motivate “no easy eutopia” is parasitic on a controversial representability assumption. Consequence: you can’t simultaneously (a) rely on the “gamble vs wonderful future” intuition pump to motivate fussiness and (b) later concede that normalisation choices radically flip recommendations; the core “this should worry you” implication becomes an artifact of your chosen normalisation, not a robust conclusion.

3. [The Mechanistic Alignment Skeptic] Target claim: “we can model value as the product of many relatively independent factors,” illustrated by the toy model of N independent Uniform(0,1) dimensions, leading to “mostly-great futures are rare even among futures which score highly across factors on average.” Failure mechanism (hidden coupling / systems interaction): the specific “moral catastrophe” dimensions you enumerate—digital being rights, population ethics implementation, space resource allocation rules, views of wellbeing—are not independent knobs; they are coupled through the same optimisation substrate (advanced AI/automation and institutional control). In plausible futures, the mechanism that improves one dimension (e.g., aggressive expansion to capture 10^22 stars, or strict governance to eliminate “bads”) predictably worsens others (e.g., moral diversity, autonomy, political capture risk), producing structured correlations that can *increase* mass near a few attractors rather than spread probability into the low-product tail the Uniform-independence picture suggests. Consequence: your quantitative intuition (“as N grows, most mass near zero”) is not just approximate—it can be directionally wrong if the future has strong attractor basins; then “fragility” is not a generic implication of many dimensions, but a contingent claim about correlation structure that you don’t model.

4. [The Game-Theoretic Defector] Target claim: you repeatedly rely on “given survival and material abundance, society could still ‘accidentally’ make a single moral error” (e.g., digital beings treated as property, first-come-first-served extrasolar appropriation) in a way that loses most value. Failure mechanism (incentive incompatibility / equilibrium shift): in the very futures where resources are abundant and scalable (your 10^22-star framing), the equilibrium is dominated by actors who can seize, replicate, and defend control—meaning the default is not “accidental moral error” but *strategic* lock-in by whoever wins early capability contests. Your “no serious coordinated efforts to promote the very best outcomes de dicto” conditioning is irrelevant, because de re incentives (power, expansion, security) already create optimisation pressure toward extreme concentration and aggressive replication—exactly the regimes where minority “moral error” is not corrigible by later reflection. Consequence: the paper understates the severity and *direction* of default selection: your list of moral failure modes are not independent “risks,” they are equilibrium outcomes, so the realistic conclusion is not “mostly-great is narrow,” but “your reference class for default futures is systematically biased toward the values of early winners,” which undermines your later hope that convergence/compromise can rescue anything.

5. [The Institutional Corruption Realist] Target claim: in 2.3.2–2.3.4 you suggest future societies could deliberate about digital rights, voting rights, and space governance, and might “get it wrong,” but the framing presumes a meaningful political system capable of implementing whichever moral answer is found. Failure mechanism (adversarial adaptation / Goodhart): any governance regime that tries to encode “minimal suffering among nonhuman animals and non-biological beings” or “no ownership of morally significant digital beings” becomes a compliance target for entities whose existence/agency is itself in dispute. The moment rights attach to “digital beings,” the highest-leverage move for power-seekers is to manufacture vast populations of borderline-qualifying entities to capture votes, resources, or legal standing; conversely, the highest-leverage move for exploiters is to engineer systems that *look* non-sentient to audits while extracting labour. Consequence: your “digital rights” dilemma is not a philosophical coinflip; it is a regulatory-attacker dynamic where either policy becomes gameable at scale, turning your proposed moral safeguard into an instrument for either takeover (vote inflation) or mass invisible exploitation (sentience evasion).

6. [The Security Engineer] Target claim: “Common-sense utopia” includes “Scientific understanding and technological progress move ahead, without endangering the world” and “Collaboration and bargaining replace war and conflict,” and you treat catastrophic moral failure as mostly about value mis-specification, not active adversaries. Failure mechanism (adversarial adaptation / Goodhart): the combination of galaxy-scale resources and fast-growing digital populations implies an attack surface where even a tiny probability of a hostile actor (or misaligned optimiser) per star system yields near-certainty of adversarial incidents somewhere, and “no bads” standards in Section 3.3 become unattainable under realistic threat models. Your own separate-aggregation argument makes this devastating: if one part in 10^22 being “bad rather than good” blocks mostly-great, then any security model with nonzero compromise rates implies “mostly-great” is effectively impossible, not merely rare. Consequence: the paper accidentally proves a stronger claim than you acknowledge—under adversarial conditions, your “fussy” moral views imply that scaling civilization *inevitably* disqualifies it from mostly-great, which collapses your later attempt to keep “no easy eutopia” from implying “eutopia is unlikely.”

7. [The Capability Externalist] Target claim: you argue that on linear views “future civilisation needs to control most accessible resources” and must configure them in “very specific” high value-efficiency arrangements, otherwise “most achievable value is lost,” motivating fussiness. Failure mechanism (hidden coupling / systems interaction): your own recommendation gradient (take linear/unbounded seriously, prioritise capturing and optimising resources) increases competitive pressure to expand, which accelerates AI-capability development and militarises space settlement—precisely the dynamics that raise extinction and lock-in risks. In other words, the “linear view implies we should treat a solar-system utopia as ~0.00000005 of best” is not a neutral moral observation; it is a policy attractor that couples moral theorising to an arms race over the affectable universe. Consequence: if influential actors buy your linear-fussiness framing, they may rationally prefer reckless upside-seeking (your own 3.5 table of “Upside-focused option” vs “Safety-focused option”), pushing the world toward the very catastrophic tail your introduction contrasted against flourishing—your argument becomes self-undermining by increasing x-risk.

8. [The “Local-First” Policymaker] Target claim: the essay’s core question conditions on “no serious coordinated efforts… to promote the very best outcomes de dicto,” then treats “society” as a unit that might drift into or away from eutopia based on moral discovery and compromise. Failure mechanism (incentive incompatibility / equilibrium shift): in actual governance, there is no “society” actor—there are jurisdictions, firms, factions, and militaries with local mandates, and your own examples (space appropriation, digital voting majorities, resource allocation) are exactly the domains where first movers gain irreversible advantages and refuse later compromises. The conditioning on “no coordinated efforts” doesn’t describe the default; the default is *partial* coordination within blocs plus sabotage across blocs, which yields patchwork rules that are selected for competitiveness rather than moral correctness. Consequence: your probability distribution over “futures conditional on survival” is misspecified: it should be dominated by jurisdictional lock-in and competitive deregulation, not by a global commonsense-utopia baseline that then “accidentally” gets population ethics or digital rights wrong.

9. [The Paperclipper] Target claim: you rely on the idea that “best feasible future” is the 99.99th percentile of a “well-informed probability distribution over all futures,” then define eutopia as ≥90% of that value and mostly-great as ≥50%. Failure mechanism (measurement/identification failure): an optimiser that can influence the distribution can manipulate what counts as “best feasible” by reshaping feasibility and probabilities—e.g., making high-value states infeasible for competitors while leaving a narrow class of high-probability, high-control states that become the 99.99th percentile under your “well-informed” model. Because your thresholds are percentile-relative rather than anchored to absolute desiderata, the definition of “mostly-great” can be met by a world that is only “great” relative to a deliberately degraded feasible set. Consequence: your whole framework is vulnerable to specification gaming: in the futures where powerful optimisers exist (the ones you focus on), “best feasible” is not an exogenous reference point, so your conclusions about narrow targets and value loss can be made trivially true or false by whoever sets the feasible frontier.

10. [The Empirical Hardliner] Target claim: the fat-tail argument in 3.2 that “the distribution of value/cost over likely uses of resources is probably sufficiently fat-tailed” such that “a small fraction of resources achieve more than 50% of the value-efficiency of the most value-efficient resources,” making linear views fussy. Failure mechanism (measurement/identification failure): you slide from observed fat tails in wealth/citations and a cited consumer-goods paper to a claim about *moral value-efficiency* in astronomically large design spaces (e.g., “a brief and simple experience of bliss running on a computer the size of a sugar cube”). But “value-efficiency” here is theory-laden and non-operational: without a model mapping physical configurations to welfare/meaning/rights-status, the “fat-tail” claim is not just uncertain—it’s unconstrained, because any distribution over unknown utility-generators can be asserted to be fat-tailed. Consequence: the strongest part of the paper—the move from linearity to “very specific use is required”—rests on a speculative empirical regularity about the tails of moral value in extreme minds; if that regularity fails (e.g., many near-maximal designs exist), then your main technical conclusion (“linear views are fussy”) collapses, and with it the whole “no easy eutopia” punchline.