> The paper's central claim that eutopia is vanishingly unlikely is only as strong as its unexamined baseline—tweak what counts as "no serious optimization" and the odds shift dramatically.

**“The Missing Prior”** — The paper’s key inference is that, *conditional on survival and absent coordinated de dicto optimization*, mostly-great futures are very unlikely, so eutopia is a narrow target. But the central object doing the work here—a “well‑informed probability distribution over all futures” under “no serious, coordinated efforts” (Sec. 1, 3.1)—is left radically under-specified, and small changes to what counts as “serious optimization” (e.g., institutional moral progress, AI-mediated governance improvements, cultural selection effects) can swing the mass on “mostly-great” outcomes by orders of magnitude. Because “likelihood of mostly-great futures” is defined relative to that distribution, the thesis risks becoming a restatement of whatever pessimism the reader bakes into the baseline dynamics. If this objection holds, the paper would need to (i) operationalize the baseline generative model (even schematically), (ii) run sensitivity analyses over plausible dynamics (moral learning rates, governance capacity, AI alignment regimes, coordination), and (iii) show robustness of “<1% mostly-great” across those baselines rather than arguing mainly from illustrative failure modes.