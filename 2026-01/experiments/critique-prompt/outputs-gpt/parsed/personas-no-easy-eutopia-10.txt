> The real risk isn't getting many independent moral questions wrong simultaneously, but that feedback loops between policy, ontology, and incentives can lock civilisation into qualitatively different regimes through phase transitions no factor-product model captures.

The paper treats “moral catastrophe” as a small set of identifiable axes (population ethics, digital rights, wellbeing theory, allocation rules) and then reasons as if you can assess how “fussy” a view is by checking sensitivity along those axes (Secs. 2.3, 3.x). But the future scenarios you cite—preference engineering, digital polity scaling, acausal trade attempts, simulator negotiation—are precisely the kind of reflexive, adaptive systems where interventions change the ontology (what counts as a being, a death, a preference) and thereby change the evaluation criteria midstream. In such systems, “single flaw” narratives are misleading: harms and goods emerge from feedback loops (e.g., rights → replication incentives → governance overload → authoritarian simplification → welfare collapse) that aren’t decomposable into independent moral decisions. The concrete consequence is that your conclusion “mostly-great futures are rare because value is multiplicative across many factors” is structurally wrong for path-dependent complex systems; the real risk is attractors and phase transitions, where small policy changes shove civilization into qualitatively different regimes that your factor-product model cannot represent.