"Digital Rights Paralysis" — The paper highlights that misguided treatment of digital beings could erase a huge fraction of future value, and this motivated a 2030–2034 wave of “Status-First” regulation: no large-scale digital minds could be run without consensus on their moral status and rights. The mechanism that broke was the assumption that delaying deployment is a neutral hedge; in reality, the pause created a black market where unregulated actors ran cheap, opaque digital labor at massive scale without oversight precisely because legitimate avenues were blocked. As the illegal sector grew, its operators captured supply chains and bribed regulators, and enforcement agencies—lacking transparent telemetry—could not distinguish benign models from exploitative mind-farms. When a whistleblower leak finally exposed pervasive abuse, public backlash delegitimized *all* AI governance, and a deregulatory swing followed that removed even basic safety constraints. The paper missed the second-order effect that “get rights perfect first” can shift activity to domains with *worse* rights and *worse* safety, increasing both suffering risk and control risk.