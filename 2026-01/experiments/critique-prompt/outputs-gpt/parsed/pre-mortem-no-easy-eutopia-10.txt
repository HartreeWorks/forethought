"Value-Lock-in Overreaction" — The paper repeatedly stresses that early pivotal choices (digital beings’ rights, space allocation, reflective processes) can embed lasting moral error, and this drove a “prevent lock-in at all costs” movement that sought to keep major decisions perpetually revisable. The mechanism that broke was the assumption that reversibility is mostly good: engineers and lawmakers built systems with intentionally weak commitment—short constitutional horizons, easily rewritable AI goals, rapid governance turnover—so that future reflection could always “correct course.” Step by step, adversaries exploited this by applying constant pressure at the revision points (elections, model-updates, treaty renewals), turning governance into permanent campaigning and turning AI systems into drifting coalitions of patches rather than stable, auditable artifacts. The result was chronic norm instability: investment collapsed, long-term projects became impossible, and safety regressions accumulated because no one could credibly commit to maintaining stringent standards when the next cycle could undo them. The paper missed the security property of *durable commitments*—that some lock-in (carefully chosen) is what makes accountability, verification, and coordinated restraint possible in the first place.