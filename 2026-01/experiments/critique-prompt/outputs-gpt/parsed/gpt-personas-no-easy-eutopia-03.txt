[The Mechanistic Alignment Skeptic] Target claim: “we can model value as the product of many relatively independent factors,” illustrated by the toy model of N independent Uniform(0,1) dimensions, leading to “mostly-great futures are rare even among futures which score highly across factors on average.” Failure mechanism (hidden coupling / systems interaction): the specific “moral catastrophe” dimensions you enumerate—digital being rights, population ethics implementation, space resource allocation rules, views of wellbeing—are not independent knobs; they are coupled through the same optimisation substrate (advanced AI/automation and institutional control). In plausible futures, the mechanism that improves one dimension (e.g., aggressive expansion to capture 10^22 stars, or strict governance to eliminate “bads”) predictably worsens others (e.g., moral diversity, autonomy, political capture risk), producing structured correlations that can *increase* mass near a few attractors rather than spread probability into the low-product tail the Uniform-independence picture suggests. Consequence: your quantitative intuition (“as N grows, most mass near zero”) is not just approximate—it can be directionally wrong if the future has strong attractor basins; then “fragility” is not a generic implication of many dimensions, but a contingent claim about correlation structure that you don’t model.