The paper repeatedly relies on choices like “society might allow extrasolar resources to be claimed by whoever gets there first” versus “allocated equally to everyone alive at the time” (Sec. 2.3.4), as if these are selectable social options rather than equilibrium outcomes under incentives. In the first land-grab regime, actors who can precommit to speed and secrecy dominate; in the equal-allocation regime, actors have maximal incentive to create shell identities, accelerate reproduction/instantiation, or lobby for definitions of “everyone alive” that include their dependents and copies. You treat these as moral dials we can set, but incentive gradients will route around the intended policy and select for the most exploitative interpretation. The concrete consequence is that your “moral catastrophe” examples systematically understate strategic manipulation: the catastrophes won’t be accidental “flaws,” they’ll be stable equilibria produced by rational agents optimizing within your proposed rule schemas.