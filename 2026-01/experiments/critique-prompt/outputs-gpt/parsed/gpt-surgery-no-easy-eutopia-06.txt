In Sec. 2.3.2 you argue digital beings are a major “moral catastrophe” axis: too few rights → exploitation; full rights → humans lose control and “human values” may be lost. **Attack type: Strategic response.** Your framing implicitly assumes political power maps to headcount (voting rights) and that governance can’t discriminate between entity types without moral error, but future actors will strategically design the franchise and ontology to preserve their interests (e.g., citizenship tests, constitutional constraints, weighted voting, nonpersonhood design, or deliberate creation of nonvoting labor minds). That strategic adaptiveness makes the “either exploit them or they take over” dichotomy non-load-bearing, and it also undercuts the “easy to accidentally introduce” claim: the system will be intentionally engineered around this exact fault line. If this critique holds, you’d need to replace the binary with an equilibrium analysis: given incentives, what governance arrangements around digital beings are stable, and do they actually concentrate probability mass on low-value outcomes rather than on engineered compromises?