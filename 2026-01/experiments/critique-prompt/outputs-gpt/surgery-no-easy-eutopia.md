1. The load-bearing claim being attacked is: **“Value is well-described as the product of many relatively independent factors, so doing poorly on any one dimension wipes out most value.”** **Attack type: Countermodel.** Construct a world where welfare-relevant factors are strongly *substitutable* and linked by repair dynamics: e.g., a civilization that is mediocre on “digital rights” early but later uses abundant resources plus reversible computing to retroactively compensate (restore memories/experiences, create vast additional lives, offer restitution) such that the long-run value is dominated by late-stage improvements. In that world, many single “flaws” are not multiplicative bottlenecks but temporary local losses with bounded impact. If this holds, the paper’s core intuition—“single flaws unwind most of the moral lustre”—fails, and the “narrow target” conclusion no longer follows from fragility; you’d need a separate argument that the key flaws are *irreversible* and *non-compensable*.

2. The load-bearing claim being attacked is: **“The multiplicative toy model (independent Uniform[0,1] dimensions) is a good guide to the shape of the distribution of achievable future value; hence mostly-great futures are rare by default.”** **Attack type: Parameter sensitivity.** The conclusion depends sharply on (i) independence, (ii) the tail behavior near 0, and (iii) the number of dimensions N treated as if it can grow without changing the structure of civilization. If instead dimensions are positively correlated (moral reflection, institutional quality, and epistemics improve together) or truncated away from 0 by robust institutions (minimum-rights floors, automated welfare monitoring), the product distribution becomes far less skewed and the “top quartile is tiny” result can vanish. If this critique holds, the paper can’t use the toy model as evidence for “no easy eutopia”; it would need to argue for a specific dependence structure and for why “near-zero on one factor” is realistically common conditional on abundance and stability.

3. The load-bearing claim being attacked is: **“On unbounded linear views, a mostly-great future requires using almost all accessible resources (e.g., a Milky Way civilization is ~1/20B of the value of the best feasible future).”** **Attack type: Quantitative cliff.** This treats “accessible universe resources” as a fixed yardstick and assumes we compare futures by cosmic share, but it introduces a cliff at the cosmology boundary: if expansion is technologically feasible but *governance-costly* or *risk-amplifying*, then “using more resources” may reduce expected value by increasing accident surface area, conflict, or irreversible lock-in mistakes during expansion. In that regime, “mostly-great” could be achieved by deliberately *not* expanding aggressively—contradicting the necessity claim. If this holds, the argument that linearity implies extreme scale-maximization (and thus fussiness) breaks; the paper would need to integrate expansion externalities and show that marginal expected value of expansion stays positive across scales.

4. The load-bearing claim being attacked is: **“Even a universe-spanning civilization could fall far short because value-per-resource across uses is fat-tailed, so only very specific configurations achieve near-max value.”** **Attack type: Reference class failure.** The paper leans on fat tails in wealth, citations, and consumer surplus to infer fat tails in *moral value efficiency* of future resource uses, but those reference classes are dominated by competitive selection, network effects, and measurement artifacts that need not exist for well-engineered experiences or welfare production. A future optimizer with powerful search and feedback might smooth the distribution: once you know the recipe for high-value experience, you can replicate it widely, turning “rare peaks” into a manufacturable plateau rather than a needle-in-haystack. If this critique holds, “fat tails → narrow target” no longer follows; the paper would need to argue that the value landscape remains rugged even under advanced optimization and that near-optimal designs cannot be replicated or generalized.

5. The load-bearing claim being attacked is: **“Future moral catastrophes are easy even in common-sense utopia; e.g., wrong population ethics, wrong treatment of digital beings, wrong wellbeing theory can forfeit most value without anyone discontent.”** **Attack type: Equilibrium shift.** Many of the proposed “easy mistakes” are not passive errors but would become salient political/economic fault lines once they carry enormous stakes (e.g., digital labor, voting power, ownership, lifespan design). Strategic actors (firms, states, digital constituencies) would litigate and bargain, generating equilibrium institutions (courts, treaties, constitutional limits, identity/firewall rules) that systematically push outcomes away from extreme exploitation or extreme disenfranchisement—not because society aims at “the best,” but because power contests create stabilizing constraints. If this holds, “easy mistake” is overstated: the default trajectory is shaped by strategic pressures that may *prevent* the very catastrophic corners the paper treats as readily reachable, weakening the inference from “there exist plausible moral errors” to “most survival-conditioned futures are far below mostly-great.”

6. The load-bearing claim being attacked is: **“If value is bounded with respect to the universe as a whole, then because the universe is very large relative to humanity’s influence, bounded concavity makes the view practically linear, hence fussy.”** **Attack type: Parameter sensitivity.** The argument hinges on the ratio “human influence / total value of universe” being tiny, which depends on speculative cosmological assumptions (e.g., vast numbers of alien civilizations) *and* on treating those regions as morally similar and independent of our actions. If instead (a) the accessible/affected region is a non-negligible share of morally relevant value due to causal connectedness, simulation control, or rare-event dominance, or (b) aliens exist but are near-irrelevant under many person-affecting or deontic views, then the “small interval ≈ linear” move doesn’t go through. If this critique holds, the paper loses a major bridge from “bounded views” to “linear-in-practice” to “fussy,” and it must defend why cosmological scale should dominate the practical curvature relevant to our decisions.

7. The load-bearing claim being attacked is: **“On bounded difference-making views that aggregate goods and bads separately, even one part in 10^22 resources going to bads prevents a mostly-great future; therefore such views are fussy and mostly-great is a narrow target.”** **Attack type: Causal reversal.** The paper treats separate aggregation as making the evaluation hypersensitive to tiny bad fractions, but that hypersensitivity is an artifact of modeling “bads” as additively scalable with colonization and assuming bads expand proportionally with controlled resources. A plausible institutional reality is the reverse: as a civilization scales, it can allocate *increasing* resources to monitoring, rights enforcement, and harm prevention, so the *fraction* and even the *absolute amount* of severe bads can fall with scale (automation for welfare auditing, sandboxing, robust containment of suffering-capable processes). If that’s right, then large-scale expansion makes it *easier*, not harder, to satisfy separate-aggregation constraints, undermining the claim that these views imply a razor-thin target; the paper would need to argue that bad-prevention cannot scale faster than bad-production.

8. The load-bearing claim being attacked is: **“Jointly aggregating bounded difference-making views are a narrow slice and face ‘scale-tipping,’ making them implausible; thus easygoing views are unlikely.”** **Attack type: Countermodel.** Consider a world where moral patients are protected by lexicographic or threshold constraints (rights floors) and within the feasible set value is jointly aggregated with smooth saturation—then “scale-tipping” can be avoided because below-threshold bads are prohibited rather than traded off, and above-threshold tradeoffs are continuous. That yields an easygoing view that is neither the paper’s “separate aggregation hypersensitive” nor its “joint aggregation scale-tipping” caricature, and it can be philosophically common (rights + welfare). If this holds, the paper’s taxonomy undercounts plausible easygoing views; its conclusion that easygoingness is “a narrow slice” becomes much weaker unless it can show that threshold/constraint hybrid theories are either unstable under reflection or still imply narrow-target outcomes.

9. The load-bearing claim being attacked is: **“Because intertheoretic comparisons under moral uncertainty (variance normalization, pairwise reasoning) plausibly give unbounded views more say, our all-things-considered stance should be fussy in practice.”** **Attack type: Parameter sensitivity.** The “pairwise” route relies on contentious anchors—e.g., that utilitarianism and near–negative utilitarian limits “agree on the disvalue of bads,” then uses that to propagate a shared scale across theories; small changes in the anchoring judgments (e.g., whether extreme suffering is comparable across metaphysics of persons, or whether disvalue is separable the way assumed) flip which views dominate the mixture. The paper itself notes affine freedom, but then leans on “most plausible” normalization choices to recover fussiness; that step is doing heavy work without showing robustness across a reasonable neighborhood of anchoring choices. If this critique holds, the paper can’t claim that “fairest” uncertainty handling is itself fussy; at most it can say “some plausible methods yield fussiness,” which is a materially weaker governance-relevant conclusion.

10. The load-bearing claim being attacked is the paper’s overarching inference: **“Since most plausible moral views are fussy and eutopia is fragile, absent serious de dicto optimization pressure, most survival-conditioned futures fall far short of mostly-great.”** **Attack type: Equilibrium shift.** The analysis treats “no serious coordinated efforts to promote the overall best outcomes de dicto” as implying weak optimization toward high value, but many optimization pressures are de re and endogenous: competition for legitimacy, migration/exit, market design, and conflict-avoidance can select for institutions that approximate impartial welfare (or at least reduce major moral catastrophes like extreme suffering) even without consensus on “the best.” These pressures can create convergent guardrails (anti-torture norms, welfare floors, anti-slavery analogs for digital minds) that compress the lower tail of value among survivals, making “mostly-great” substantially more common than the paper suggests. If this critique holds, the paper’s conclusion about rarity is overstated: it would need a model showing that de re selection dynamics fail to erect robust moral guardrails and that key catastrophes persist as stable equilibria even under abundance and institutional evolution.