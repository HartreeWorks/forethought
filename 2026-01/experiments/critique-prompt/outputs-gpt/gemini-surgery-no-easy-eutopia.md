1. The paper’s first keystone is: “Conditional on survival, most futures are far below 50% of best-feasible value unless we deliberately optimize for ‘the best’.” The load-bearing inference is from “many ways to be wrong” to “default outcomes are usually < mostly-great,” which silently assumes a broad distribution over futures where moral error is roughly as likely as moral success on each key dimension. A counter-model fits their premises but flips the conclusion: suppose post-scarcity governance converges on a robust “procedural corrigibility” norm (no value lock-in, ongoing revision, reversible commitments), so most futures avoid large irreversible mistakes even without de dicto maximization; then lots of futures cluster above 0.5 even if none are near-best. If this critique holds, the paper must replace “many possible errors exist” with an argument that default dynamics systematically *select* for high-impact irreversible errors rather than self-correcting institutions.

2. A second keystone is the “multiplicative fragility” frame: value ≈ product of many independent factors, so missing one factor drives value near zero. The beam here is the independence assumption plus the choice of a product aggregator, which is doing essentially all the work in turning “several important dimensions” into “mostly-great is rare.” Counter-example: if the true structure is “lexicographic thresholds + redundancy” (e.g., once you clear basic thresholds on suffering, autonomy, and rights, marginal improvements have diminishing moral returns and failures are partly substitutable), then high-value futures are *not* exponentially rare; they’re common plateaus with occasional spikes. To salvage the argument, the author would need to show not just that multiple dimensions matter, but that (i) they are weakly correlated under default trajectories, (ii) they are non-substitutable, and (iii) typical shortfalls are *catastrophic* rather than marginal under a wide class of plausible moral theories.

3. The argument repeatedly uses “ongoing moral catastrophes across history” as evidence that future societies will unknowingly commit similarly massive errors, i.e., “non-obvious severe flaws are the norm.” The hidden lemma is an inductive step: past moral blindness implies future moral blindness at comparable scale even under radically improved epistemics, institutional checks, and moral reflection tools. Counter-example: history may be dominated by small-elite power structures and low-information conditions; in a future with high transparency, reversible policy experimentation, and AI-augmented moral debate, the *mechanism* that produced past catastrophes (coercive institutions + misinformation + inability to coordinate reform) is absent, so “catastrophe as the norm” no longer projects. If this holds, the paper must argue at the mechanism level—identify which failure modes persist even under improved epistemics and why they remain stable attractors rather than transient errors.

4. In §2.3 the paper treats “people get what they want under abundance” as compatible with enormous moral loss via preference engineering (“they could engineer preferences to be satisfied with tragically mediocre circumstances”). This is a bait-and-switch between (a) a liberal desideratum of preference satisfaction and (b) an external metric of “true value” that can declare preference-satisfied worlds catastrophic; the paper uses (a) to set up intuitive plausibility and then evaluates with (b) to claim fragility. Counter-example: under a coherent preference-satisfactionist or procedural liberal view, voluntary preference modification that increases stability, meaning, and satisfaction is not a moral catastrophe but part of flourishing; the “mediocrity” verdict only follows on objective-list or non-voluntarist constraints the paper hasn’t earned. To fix this, the author must either (i) restrict the target audience to non-preference accounts and show they dominate moral uncertainty, or (ii) provide an argument that preference engineering is bad even by liberal/procedural lights (e.g., because it predictably violates informed consent, autonomy over time, or creates coercive preference markets).

5. The “scale-insensitivity” section argues that commonsense utopia may miss most value by being too small (solar-system limited), implying many futures that feel great are <0.5 on linear/unbounded views. The fragile beam is the assumption that “best feasible” includes capturing ~all accessible cosmic resources and that failing to do so is a *moral* shortfall rather than a mere missed opportunity; this depends on strong cosmopolitan/expansionist premises. Counter-example: a moral theory that treats expansion beyond some point as morally optional (or even morally risky due to harm externalities, corruption, or rights-violations in colonization) can rank a stable, contained utopia as >0.5 of best feasible *because best feasible itself is defined by quality not maximal scale*. The revision required is to separate “feasible maximum scale” from “morally mandatory scale,” and to justify why “best feasible” should be pegged to near-maximal expansion rather than to a satisficing-quality benchmark.

6. In §3.2 the paper leans heavily on: “For linear unbounded views, value-per-resource is fat-tailed, so mostly-great requires using resources in very specific near-optimal ways.” The hidden assumption is that the distribution of *marginal moral value* over resource configurations remains fat-tailed at astronomic scales rather than washing out via replication of many near-best patterns, and that the top tail is not broad. Counter-example: suppose there is a wide basin of near-optimal designs (many architectures yield within, say, 90% of maximal wellbeing density) because physics/compute constraints produce convergent efficient implementations; then the “specific use” requirement collapses and mostly-great becomes easy even on linear views. To repair, the author must argue that near-optimal basins are narrow (highly non-robust) in the relevant design space, not merely that some tails in current human domains are fat.

7. The bounded-views section asserts that if value is bounded “for the universe as a whole,” then because humanity’s influence is tiny relative to a possibly huge/infinite universe, concavity makes the function locally linear, restoring fussiness. This inference relies on a tacit move: that the moral value function is defined over the *entire* universe’s welfare-state, so our marginal contribution is evaluated against an astronomically large background of alien value. Counter-example: a perfectly coherent bounded consequentialism could be *agent-relative* or “domain-relative” (value defined over the agent’s causal cone or controllable region) without being the “difference-making boundedness” the paper later attacks; then concavity need not imply local linearity, and easygoingness can survive. If this holds, the paper must tighten the taxonomy: distinguish “global state boundedness,” “domain-relative boundedness,” and “difference-making boundedness,” and show that the non-fussy variants are either inconsistent or independently implausible rather than assuming the global framing.

8. The separate-aggregation bounded view critique (“one part in 10^22 bads prevents mostly-great”) depends on a load-bearing modeling choice: treating “goods” and “bads” as extensive quantities that each approach their own upper bounds as scale grows, so tiny fractions become huge absolute bads. Counter-example: on a bounded view where bads are “localized rights violations” that do not aggregate linearly beyond a saturation point (e.g., once institutions guarantee near-zero severe suffering, residual trivial harms don’t scale disvalue proportionally), a civilization with a star-system’s worth of mild bads spread thinly across 10^22 stars need not fall below 0.5. To revise, the author would need to justify why disvalue from bads remains extensive and unsaturating across scale (especially for bounded views), rather than assuming linear bad-aggregation while simultaneously exploring non-linear good-aggregation.

9. The “joint aggregation” attack (“scale tipping is intuitive nonsense”) is presented as a decisive implausibility, but it smuggles in an intuition about continuity of moral value with respect to balances of goods/bads that many coherent joint functions intentionally violate near thresholds (e.g., threshold deontology, catastrophe-avoidance). Counter-example: a moral view with a catastrophe threshold (any sufficiently large region of extreme suffering triggers near-maximal disvalue) can produce sharp tipping without incoherence, and can still be easygoing about *non-catastrophic* futures; then joint aggregation isn’t ruled out, and the “narrow slice” claim weakens. If this critique holds, the paper must replace “tipping seems weird” with a formal argument that such tipping violates the very vNM assumptions the paper itself adopts (continuity/independence), or else admit that rejecting tipping forces rejecting parts of the framework rather than the moral view.

10. The moral-uncertainty section is structurally pivotal because it’s meant to show that even if some easygoing views exist, plausible intertheoretic aggregation will still make the overall stance fussy. The keystone inference is: “variance normalization or pairwise comparisons are the most plausible; both give unbounded/linear/fussy views disproportionate influence,” but the paper’s own examples show the ranking flips under reasonable normalizations, meaning the conclusion depends on choosing a particular intertheoretic currency. Counter-example consistent with their setup: adopt an intertheoretic method keyed to action-guidingness or choice-worthiness under bounded rationality (e.g., normalize by each theory’s sensitivity over the realistically reachable outcome set), and bounded easygoing views dominate practical decisions, making “mostly-great” comparatively likely and “no easy eutopia” no longer action-relevant. If this holds, the author must either (i) prove a representation-invariant dominance result (“fussy conclusions are robust across all reasonable normalizations”), or (ii) weaken the conclusion to be conditional on a specific intertheoretic comparison rule rather than presenting it as a broad implication of moral uncertainty.