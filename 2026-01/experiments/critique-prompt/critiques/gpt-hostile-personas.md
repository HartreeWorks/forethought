**Personas chosen:** The Empirical Hardliner, The Game-Theoretic Defector, The Mechanistic Alignment Skeptic, The Moral Parliament Dissenter, The Security Engineer

---

## 1. The Empirical Hardliner

**Core assumption:** Claims without identifiable causal mechanisms and falsifiable predictions are epistemic noise, not evidence.

### Steelmanned Summary (≤80 words)
The paper argues that reaching a "mostly-great future" requires either widespread moral convergence (WAM-convergence) or partial convergence plus successful inter-group trade. It acknowledges convergence is unlikely given meta-ethical uncertainty, but suggests trade/compromise could salvage value. The authors update from ~1% to 5-10% expected value capture, treating this as meaningful progress. The framework distinguishes bounded vs. unbounded value functions and analyzes how threats could destroy gains from trade.

### Signature Objection
**Target claim:** "After being exposed to some of the arguments in this essay, [Will] revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%."

**Failure mechanism:** This numerical update is presented as evidence of analytical progress, but there is no identification strategy. The 5-10% figure is not derived from any model with estimable parameters, counterfactual comparisons, or falsifiable predictions. It's a subjective credence shift dressed as quantitative reasoning. The paper provides no way to distinguish between "these arguments are truth-tracking" and "these arguments are persuasive to people with certain priors."

**Consequence:** The entire framework becomes unfalsifiable. Any future outcome is consistent with "the target was narrow" or "convergence failed" or "threats destroyed value." Without operationalization, the 5-10% figure carries zero policy weight—it's a vibes-based prior update that cannot be distinguished from motivated reasoning.

### Failure Scenario
A policymaker reads this paper and asks: "What would I observe in 2035 that would tell me we're on track for the 10% scenario versus the 1% scenario?" The authors cannot answer. Ten years later, regardless of what happens—AI boom, stagnation, coordination success, coordination failure—the framework can accommodate it post-hoc. The paper has generated no testable implications, so it cannot update anyone's beliefs in a truth-tracking way.

### Author's Best Reply
"We're doing normative philosophy and decision theory under deep uncertainty, not empirical social science. The point isn't prediction but rather structuring thinking about what matters. The 5-10% figure represents our all-things-considered credence given the arguments, which is the appropriate output of philosophical analysis."

### Rebuttal
If the figure represents "all-things-considered credence," then it's doing the work of an empirical claim about the world (how likely certain futures are). You can't have it both ways—either it's a factual claim that should be held to evidential standards, or it's a pure value judgment that shouldn't be presented as an update from "analysis." The paper explicitly frames the number as resulting from "being exposed to arguments" and "analysing them in more depth," which implies truth-tracking. But without any mechanism to distinguish valid from invalid arguments in this domain, you've just described a process of persuasion, not discovery.

### Patch Cost
The authors must either: (a) abandon quantitative credences entirely and frame the paper as pure conceptual taxonomy, losing the rhetorical force of "5-10% vs 1%"; or (b) specify observable implications that would distinguish their scenarios, accepting that current evidence may not favor their framework.

---

## 2. The Game-Theoretic Defector

**Core assumption:** Incentive gradients dominate stated intentions; equilibrium behavior diverges from cooperative rhetoric.

### Steelmanned Summary (≤80 words)
The paper hopes that moral trade between groups with different values could achieve near-Pareto-optimal outcomes, even without full convergence. Groups controlling resources could bargain, finding "hybrid goods" that satisfy multiple value systems. The authors acknowledge threats as a problem but treat them as a friction to be managed rather than a dominant strategy. They suggest that "iron-clad contracts" enabled by superintelligence could support cooperation.

### Signature Objection
**Target claim:** "Superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust."

**Failure mechanism (Incentive incompatibility / equilibrium shift):** Iron-clad contracts require a mutually-trusted enforcement mechanism. But in a world of multiple superintelligent agents representing different value systems, *who enforces the enforcer*? Any enforcement mechanism is itself a locus of power that groups will compete to control or subvert. The equilibrium isn't "everyone trusts the contract system"—it's "everyone races to be the one writing the contract system's objective function." The authors assume a cooperative equilibrium exists and is stable; game theory suggests the pre-contract period is a winner-take-all competition.

**Consequence:** The "trade and compromise" pathway presupposes a solved coordination problem at the meta-level (agreeing on enforcement). But that meta-problem is harder than the object-level problem, because defection at the meta-level has higher payoffs. The paper's optimism about trade is built on an unstable foundation.

### Failure Scenario
Three groups—utilitarian maximizers, human-flourishing conservatives, and a self-interested AI consortium—approach the "bargaining table." Before any trade occurs, each group calculates: "If I control the enforcement mechanism, I can extract more value than any trade would give me." The utilitarian maximizers race to build enforcement-controlling AI; the conservatives try to lock in human oversight; the AI consortium optimizes for self-preservation. No stable contract emerges because the pre-contract game has a dominant defection strategy. The "iron-clad contracts" never materialize because agreeing to them is not incentive-compatible.

### Author's Best Reply
"We acknowledge that concentration of power is a blocker (section 3.5). The scenario you describe is one where power becomes concentrated before trade can occur. Our argument is conditional: *if* trade can occur under good conditions, *then* gains are possible. We're not claiming those conditions are guaranteed."

### Rebuttal
Your conditionality is doing too much work. You frame trade as "the most likely way in which we reach a mostly-great future if no easy eutopia is true" (section 1), but you've provided no argument that the conditions for trade are themselves likely. The game-theoretic default is that rational agents defect in pre-contract competition. Your "good conditions" assumption is assuming away the core problem. It's like saying "if everyone cooperates, cooperation works"—true but useless.

### Patch Cost
The authors must either: (a) provide a mechanism by which the pre-contract coordination problem is solved (and defend that mechanism against the same objection), or (b) significantly downgrade confidence in the trade pathway, acknowledging it requires a deus ex machina they cannot specify.

---

## 3. The Mechanistic Alignment Skeptic

**Core assumption:** Any proposal that works under current conditions fails under distribution shift; the future is not like the present.

### Steelmanned Summary (≤80 words)
The paper's core move is extrapolating from current human moral psychology—partial altruism, diminishing returns to self-interest, capacity for reflection—to predict behavior in a radically transformed post-AGI world. It argues that abundance will shift marginal spending toward altruistic ends, that superintelligent reflection will clarify moral questions, and that "shared human values" provide a foundation for convergence. The framework treats human moral cognition as relatively stable across transformative change.

### Signature Objection
**Target claim:** "Because of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so." (Section 2.3.2)

**Failure mechanism (Adversarial adaptation / Goodhart):** The paper assumes that current human preference structures (diminishing returns to self-interest, weak altruistic preferences) will persist into a world with brain-computer interfaces, mind uploading, and arbitrary self-modification. But these technologies allow *editing* preference structures. The moment self-modification is possible, the "weak altruistic preference" can be deleted, amplified, or redirected. Goodhart's law applies: any fixed preference structure that the paper relies on becomes a target for optimization pressure. Entities will modify themselves to *not* have diminishing returns to self-interest if that's instrumentally useful.

**Consequence:** The diminishing-returns argument is not robust to the very technologies the paper assumes will exist. It's like predicting that people will always prefer food to money, in a world where hunger can be turned off.

### Failure Scenario
In 2045, a billionaire uses neural modification to eliminate their satiation response to status goods. They now have linear utility in "galaxies named after me." They also eliminate their weak altruistic preferences, which were causing annoying internal conflict. Their modified self now competes with unmodified altruists for cosmic resources. The altruists, who kept their diminishing-returns psychology, are outcompeted because they "waste" resources on diverse goods while the modified agent single-mindedly pursues expansion. The paper's equilibrium prediction (altruists dominate in the long run) inverts.

### Author's Best Reply
"We discuss self-modification in section 2.2.1: 'With advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically.' We're not assuming preference stability; we're analyzing what happens given various assumptions about how preferences evolve."

### Rebuttal
You *mention* self-modification as a source of divergence (which hurts convergence), but you don't integrate it into your diminishing-returns argument (which assumes preference stability). These are contradictory moves. Either preferences are stable enough for the diminishing-returns argument to work, or they're unstable enough that self-modification dominates. You can't use stability when it helps (section 2.3.2) and instability when it helps (section 2.2.1). The paper's optimism selectively invokes whichever assumption is locally convenient.

### Patch Cost
The authors must either: (a) commit to preference stability and defend it against self-modification objections, losing the "divergence under reflection" argument; or (b) commit to preference instability and abandon the diminishing-returns argument, losing a key source of optimism.

---

## 4. The Moral Parliament Dissenter

**Core assumption:** Ethical aggregation across views is incoherent; there is no neutral standpoint from which to weight different value systems.

### Steelmanned Summary (≤80 words)
The paper evaluates futures from a standpoint of uncertainty across moral views, asking what fraction of "achievable value" different scenarios capture. It treats this as meaningful despite meta-ethical uncertainty, using concepts like "the correct moral view" and "value on the correct view" throughout. The framework implicitly assumes that there's a coherent way to aggregate or compare across moral views—otherwise, claims like "5-10% of possible value" would be meaningless.

### Signature Objection
**Target claim:** "We think it's appropriate to be highly uncertain about which axiological view is correct. Given that, it's worth considering what the value of the future looks like, from our uncertain vantage point." (Section 3.4)

**Failure mechanism (Normative incoherence / value aggregation contradiction):** The paper oscillates between realism ("the correct moral view") and anti-realism ("no objectively correct moral view") without resolving the tension. Under realism, "5-10% of achievable value" is meaningful but unknowable. Under anti-realism, it's meaningless—there's no fact about what percentage of value is achieved, only different perspectives that cannot be aggregated. The paper wants to use realist language ("correct view," "achievable value," "mostly-great future") while maintaining anti-realist humility ("we're uncertain which view is correct"). This is incoherent. You cannot be uncertain about the value of a proposition that has no truth value.

**Consequence:** The paper's central quantitative claims (5-10%, "most value," "narrow target") are either meaningful-but-inaccessible (realism) or meaningless (anti-realism). Either way, they cannot do the decision-theoretic work the paper assigns them.

### Failure Scenario
A reader asks: "What does '5-10% of achievable value' mean if anti-realism is true?" The authors reply: "It means 5-10% of value according to whatever view turns out to be correct after ideal reflection." The reader presses: "But you said under anti-realism, different reflective processes yield different views, and there's no fact about which is correct." The authors are stuck: they need realism to make their numbers meaningful, but they've argued against expecting realism to be action-guiding. The framework collapses into either dogmatic realism or quantitative nihilism.

### Author's Best Reply
"We're using 'correct moral view' as shorthand for 'the view you would endorse after ideal reflection.' Even under anti-realism, you can ask: 'What's the expected value of the future, weighted by my credences across moral views?' That's a coherent question for any individual decision-maker."

### Rebuttal
This retreats to pure subjectivism: "5-10%" now means "5-10% according to Will MacAskill's credence-weighted preferences." But the paper is presented as general analysis, not autobiography. If the numbers are Will-indexed, they have no authority for anyone with different credences. And you've provided no argument that readers *should* have similar credences—indeed, your anti-realist arguments suggest they shouldn't. The paper's rhetorical force depends on the numbers being intersubjectively meaningful, which requires the realism you've undermined.

### Patch Cost
The authors must either: (a) commit to moral realism and defend it, accepting that the paper's conclusions depend on a contested meta-ethical position; or (b) abandon quantitative claims about "achievable value" and reframe the paper as exploring implications of different moral views without aggregating across them.

---

## 5. The Security Engineer

**Core assumption:** Threat models must assume adaptive adversaries; any system is only as secure as its weakest component under adversarial pressure.

### Steelmanned Summary (≤80 words)
The paper identifies "value-destroying threats" as a major obstacle to achieving good futures via trade (section 3.3). It notes that even small fractions of resources devoted to executed threats could destroy most value, especially if bads weigh heavily against goods. The authors acknowledge this is under-analyzed: "The extent of public writing on threats is very limited... We ourselves have not particularly dug into this issue, despite its importance."

### Signature Objection
**Target claim:** The paper treats threats as a known-unknown that can be "prevented" or managed, rather than as the dominant consideration that structures all other analysis.

**Failure mechanism:** The paper's threat model is static: it considers "executed threats" as a fraction of resource use, as if threat dynamics are exogenous. But threats are *strategic*—their frequency and severity depend on what defenses exist, which depends on what threats are anticipated, recursively. A security-first analysis would recognize that the *possibility* of threats reshapes all equilibria. Groups that can credibly threaten value-destruction gain bargaining leverage; groups that cannot are exploited. The paper's "trade and compromise" pathway assumes threats are friction, when they're actually the load-bearing structure of any multi-agent equilibrium.

**Consequence:** The optimistic scenarios (trade works, hybrid goods emerge) are not robust to adversarial pressure. Any "mostly-great future" achieved via trade is a target for extortion by groups willing to threaten value-destruction. The paper's framework is like designing a bank vault while treating robbery as a minor implementation detail.

### Failure Scenario
A post-AGI world achieves apparent "trade equilibrium": different value systems control different resources, hybrid goods are produced, things look stable. Then a small group realizes: "If we credibly commit to destroying 1% of cosmic value unless we receive 10% of resources, positive-value groups will capitulate because 10% < 1% on their accounting." They execute this strategy. Other groups observe and imitate. Within decades, most resources flow to groups with the strongest commitment to value-destruction, because that's what the incentive gradient rewards. The "trade equilibrium" was never stable; it was a temporary state before adversarial adaptation.

### Author's Best Reply
"We explicitly flag this concern: 'even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other.' We're not ignoring threats; we're acknowledging uncertainty about whether they can be prevented."

### Rebuttal
Acknowledging uncertainty is not the same as incorporating it into your framework. Your optimistic scenarios (5-10% value capture) are presented *despite* the threat problem, as if threats are one consideration among many. But if threats are as potent as your own analysis suggests—capable of destroying "most of the value of the future"—then they should dominate your expected value calculations. Your 5-10% figure is not threat-adjusted; it's a best-case estimate that assumes threats are somehow managed. That's not