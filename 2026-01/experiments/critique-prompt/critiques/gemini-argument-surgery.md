# Surgical Critique of "Convergence and Compromise: Will Society Aim for Good Futures?"

---

## Critique 1: The Motivational Bootstrapping Paradox

**The Target:** The paper's central framework assumes that "partial AM-convergence" combined with trade can deliver a mostly-great future. Specifically, section 3 argues: "if (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we'll get to a mostly-great future."

**The Incision:** The paper treats "convergence on the correct moral view" and "motivation to act on that view" as separable properties that can be independently assessed and then combined. But the argument in section 2.4.1 explicitly states that on internalist views, "if internalism about moral judgements is correct, then people might just prefer not to learn facts that end up motivating them to act against their personal interests." This creates a fatal circularity: the paper needs people to *first* be motivated to engage in open-ended moral reflection (de dicto) in order to *then* converge on views that would motivate them. But the motivation to engage in such reflection is precisely what's in question. The paper assumes the existence of a population already motivated to pursue "the good de dicto" in order to argue that such a population could arise. This is not an argument for AM-convergence; it's a description of what would happen *if* AM-convergence had already occurred at the meta-level.

**The Fatal Counter-Example:** Consider a population where everyone has the following reflective preference structure: "I will adopt whatever moral views emerge from reflection, *provided* those views don't require me to sacrifice more than 10% of my resources for beings I don't currently care about." This population satisfies the paper's description of people who are "open to reflection" and would "change their behavior if they learned something else was best." They would engage in superintelligent-assisted reflection. But their conditional willingness to be motivated places a hard ceiling on convergence. The reflection process itself is constrained by pre-reflective motivational limits. Under the paper's own framework, this population would appear to be engaging in AM-convergence, but they systematically cannot reach views requiring significant sacrifice—which, given "no easy eutopia," are precisely the views needed.

**The Author's Best Defense:** The authors could argue that the 1-in-a-million "meaningful fraction" of people with genuine de dicto motivation exists empirically (pointing to effective altruists, certain religious traditions, etc.), and that trade mechanisms allow this small group to acquire disproportionate resources over time due to lower time preference (section 2.3.3's "long views win" argument).

**The Rebuttal:** This defense fails because it relies on the "long views win" selection mechanism having sufficient time to operate before lock-in occurs. But the paper explicitly acknowledges in section 2.3.3 that "if the major decisions are made soon, and then persist, then there just won't be time for this selection effect to win out." The paper cannot simultaneously hold that (a) we face imminent lock-in risk requiring urgent action, and (b) patient capital accumulation by the altruistically-minded will eventually give them sufficient resources for meaningful trade. These timescales are incompatible. The defense also ignores that the "long views win" argument applies equally to *any* non-discounting preference, including ideological or self-interested ones, diluting the fraction controlled by those with correct moral views.

---

## Critique 2: The Resource-Compatibility Illusion

**The Target:** Section 3.2 argues that "some views can be 'resource-compatible,' meaning there is some way to almost fully satisfy both views with the same resources" and offers the example: "hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise."

**The Incision:** This argument commits a bait-and-switch between *weak* and *strong* resource-compatibility. Weak compatibility means two views can *both gain something* from a shared arrangement. Strong compatibility means two views can *each achieve nearly all possible value* from a shared arrangement. The paper needs strong compatibility for its conclusion (reaching a "mostly-great future"), but its examples only demonstrate weak compatibility. The hedonist-wisdom example fails because, as the paper itself argued in section 2.2.1: "A life that's increasingly optimised for maximal hedonic experience will likely begin to look very different from a life that's increasingly optimised for preference-satisfaction... With limited optimisation power, both views mostly agreed on the same 'low-hanging fruit' improvements... But with more optimisation power, the changes these views want to see in the world become increasingly different." The same logic applies to hedonism vs. objective-list theories. A being optimized for maximal bliss and a being optimized for maximal wisdom will, at technological maturity, be radically different beings. The "hybrid" is not 95% as good as the optimum for each view; it's a compromise that sacrifices most of what each view distinctively values.

**The Fatal Counter-Example:** Suppose the correct view is classical hedonistic utilitarianism, and the optimal configuration is converting all available matter into "hedonium"—computronium running maximally efficient bliss-generating algorithms. A second prevalent view holds that value requires narrative structure, relationships, and achievement, not just raw hedonic states. The "hybrid good" these views could agree on—beings with relationships and achievements who also experience significant pleasure—captures perhaps 0.1% of the hedonic value available from pure hedonium (because narrative structure is computationally expensive and hedonic efficiency requires eliminating it), while also failing to maximize the second view's values (because the pleasure component crowds out deeper narrative complexity). Both views get something, but neither gets anywhere close to a "mostly-great" outcome. The hybrid is a Pareto improvement over *conflict*, but not over *separation*—and separation requires the correct view to already control enough resources to build its own optimized future, which returns us to the question the paper was trying to answer.

**The Author's Best Defense:** The authors could argue that the "free parameters" problem (section 2.4.2) cuts both ways: if we're uncertain about which precise configuration maximizes value, then we should be uncertain about whether the hybrid is much worse than the supposed optimum. Maybe hedonium isn't actually optimal for hedonism; maybe beings with narrative structure produce more net hedonic value due to factors we don't understand.

**The Rebuttal:** This defense undermines the paper's own framework. If uncertainty about the correct view is so profound that we can't distinguish hedonium from narrative-structured beings in hedonic value, then the paper's entire analysis of "which views are fussy" (referenced from "No Easy Eutopia") becomes meaningless. The paper explicitly relies on being able to say that different views have different optima that diverge at technological maturity. If we retreat to "maybe all optima are actually similar," we've abandoned the "no easy eutopia" premise that generates the paper's central question. The defense is self-defeating: it purchases optimism about trade by destroying the pessimism about convergence that motivated the need for trade in the first place.

---

## Critique 3: The Threat Asymmetry Doom Loop

**The Target:** Section 3.3 acknowledges that "threats could undermine that optimism" about trade, and section 3.4 attempts to taxonomize outcomes based on whether views are bounded/unbounded and whether bads weigh heavily against goods. The paper concludes that "we should try hard to prevent such threats, even if doing so is itself costly."

**The Incision:** The paper's analysis of threats contains a hidden lemma that invalidates its optimistic scenarios: *the capacity to make credible threats is itself a function of one's willingness to destroy value*. Groups that genuinely hold the correct moral view (which, on most candidates, includes strong prohibitions against creating suffering or destroying value) are systematically less able to make credible threats than groups with fewer moral constraints. This isn't just about threats being *executed*; it's about the *bargaining position* created by the *credibility* of threats. A group known to be unwilling to torture digital beings as leverage has less bargaining power than a group known to be willing. The paper treats "preventing threats" as a solvable coordination problem, but the asymmetry is structural: those with correct moral views are, by virtue of holding those views, worse at threatening. This means that even in scenarios where no threats are executed, the threat of threats redistributes resources *away* from those with correct moral views and *toward* those willing to threaten.

**The Fatal Counter-Example:** Consider a post-AGI scenario with two groups: Group A holds the correct moral view (suppose it's negative-leaning, placing enormous weight on preventing suffering). Group B is indifferent to suffering they don't personally experience. Both groups control 50% of resources initially. Group B credibly commits: "Transfer 40% of your resources to us, or we will use 10% of our resources to create astronomical suffering." Group A, by virtue of holding the correct moral view, *must* capitulate—the expected disvalue of the threatened suffering exceeds the value of the resources. Group B, knowing this, makes the threat. No suffering is created, but Group A now controls 10% of resources and Group B controls 90%. Iterate. The "correct moral view" is selected against in any bargaining environment where threats are possible, *even if threats are never executed*. The paper's optimistic scenarios (section 3.4) all assume the correct view retains enough resources to matter post-bargaining, but the threat asymmetry ensures it won't.

**The Author's Best Defense:** The authors could invoke the possibility of "iron-clad contracts" enabled by superintelligence (mentioned in section 3.1) that could establish a legal framework preventing value-undermining threats. If such a framework is established early, the threat asymmetry never materializes.

**The Rebuttal:** This defense requires that the legal framework be established *before* the threat asymmetry shifts bargaining power. But the establishment of any legal framework is itself a bargaining problem. Why would Group B (those willing to threaten) agree to a framework that removes their primary bargaining advantage? The paper acknowledges this: "it's not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable." For the framework to be established, those with correct moral views must already have enough power to impose it—but if they had such power, they wouldn't need the framework. The defense assumes the solution to the problem it's trying to solve. Furthermore, "iron-clad contracts" enforced by superintelligence presuppose that the superintelligence itself isn't controlled by groups willing to threaten, which returns us to the original question of who controls the superintelligence and with what values—the very question the paper's framework was meant to address.