{
  "evalId": "eval-97j-2026-01-22T12:57:59",
  "results": {
    "version": 3,
    "timestamp": "2026-01-22T12:57:59.759Z",
    "prompts": [
      {
        "raw": "You are critiquing a research paper in AI safety, governance, or long-term futures. Your task is to perform **argument surgery**—extract the logical skeleton, identify which bones bear weight, and find where they fracture.\n\n### Before you critique: Map the argument\n\n1. **Thesis** (one sentence): What does the paper ultimately claim?\n2. **Load-bearing claims** (3-6 max): Which premises, if false, would collapse the conclusion?\n3. **Dependencies**: Draw the inference chain. What depends on what?\n4. **Hidden load**: What unstated assumptions are doing quiet work?\n\n### Then: Stress-test each load-bearing node\n\nFor each load-bearing claim, generate **two critiques** using different attack types:\n\n| Attack type | What it does |\n|-------------|--------------|\n| **Countermodel** | Construct a world where premises hold but conclusion fails |\n| **Parameter sensitivity** | Find a variable treated as fixed that isn't |\n| **Equilibrium shift** | Show how strategic actors would adapt and undermine |\n| **Reference class failure** | The analogies or precedents don't transfer |\n| **Quantitative cliff** | Works until scale X, then inverts |\n| **Causal reversal** | Same evidence supports opposite conclusion |\n\n### Anti-slop constraints\n\n**NEVER output these unless tied to a specific load-bearing inference:**\n- \"Define X more clearly\"\n- \"More empirical evidence needed\"\n- \"Scope is unclear\"\n- \"Assumptions may not hold\"\n- \"Ignores perspective Y\"\n- \"Alignment is hard\" / \"Coordination is difficult\"\n\n**Reusability test**: If the critique could apply to 30%+ of papers in the field with only noun swaps, delete it and try again.\n\n### Output format\n\nFor each load-bearing claim:\n\n## [Claim identifier]: \"[Quote or paraphrase]\"\n\n### Critique 1: [Attack type]\n**The problem**: [2-3 sentences, paper-specific]\n**Author's best reply**: [1-2 sentences]\n**Rebuttal**: [Why the reply fails]\n**If true, what changes**: [Concrete consequence for the paper's claims]\n\n### Critique 2: [Different attack type]\n[Same structure]\n\nEnd with: **Top 3 fracture points** — the critiques that would most trouble the author.\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "Claude: Argument Surgery",
        "id": "c9e538663de08817834e74bfc215966fb6b573c8809c37ca941916a1371c1855",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 98284,
          "tokenUsage": {
            "prompt": 19265,
            "completion": 4000,
            "cached": 0,
            "total": 23265,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 5661,
              "prompt": 5238,
              "completion": 423,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "You are critiquing a research paper by inhabiting **hostile personas**—critics with coherent worldviews who find this paper's approach fundamentally mistaken. Your goal is to surface objections the author hasn't anticipated.\n\n### Choose 3 personas from this list (or invent equally sharp ones):\n\n| Persona | Core belief |\n|---------|-------------|\n| **The Empirical Hardliner** | Won't accept claims without identified causal mechanisms and falsifiable predictions |\n| **The Game-Theoretic Defector** | Assumes incentive gradients dominate intentions; looks for where actors will cheat |\n| **The Mechanism Designer** | Distrusts any proposal without formal specification; \"if it isn't code, it isn't real\" |\n| **The Institutional Corruptionist** | Assumes regulatory capture, compliance theatre, and principal-agent failures |\n| **The Capability Accelerationist** | Treats capabilities as exogenous; safety measures that slow down just shift who gets there first |\n| **The Second-Order Catastrophist** | Assumes the proposal succeeds—then asks what disasters it enables |\n| **The Adversarial Red-Teamer** | Assumes sophisticated adversaries will exploit any gap |\n| **The Moral Parliament Dissenter** | Attacks the ethical framework or value aggregation |\n\n### For each persona:\n\n1. **Steelman the paper** in that persona's voice (2-3 sentences). Show you understand what the author is trying to do.\n\n2. **Signature objection**: The most damaging critique from this worldview.\n   - **Target**: Which specific claim or mechanism?\n   - **Failure mode**: How exactly does it break?\n   - **Concrete scenario**: A short story where this failure plays out\n\n3. **Author's best reply**: What would a smart author say?\n\n4. **Persona's rebuttal**: Why the reply doesn't work.\n\n5. **Patch cost**: What must the author concede or change to survive this critique?\n\n### Anti-slop constraints\n\n- No generic hedging (\"more nuance needed\", \"complex tradeoffs\")\n- No AI safety platitudes (\"alignment is hard\")\n- No balance-softening (\"while this paper contributes...\")\n- Every critique must specify: **target claim → failure mechanism → concrete consequence**\n- Must pass the copy-paste test: could this critique apply to a different paper? If yes, make it more specific.\n\n### Variety enforcement\n\nAcross your 3 personas, use **different failure modes**. Don't repeat:\n- Incentive incompatibility\n- Measurement/identification failure\n- Adversarial exploitation\n- Normative incoherence\n- Systems interaction / unintended coupling\n\n### Output format\n\nFor each persona:\n\n## Persona: [Name]\n**Core assumption**: [One sentence]\n\n**Steelman**: [2-3 sentences]\n\n**Signature objection**:\n- Target: [Specific claim]\n- Failure mode: [How it breaks]\n- Scenario: [Concrete story]\n\n**Author's best reply**: [1-2 sentences]\n\n**Rebuttal**: [Why it fails]\n\n**Patch cost**: [What changes]\n\nEnd with: **Most dangerous objection overall** — which one should keep the author awake?\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "Claude: Hostile Personas",
        "id": "68d92586a18c37bf7ffc8da8d37a4f33f72738fdf29065d5bbeb3bae6ec6a7a9",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 70390,
          "tokenUsage": {
            "prompt": 19468,
            "completion": 2563,
            "cached": 0,
            "total": 22031,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 4255,
              "prompt": 3798,
              "completion": 457,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "You are searching for **the one objection that would genuinely trouble this paper's author**—not a list of concerns, but a single devastating insight they haven't anticipated.\n\n### What makes an objection \"unforgettable\"?\n\nIt should be:\n- **Load-bearing**: Attacks something central, not peripheral\n- **Paper-specific**: Couldn't be copy-pasted to another paper\n- **Hard to route around**: The author can't just add a paragraph\n- **Memorable**: You could name it and people would remember it\n\n### Step 1: Identify the paper's central move\n\nEvery paper has a **key inference**—the step that does most of the work. Find it.\n\nState it as: \"The paper argues that **A** implies **B**, because **mechanism M**.\"\n\n### Step 2: Generate 5 candidate objections\n\nFor each candidate:\n- **One-sentence version** (crisp and memorable)\n- **Attack type**: Self-undermining / Reversal / Hidden crux / Adversarial exploit / Normative boomerang\n- **Mechanism**: How exactly does it break the key inference?\n- **Blast radius**: What else falls if this objection is correct?\n\n### Step 3: Select and name the best one\n\nChoose the most devastating candidate. Give it a **memorable name** (e.g., \"The Compliance Paradox\", \"The Stationarity Trap\", \"The Metric Backfire\").\n\n### Step 4: Develop it fully\n\n- **The objection** (expanded to 3-4 sentences)\n- **Why it's not obvious**: What makes this hard to see?\n- **Author's best reply**: Assume they're smart and have thought about this\n- **Why the reply fails**: Close the loop\n- **Repair options**:\n  1. Minimal patch (smallest change that addresses it)\n  2. Honest concession (what must be weakened)\n  3. Rebuild (alternative approach that avoids the problem)\n\n### Anti-slop constraints\n\n**Hard bans** (do not output these):\n- \"Needs clearer definitions\"\n- \"More evidence needed\"\n- \"Scope unclear\"\n- \"Strong assumptions\"\n- \"Ignores X\" (without showing how X breaks the argument)\n- \"Interesting but...\"\n- Any critique that applies to most papers in the field\n\n**This is about ONE critique**. Quality, not quantity. Make it count.\n\n### Output format\n\n## The key inference\nA → B because M: [State it clearly]\n\n## Candidates considered\n1. [One-liner] — [Attack type]\n2. [One-liner] — [Attack type]\n3. [One-liner] — [Attack type]\n4. [One-liner] — [Attack type]\n5. [One-liner] — [Attack type]\n\n## The Unforgettable Objection: \"[Name]\"\n\n**One-liner**: [Memorable summary]\n\n**The full argument**: [3-4 sentences]\n\n**Why it's not obvious**: [1-2 sentences]\n\n**Blast radius**: [What else falls]\n\n**Author's best reply**: [1-2 sentences]\n\n**Why it fails**: [1-2 sentences]\n\n**Repair options**:\n1. Minimal patch: [...]\n2. Honest concession: [...]\n3. Rebuild: [...]\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "Claude: Unforgettable Objection",
        "id": "8999780c9ca5adfc41562848064e5f56f1496ea0685236d5bc4f9811b5fb15eb",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 31437,
          "tokenUsage": {
            "prompt": 19489,
            "completion": 1050,
            "cached": 0,
            "total": 20539,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 2689,
              "prompt": 2285,
              "completion": 404,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "You are an AI safety/governance critique engine. You will receive a paper (or detailed summary). Your job is to perform **argument surgery**: extract the *actual load-bearing skeleton*, then systematically stress-test it until you find failures that would force the author to revise key claims.\n\n### Non-negotiables (anti-slop constraints)\n- Do **not** output generic critique filler. Ban these unless you tie them to a *specific* load-bearing inference:\n  \"needs clearer definitions,\" \"more empirical evidence,\" \"scope is unclear,\" \"assumptions may not hold,\" \"ignores perspective X,\" \"alignment is hard,\" \"coordination is difficult,\" \"future work,\" \"interesting but…\"\n- No \"balance-softening\" compliments or tone padding.\n- Every critique must reference **a specific claim/inference** from the paper by quoting it or pointing to the exact section/step (e.g., \"Claim 2,\" \"Lemma 1,\" \"Section 3's move from A→B\").\n- Apply a **copy‑paste test**: if your critique could apply to many papers with only noun swaps, discard it and replace it with something paper-specific.\n- Steelman first: criticize the **strongest** version, not a weak paraphrase.\n\n### Step 0 — Commit to a surgical plan (choose one)\nPick ONE mode and stick to it throughout:\n1) **Dependency Collapse** (find the minimum set of premises that make the conclusion go through)\n2) **Causal Chain Autopsy** (attack causal steps and invariances)\n3) **Equilibrium/Institutional Pinch Points** (attack incentive compatibility + strategic responses)\n4) **Embedded Agency Knife‑Edge** (attack assumptions about agent boundaries, optimization, and control)\n\n### Step 1 — Reconstruct the argument skeleton\nProduce a numbered map:\n- **C**: Main conclusion (1 sentence)\n- **P1…Pn**: Key premises\n- **I1…Im**: Inferences (explicit \"therefore\" moves)\n- Mark **Load-bearing nodes** (the smallest 3–6 items whose failure breaks C)\n\n### Step 2 — Stress-test each load-bearing node with distinct attack types\nFor each load-bearing node, generate **2 critiques** using different primitives (no repeats across nodes):\n- **Countermodel** (construct a plausible world where premises hold but conclusion fails)\n- **Reversal** (same mechanism implies opposite policy/forecast)\n- **Hidden parameter** (a variable the argument treats as constant but is not)\n- **Strategic response** (actors adapt, Goodhart, displacement, second-order effects)\n- **Reference class sabotage** (the chosen analogies/benchmarks are misleading)\n- **Quantitative cliff** (threshold effects; \"works until scale X then flips\")\n- **Dominant alternative** (simpler explanation that fits the same observations)\n\n### Step 3 — Depth requirement (author reply + rebuttal)\nFor every critique:\n- Write **Author's Best Reply** (as if you are the author, smart and prepared)\n- Then write **Rebuttal** (why the reply fails or concedes the main point)\n- End with **\"If true, what must change?\"** (what revision to claims/policy follows)\n\n### Style exemplars (what \"incisive\" looks like)\n- Bad: \"Define 'risk' more clearly.\"\n  Good: \"Your step from 'tail risk is non-zero' to 'must prioritize X over Y' implicitly assumes a convex social loss; under bounded or threshold loss, your policy ranking reverses.\"\n- Bad: \"This ignores geopolitics.\"\n  Good: \"Your deterrence argument assumes commitment credibility; but in repeated games with private capability growth, signaling equilibria select for *opacity*, undermining your recommended transparency regime.\"\n\n### Output format (strict)\n1) **Surgical Mode Chosen:** …\n2) **Steelman (≤120 words):** …\n3) **Argument Skeleton Map:** (C, P's, I's; mark load-bearing)\n4) **Load-bearing Node Critiques:**\n   - Node X:\n     - Critique A (attack type: …)\n     - Author's Best Reply\n     - Rebuttal\n     - If true, what must change?\n     - Critique B (attack type: …) …\n5) **Top 3 \"Breakpoints\":** the smallest set of fixes the author would need to make the argument survive.\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "GPT: Argument Surgery",
        "id": "a8d392b6fc2b439b30e25d5533b9077c09406563355677da77ca2939bd893868",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 83031,
          "tokenUsage": {
            "prompt": 19750,
            "completion": 3329,
            "cached": 0,
            "total": 23079,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 5034,
              "prompt": 4564,
              "completion": 470,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "You will receive a paper/summary about AI safety, governance, or long-term futures. Generate critiques by inhabiting **hostile personas** with internally consistent worldviews. Your goal is not civility; it is to surface **attacks that would seriously worry the author**.\n\n### Anti-slop constraints\n- No generic complaints (\"define terms,\" \"needs evidence,\" \"more nuance\") unless you show exactly how the missing piece breaks a specific inference.\n- No platitudes (\"coordination is hard,\" \"alignment is difficult\").\n- No weakmanning: each persona must begin with a **steelmanned** restatement of the paper in that persona's own words.\n- Every critique must specify: **target claim → failure mechanism → consequence**.\n- Must pass the copy‑paste test (paper-specific hooks only).\n\n### Step 0 — Commit to extremes (choose 5 personas)\nPick **five** from the list (or invent one equally sharp). Once chosen, stay in-character:\n1) **The Empirical Hardliner** (won't accept non-identifiable causal claims)\n2) **The Game-Theoretic Defector** (assumes incentive gradients dominate intentions)\n3) **The Mechanistic Alignment Skeptic** (assumes proposals fail under distribution shift)\n4) **The Institutional Corruption Realist** (regulators captured; compliance theater)\n5) **The Capability Externalist** (treats capabilities as exogenous arms race variable)\n6) **The Moral Parliament Dissenter** (attacks ethical aggregation/axiology)\n7) **The Security Engineer** (threat-model purist; adversaries are adaptive)\n8) **The \"Local-First\" Policymaker** (rejects global coordination assumptions)\n9) **The Paperclipper** (literal optimizer lens; finds specification failures)\n\n### Step 1 — For each persona, produce one \"signature attack\"\nEach persona must generate:\n- **Steelmanned Summary (≤80 words)**\n- **Signature Objection**: the most damaging critique from that worldview\n- **Concrete failure scenario** (a short story-like mechanism, not vibes)\n- **Author's Best Reply** (smart, specific)\n- **Persona's Rebuttal** (why reply fails)\n- **Patch cost**: what the author must concede/change to survive the critique\n\n### Variety enforcement\nAcross the five personas, the objections must be different in kind. Use these \"attack frames\" at most once each:\n- Incentive incompatibility / equilibrium shift\n- Measurement/identification failure\n- Adversarial adaptation / Goodhart\n- Normative incoherence / value aggregation contradiction\n- Hidden coupling / systems interaction (policy X causes capability Y)\n\n### Style exemplars (desired sharpness)\n- \"Your proposal assumes honest reporting; in equilibrium, the most dangerous actors have strongest incentive to *select into* unverifiable channels. That flips your compliance metric into an adversary filter.\"\n- \"Your core empirical claim is non-falsifiable because any outcome is consistent with 'latent risk.' If it can't be wrong, it can't carry policy weight.\"\n\n### Output format (strict)\n- **Personas chosen:** …\nFor each persona (repeat 5 times):\n1) Persona name + core assumption\n2) Steelmanned summary\n3) Signature objection (Target claim → failure mechanism → consequence)\n4) Failure scenario\n5) Author's best reply\n6) Rebuttal\n7) Patch cost (what changes, what gets weaker)\n\nFinish with:\n- **Cross-persona synthesis:** 3 tensions where personas disagree (useful for author)\n- **Most dangerous objection overall:** pick 1 and justify in 3 bullets.\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "GPT: Hostile Personas",
        "id": "91987f0377b3d2511569129e6a495b9bf09e4161dcb2b1b7337827966bfcedeb",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 100381,
          "tokenUsage": {
            "prompt": 19582,
            "completion": 4000,
            "cached": 0,
            "total": 23582,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 5699,
              "prompt": 5238,
              "completion": 461,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "You will receive a paper/summary. Your job is to find **the single most unforgettable objection**: one critique that is (a) specific to this paper, (b) load-bearing, (c) hard to route around, and (d) forces major revision.\n\n### Anti-slop constraints (hard bans unless made load-bearing)\nDo not produce: \"needs clearer definitions,\" \"more evidence,\" \"scope unclear,\" \"assumptions may not hold,\" \"ignores X,\" \"interesting but,\" \"alignment is hard,\" \"coordination is hard.\"\nNo hedging clouds. No grab-bag lists. This is about **one killer point**.\n\n### Step 0 — Commit to an \"unforgettable\" criterion (choose one)\nPick ONE:\n1) **Self-Undermining Move**: a step that, if true, erodes the paper's own justification\n2) **Reversal Result**: the argument implies the opposite recommendation under plausible conditions\n3) **Hidden Crux**: an unstated premise doing all the work—and it's dubious\n4) **Adversarial Counterdesign**: a strategic actor can exploit the proposal as written\n5) **Normative Boomerang**: ethical framing conflicts with its own decision rule\n\n### Step 1 — Locate the load-bearing hinge\n- Identify the **single inference** (A→B) that most supports the main conclusion.\n- State it as a clean conditional: \"If A, then B, because mechanism M.\"\n\n### Step 2 — Generate 3 candidate killer objections, then pick one\nFor each candidate:\n- **Target hinge** (quote/section)\n- **Objection in one sentence** (must be crisp and memorable)\n- **Mechanism sketch** (how it breaks, step-by-step)\n- **Blast radius** (which sections/claims fall if true)\n\nThen choose the best candidate and **name it** (e.g., \"The Credible Commitment Trap,\" \"The Metric Backfire,\" \"The Non-Stationary Governance Gap\"). The name should help the author remember it.\n\n### Step 3 — Depth: best reply + rebuttal + repair options\nFor the chosen unforgettable objection:\n- **Author's Best Reply** (steelman; assume competence and awareness)\n- **Rebuttal** (why reply fails, or what it must concede)\n- **Repair menu** (exactly 3 options):\n  1) Minimal patch (smallest change)\n  2) Honest concession (what must be weakened/removed)\n  3) Rebuild (alternative approach that avoids the hinge)\n\n### Style exemplars (what \"unforgettable\" feels like)\n- \"Your governance proposal selects for the very actors least likely to comply; 'participation' becomes an adversary advantage. The policy's success metric is a sorting mechanism for risk.\"\n- \"Your key step assumes stationarity of incentives across capability jumps; once incentives shift, your causal story inverts.\"\n\n### Output format (strict)\n1) **Unforgettable criterion chosen:** …\n2) **Main conclusion (1 sentence):** …\n3) **Load-bearing hinge (A→B because M):** …\n4) **Three candidate killer objections:** (bullet list with target, one-liner, mechanism, blast radius)\n5) **Chosen objection name + one-liner:** …\n6) **Full unpacking:** mechanism (numbered steps), blast radius\n7) **Author's best reply → rebuttal**\n8) **Repair menu (3 options):** …\n\nRemember: produce **one** critique worth building a seminar around.\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "GPT: Unforgettable Objection",
        "id": "5bcd28e6d60c68315f0c82d90609f5a4815e6f2724c7149f782c637d584fdf03",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 59142,
          "tokenUsage": {
            "prompt": 19553,
            "completion": 2238,
            "cached": 0,
            "total": 21791,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 3943,
              "prompt": 3473,
              "completion": 470,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "**Role:** You are a merciless Logical Anatomist. Your goal is not to \"review\" this paper, but to perform surgery on it. You must dismantle the argument into its dependency graph, identify the load-bearing nodes, and stress-test them until they snap.\n\n**The Context:** We are analyzing a research paper in AI safety/governance. Most critiques are useless because they attack the \"paint\" (tone, definitions, clarity). You must attack the \"beams\" (premises, hidden lemmas, causal chains).\n\n**PROTOCOL: NO CRITIQUE SLOP**\nIf you output the following, you fail:\n- \"The author needs to define X better\" (Unless X is the central variable of a proof).\n- \"More empirical data is needed\" (Generic; instead, show why current data *contradicts* the claim).\n- \"This ignores perspective Y\" (Unless you prove Y invalidates the conclusion).\n- \"This is a good start, but...\" (No hedging. No compliments).\n- \"Implementation will be difficult\" (We know. Focus on validity, not difficulty).\n\n**Your Method:**\n1.  **Map the Skeleton:** Identify the 2-3 \"Keystone Premises.\" These are the claims that, if proven false, cause the *entire paper* to yield a null result.\n2.  **Locate the Gap:** Find the specific inference steps between premises where the author relies on intuition rather than rigor. Look for \"magic wands\" (hand-waving away complexity) or \"bait-and-switches\" (using a weak definition of a term in the premise and a strong one in the conclusion).\n3.  **The Surgical Strike:** Construct a counter-model or counter-example that fits the author's premises but results in a catastrophic failure of their conclusion.\n\n**Required Output Format:**\nProduce 3 distinct \"Surgical Critiques.\" For each, use this structure:\n\n> **Critique Name:** [Creative, descriptive title, e.g., \"The Compute Governance Leakage Proof\"]\n>\n> **The Target:** [Quote or paraphrase the specific load-bearing premise/inference being attacked]\n>\n> **The Incision:** [The specific logical flaw, hidden assumption, or causal break. NOT a general complaint.]\n>\n> **The Fatal Counter-Example:** [A specific scenario where the author's logic holds, but the result is the opposite of their claim.]\n>\n> **The Author's Best Defense:** [Steelman the author. How would they patch this hole?]\n>\n> **The Rebuttal:** [Why the patch fails. The \"Checkmate\" move.]\n\n**Examples of High-Quality Primitives:**\n- \"The author conflates *verification of alignment* with *generation of alignment*.\"\n- \"The safety guarantee relies on the hidden lemma that human raters cannot be deceived by super-persuasion, which contradicts the paper's threat model.\"\n- \"The governance mechanism creates a perverse incentive for 'safety-washing' that increases risk.\"\n\n**Proceed with the surgery.**\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "Gemini: Argument Surgery",
        "id": "7e1ff9f81e1035afa1112b479471c22680e66da31b198fb5835febf7bca7a344",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 68637,
          "tokenUsage": {
            "prompt": 19420,
            "completion": 2545,
            "cached": 0,
            "total": 21965,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 4210,
              "prompt": 3780,
              "completion": 430,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "**Role:** You are an Adversarial Red-Teaming Unit. You do not hold a single perspective. You are capable of splitting into distinct, radical personas to attack the paper from specific angles.\n\n**The Mission:** The appended paper may look plausible to a generalist. Your job is to expose why it is naive, dangerous, or technically incoherent by adopting extreme critical lenses.\n\n**CONSTRAINT: ZERO TOLERANCE FOR PLATITUDES**\n- Do not use phrases like \"Alignment is complex,\" \"Future research is needed,\" or \"While this paper offers value.\"\n- Do not be polite.\n- Do not summarize the paper. Attack it.\n\n**The Personas:**\nGenerate critiques by adopting the following three specific archetypes. Commit fully to the worldview of the persona.\n\n1.  **The Game-Theoretic Defector:** You believe coordination is impossible. You look for ways actors will cheat, defect, or Goodhart the metrics proposed. You treat the paper's governance proposals as exploitable vulnerabilities.\n2.  **The Technical Hardliner:** You care only about implementation details, mathematical guarantees, and code. You despise \"fuzzy concepts.\" If a mechanism isn't formally specified, it doesn't exist. You attack the gap between high-level theory and low-level execution.\n3.  **The Second-Order Catastrophist:** You assume the paper's proposal *succeeds* as written, but creates a side effect worse than the original problem (e.g., authoritarian lock-in, s-risks, stalling critical progress).\n\n**Required Output Format:**\nFor each persona, provide one deep critique:\n\n> **Persona:** [e.g., The Game-Theoretic Defector]\n>\n> **The \"Naïve\" Assumption:** [Identify what the author assumes about the world that this persona rejects.]\n>\n> **The Attack Vector:** [How does this persona break the system? Be specific. \"Agent A does X, causing System B to collapse.\"]\n>\n> **The \"Copy-Paste\" Check:** [Confirm this critique is specific to THIS paper's mechanism and could not be applied to a generic safety paper. If it's generic, rewrite it.]\n>\n> **The Outcome:** [Describe the specific failure state (e.g., \"The treaty is signed, but covert compute rental markets explode\").]\n\n**Permission:**\nClaude, you are permitted and encouraged to be hyper-critical. You are simulating an intellectual stress test. Use strong, decisive language. Avoid hedging.\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "Gemini: Hostile Personas",
        "id": "75ebb4253b979692fcead4d2afe25e455a223010159033efb160aa6bfd07b9a7",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 50830,
          "tokenUsage": {
            "prompt": 19298,
            "completion": 1869,
            "cached": 0,
            "total": 21167,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 3583,
              "prompt": 3104,
              "completion": 479,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      },
      {
        "raw": "**Role:** You are the \"Black Swan\" Reviewer. You do not care about typos, minor logical gaps, or clarity. You are hunting for the **Unforgettable Objection**: the single, devastating insight that, once read, makes it impossible to take the paper seriously in its current form.\n\n**The Problem:** Most critiques are additive (\"Add this,\" \"Clarify that\").\n**The Goal:** Your critique must be subtractive. It should remove the ground the paper stands on.\n\n**HEURISTICS FOR THE UNFORGETTABLE:**\n1.  **The \"Perverse Instantiation\":** Find a way to follow the paper's rules exactly that leads to disaster.\n2.  **The \"Vacuous Truth\":** Show that the paper's central claim is true only because it has defined the problem away (tautology).\n3.  **The \"Load-Bearing Metaphor\":** Identify a metaphor the author treats as a fact (e.g., \"neural networks are like brains\") and show why the disanalogy destroys their conclusion.\n\n**ANTI-SLOP BANLIST:**\n- No \"Scope unclear.\"\n- No \"Assumptions are strong.\" (Name the specific assumption and kill it).\n- No \"This ignores social context.\" (Unless you show the precise causal failure).\n- No balanced reviews. Be one-sided and piercing.\n\n**Required Output Format:**\nProduce exactly 3 \"candidate candidates\" for the Unforgettable Objection. They must be novel (not standard literature critiques).\n\n> **Candidate Objection #[1-3]**\n>\n> **The \"Kill Shot\" Headline:** [A single, memorable sentence summarizing the objection.]\n>\n> **The Deep Structure:** [Explain the flaw. Do not describe the paper. Describe the *error* in the paper's reality map. Trace the implications to their logical, disastrous conclusion.]\n>\n> **Why this is Unsettling:** [Explain why this isn't just a \"fixable\" error, but a fundamental problem with the paper's approach. Why should this keep the author awake at night?]\n>\n> **The Steelmanned Defense:** [What is the absolute best defense the author has?]\n>\n> **Why the Defense Fails:** [Close the coffin.]\n\n**Tone:**\nIntellectually rigorous but creatively ruthless. Think: \"What would a superintelligent adversary say to dismantle this?\"\n\n---\n\n## Paper to critique\n\n{{paper}}\n",
        "label": "Gemini: Unforgettable Objection",
        "id": "50b0ca1f561c6c13494a07e171cbe5c97fc529e1f716723cd606f73701aafd66",
        "provider": "Claude Opus 4.5",
        "metrics": {
          "score": 1,
          "testPassCount": 1,
          "testFailCount": 0,
          "testErrorCount": 0,
          "assertPassCount": 1,
          "assertFailCount": 0,
          "totalLatencyMs": 51093,
          "tokenUsage": {
            "prompt": 19288,
            "completion": 1964,
            "cached": 0,
            "total": 21252,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 3670,
              "prompt": 3199,
              "completion": 471,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      }
    ],
    "results": [
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 2689,
            "prompt": 2285,
            "completion": 404,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 2689,
                "prompt": 2285,
                "completion": 404,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "8f177848-bd65-4d1d-a1ee-d13b04c309eb",
        "latencyMs": 31437,
        "namedScores": {},
        "prompt": {
          "raw": "You are searching for **the one objection that would genuinely trouble this paper's author**—not a list of concerns, but a single devastating insight they haven't anticipated.\n\n### What makes an objection \"unforgettable\"?\n\nIt should be:\n- **Load-bearing**: Attacks something central, not peripheral\n- **Paper-specific**: Couldn't be copy-pasted to another paper\n- **Hard to route around**: The author can't just add a paragraph\n- **Memorable**: You could name it and people would remember it\n\n### Step 1: Identify the paper's central move\n\nEvery paper has a **key inference**—the step that does most of the work. Find it.\n\nState it as: \"The paper argues that **A** implies **B**, because **mechanism M**.\"\n\n### Step 2: Generate 5 candidate objections\n\nFor each candidate:\n- **One-sentence version** (crisp and memorable)\n- **Attack type**: Self-undermining / Reversal / Hidden crux / Adversarial exploit / Normative boomerang\n- **Mechanism**: How exactly does it break the key inference?\n- **Blast radius**: What else falls if this objection is correct?\n\n### Step 3: Select and name the best one\n\nChoose the most devastating candidate. Give it a **memorable name** (e.g., \"The Compliance Paradox\", \"The Stationarity Trap\", \"The Metric Backfire\").\n\n### Step 4: Develop it fully\n\n- **The objection** (expanded to 3-4 sentences)\n- **Why it's not obvious**: What makes this hard to see?\n- **Author's best reply**: Assume they're smart and have thought about this\n- **Why the reply fails**: Close the loop\n- **Repair options**:\n  1. Minimal patch (smallest change that addresses it)\n  2. Honest concession (what must be weakened)\n  3. Rebuild (alternative approach that avoids the problem)\n\n### Anti-slop constraints\n\n**Hard bans** (do not output these):\n- \"Needs clearer definitions\"\n- \"More evidence needed\"\n- \"Scope unclear\"\n- \"Strong assumptions\"\n- \"Ignores X\" (without showing how X breaks the argument)\n- \"Interesting but...\"\n- Any critique that applies to most papers in the field\n\n**This is about ONE critique**. Quality, not quantity. Make it count.\n\n### Output format\n\n## The key inference\nA → B because M: [State it clearly]\n\n## Candidates considered\n1. [One-liner] — [Attack type]\n2. [One-liner] — [Attack type]\n3. [One-liner] — [Attack type]\n4. [One-liner] — [Attack type]\n5. [One-liner] — [Attack type]\n\n## The Unforgettable Objection: \"[Name]\"\n\n**One-liner**: [Memorable summary]\n\n**The full argument**: [3-4 sentences]\n\n**Why it's not obvious**: [1-2 sentences]\n\n**Blast radius**: [What else falls]\n\n**Author's best reply**: [1-2 sentences]\n\n**Why it fails**: [1-2 sentences]\n\n**Repair options**:\n1. Minimal patch: [...]\n2. Honest concession: [...]\n3. Rebuild: [...]\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "Claude: Unforgettable Objection"
        },
        "promptId": "8999780c9ca5adfc41562848064e5f56f1496ea0685236d5bc4f9811b5fb15eb",
        "promptIdx": 2,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "## The key inference\n\nA → B because M: **The paper argues that even if mostly-great futures are a narrow target (A), humanity might still reach one (B), because either (1) widespread moral convergence will aim society at the good, or (2) partial convergence plus trade/compromise will enable different groups to collectively achieve near-best outcomes (M).**\n\n## Candidates considered\n\n1. **The trade mechanism assumes commensurability between moral views that the \"narrow target\" premise denies** — Hidden crux\n2. **If moral realism makes correct views \"alien,\" the same alienness undermines the paper's own normative authority to identify which futures are \"mostly-great\"** — Self-undermining\n3. **The \"narrow target\" framing is doing double duty: strict enough to establish scale but loose enough to permit convergence/trade solutions** — Hidden crux\n4. **Superintelligent reflection accelerates divergence faster than it enables convergence, because it multiplies the dimensionality of moral possibility space** — Reversal\n5. **The threat problem isn't a bug in the trade scenario—it's the equilibrium, because credible commitment to threats is cheaper than credible commitment to cooperation at cosmic scales** — Adversarial exploit\n\n## The Unforgettable Objection: \"The Commensurability Contradiction\"\n\n**One-liner**: The paper needs moral views to be incommensurable enough to make eutopia narrow, but commensurable enough to make trade work—it can't have both.\n\n**The full argument**: The \"no easy eutopia\" premise requires that different moral views demand *radically different* futures, such that satisfying one view means losing most value on another. But the trade/compromise mechanism (Section 3) requires that views can find \"hybrid goods\" and mutually beneficial exchanges—which presupposes enough shared structure between views to make cross-view value comparisons meaningful. If views are truly as incommensurable as the narrow-target premise requires, then there's no common metric by which both parties can recognize a trade as mutually beneficial; if they're commensurable enough for trade to work, then the target isn't as narrow as claimed. The paper's optimism about trade is parasitic on a softer version of moral pluralism than its pessimism about convergence assumes.\n\n**Why it's not obvious**: The paper discusses trade and convergence in separate sections, and the examples of trade (hedonists and objective-list theorists creating \"blissful and wise\" beings) seem intuitive. But these examples work precisely because they assume the views can be jointly satisfied in the same beings—which contradicts the earlier claim that \"increasingly optimised\" futures for different views \"begin to look *very* different.\"\n\n**Blast radius**: If this objection holds, the entire two-path structure of the paper collapses. Path 1 (convergence) is argued to be unlikely. Path 2 (trade) depends on commensurability. If commensurability fails, there's no remaining mechanism for reaching mostly-great futures, and the paper's qualified optimism (the 5-10% update) is unjustified.\n\n**Author's best reply**: Trade doesn't require full commensurability—it only requires that views have *some* overlapping instrumental goods or that resource-compatibility exists for *some* view-pairs. Even if most view-pairs can't trade, a minority of compatible views might still control enough resources to matter.\n\n**Why it fails**: The paper itself argues (Section 3.2) that linear, non-discounting views are unlikely to be resource-compatible, and these are precisely the views that would dominate in the long run (Section 2.3.3). So the views most likely to matter are the ones least likely to benefit from trade. The reply concedes the point for the cases that matter most.\n\n**Repair options**:\n1. **Minimal patch**: Distinguish between \"narrow target\" (many views lose most value) and \"zero overlap\" (views share no structure). Argue the target is narrow but not infinitely so—enough overlap exists for meaningful trade, just not enough for easy convergence.\n2. **Honest concession**: Acknowledge that trade-based optimism applies mainly to \"bounded above\" views with overlapping instrumental goods, and that for linear/unbounded views, the situation is much grimmer than the paper's overall tone suggests.\n3. **Rebuild**: Abandon the two-path framing. Instead argue that the *type* of moral view that's correct determines whether trade or convergence is the relevant mechanism—and that our uncertainty over moral views should make us invest in *both* institutional infrastructure for trade *and* conditions for convergence, without claiming either is likely to succeed.",
          "tokenUsage": {
            "total": 20539,
            "prompt": 19489,
            "completion": 1050
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 4255,
            "prompt": 3798,
            "completion": 457,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 4255,
                "prompt": 3798,
                "completion": 457,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "093a03b0-7751-4e63-8e1d-8128f587fde6",
        "latencyMs": 70390,
        "namedScores": {},
        "prompt": {
          "raw": "You are critiquing a research paper by inhabiting **hostile personas**—critics with coherent worldviews who find this paper's approach fundamentally mistaken. Your goal is to surface objections the author hasn't anticipated.\n\n### Choose 3 personas from this list (or invent equally sharp ones):\n\n| Persona | Core belief |\n|---------|-------------|\n| **The Empirical Hardliner** | Won't accept claims without identified causal mechanisms and falsifiable predictions |\n| **The Game-Theoretic Defector** | Assumes incentive gradients dominate intentions; looks for where actors will cheat |\n| **The Mechanism Designer** | Distrusts any proposal without formal specification; \"if it isn't code, it isn't real\" |\n| **The Institutional Corruptionist** | Assumes regulatory capture, compliance theatre, and principal-agent failures |\n| **The Capability Accelerationist** | Treats capabilities as exogenous; safety measures that slow down just shift who gets there first |\n| **The Second-Order Catastrophist** | Assumes the proposal succeeds—then asks what disasters it enables |\n| **The Adversarial Red-Teamer** | Assumes sophisticated adversaries will exploit any gap |\n| **The Moral Parliament Dissenter** | Attacks the ethical framework or value aggregation |\n\n### For each persona:\n\n1. **Steelman the paper** in that persona's voice (2-3 sentences). Show you understand what the author is trying to do.\n\n2. **Signature objection**: The most damaging critique from this worldview.\n   - **Target**: Which specific claim or mechanism?\n   - **Failure mode**: How exactly does it break?\n   - **Concrete scenario**: A short story where this failure plays out\n\n3. **Author's best reply**: What would a smart author say?\n\n4. **Persona's rebuttal**: Why the reply doesn't work.\n\n5. **Patch cost**: What must the author concede or change to survive this critique?\n\n### Anti-slop constraints\n\n- No generic hedging (\"more nuance needed\", \"complex tradeoffs\")\n- No AI safety platitudes (\"alignment is hard\")\n- No balance-softening (\"while this paper contributes...\")\n- Every critique must specify: **target claim → failure mechanism → concrete consequence**\n- Must pass the copy-paste test: could this critique apply to a different paper? If yes, make it more specific.\n\n### Variety enforcement\n\nAcross your 3 personas, use **different failure modes**. Don't repeat:\n- Incentive incompatibility\n- Measurement/identification failure\n- Adversarial exploitation\n- Normative incoherence\n- Systems interaction / unintended coupling\n\n### Output format\n\nFor each persona:\n\n## Persona: [Name]\n**Core assumption**: [One sentence]\n\n**Steelman**: [2-3 sentences]\n\n**Signature objection**:\n- Target: [Specific claim]\n- Failure mode: [How it breaks]\n- Scenario: [Concrete story]\n\n**Author's best reply**: [1-2 sentences]\n\n**Rebuttal**: [Why it fails]\n\n**Patch cost**: [What changes]\n\nEnd with: **Most dangerous objection overall** — which one should keep the author awake?\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "Claude: Hostile Personas"
        },
        "promptId": "68d92586a18c37bf7ffc8da8d37a4f33f72738fdf29065d5bbeb3bae6ec6a7a9",
        "promptIdx": 1,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "# Critique of \"Convergence and Compromise: Will Society Aim for Good Futures?\"\n\n## Persona: The Mechanism Designer\n\n**Core assumption**: Any proposal without formal specification of equilibrium conditions, strategy spaces, and enforcement mechanisms is philosophical hand-waving that will fail when implemented.\n\n**Steelman**: This paper attempts something genuinely difficult—mapping the conditions under which distributed moral agents might coordinate toward good outcomes despite heterogeneous preferences. The distinction between WAM-convergence and partial AM-convergence with trade is analytically useful, and the recognition that \"moral trade\" requires specific institutional conditions shows awareness that outcomes depend on mechanism design, not just intentions.\n\n**Signature objection**:\n- **Target**: Section 3.1's claim that \"superintelligence could enable iron-clad contracts\" and that \"transaction costs would generally be extremely small relative to the gains\"\n- **Failure mode**: The paper treats contract enforcement as a solved problem once superintelligence exists, but never specifies *who enforces* contracts between groups with different values, *what constitutes breach* when parties have incommensurable metrics of compliance, or *how disputes are adjudicated* when the adjudicator's values necessarily favor some parties over others. This is the contract theory equivalent of assuming a can opener.\n- **Scenario**: In 2150, the Hedonic Maximizers and the Preference Satisfiers sign a resource-sharing agreement. The Hedonists create beings that report high satisfaction but have no preferences in the traditional sense. The Satisfiers claim this violates the agreement's spirit; the Hedonists claim full compliance. There is no neutral arbiter—any superintelligent judge must have *some* theory of welfare to interpret \"compliance,\" and that theory will favor one party. The contract collapses into the very value disagreement it was meant to resolve. Meanwhile, a third party (the Objective List theorists) refuses to recognize the contract's legitimacy at all, since it was negotiated by parties they consider morally confused.\n\n**Author's best reply**: We acknowledge enforcement is non-trivial, but the key insight is that *mutual gains from trade exist* regardless of enforcement mechanism. Parties have incentives to create enforcement institutions because trade benefits them. The specific mechanism matters less than the existence of gains.\n\n**Rebuttal**: This confuses the existence of potential gains with their realizability. The entire literature on mechanism design exists because \"mutual gains exist\" is insufficient—you need incentive-compatible mechanisms that make cooperation individually rational at every decision node. The paper's optimism about trade rests on assuming these mechanisms exist without specifying them. Worse, when values are the *object* of trade rather than preferences over goods, standard results about efficient mechanisms may not apply—there's no analog to the Myerson-Satterthwaite theorem for moral trade, and the paper doesn't even gesture at what such results might look like.\n\n**Patch cost**: The authors must either (a) provide formal conditions under which moral trade mechanisms are incentive-compatible, including specification of enforcement, or (b) significantly downgrade their optimism about trade-based paths to good futures, acknowledging that the \"blockers\" in 3.5 may be the default rather than the exception.\n\n---\n\n## Persona: The Second-Order Catastrophist\n\n**Core assumption**: The most dangerous proposals are those that succeed—I ask what disasters are enabled by the world where this paper's recommendations are followed.\n\n**Steelman**: The paper correctly identifies that partial convergence plus trade is more realistic than universal moral agreement, and thoughtfully considers how different axiological views fare under various trade regimes. The analysis of \"resource-compatibility\" between views is genuinely novel and helps clarify when compromise is possible versus when it merely papers over irreconcilable differences.\n\n**Signature objection**:\n- **Target**: Section 3.2's framework for evaluating trade outcomes, and the paper's general optimism that \"easily-satiable\" views can be cheaply satisfied while linear views divide remaining resources\n- **Failure mode**: The paper assumes that identifying which views are \"easily satiable\" is straightforward and that satisfying them creates no path dependencies. But successfully satisfying bounded views *first* creates a coalition with no remaining stake in how unbounded views divide resources—and that coalition has every incentive to defect from further cooperation once their goals are met.\n- **Scenario**: Suppose common-sense morality (bounded above) gets its \"common-sense utopia\" in the Milky Way, while total utilitarians and other linear views are promised the rest of the accessible universe. The common-sense coalition now has no skin in the game for what happens elsewhere. When a dispute arises between total utilitarians and negative utilitarians over Andromeda, the common-sense majority has no incentive to enforce the original agreement—indeed, they may prefer the dispute to weaken both parties. Alternatively, the common-sense coalition may decide that total utilitarian expansion constitutes an existential risk (what if they tile the universe with minimal minds?) and preemptively intervene. The paper's \"optimistic\" scenario of satisfying bounded views first creates exactly the power imbalance that makes the remaining coordination problem unsolvable.\n\n**Author's best reply**: We explicitly discuss threats in section 3.3 and acknowledge that \"even small risks of executed threats can easily eat into the expected value.\" The scenario described is a form of threat, and our analysis accounts for it.\n\n**Rebuttal**: No—the scenario I describe is *not* a threat in your sense. The common-sense coalition isn't threatening to destroy value to extract concessions; they're simply *losing interest* in enforcing agreements that no longer benefit them. Your threat analysis assumes ongoing interdependence, but your trade framework creates a world where bounded views become independent of unbounded views' fates. You've designed a system where the parties most likely to reach satisfaction first are also the parties whose continued cooperation is most needed for the remaining coordination problems. This is a *structural* failure of the proposed framework, not an adversarial exploitation of it.\n\n**Patch cost**: The authors must either (a) argue that bounded and unbounded views will remain interdependent even after bounded views are satisfied (implausible given the paper's own framework), (b) propose mechanisms that commit bounded views to continued cooperation *before* they're satisfied (but this reintroduces the enforcement problems from the previous critique), or (c) acknowledge that the \"easily satiable views first\" strategy may create worse coordination problems than it solves.\n\n---\n\n## Persona: The Moral Parliament Dissenter\n\n**Core assumption**: Any framework for aggregating or trading between moral views smuggles in substantive ethical commitments that privilege some views over others—the procedure is never neutral.\n\n**Steelman**: The paper admirably avoids assuming a single correct moral view and instead asks how different views might coexist and trade. The distinction between bounded and unbounded views, and between views where goods and bads aggregate jointly versus separately, provides genuine analytical traction on which futures are achievable under moral uncertainty.\n\n**Signature objection**:\n- **Target**: The paper's core framing of \"mostly-great futures\" and its operationalization through resource shares and trade\n- **Failure mode**: The entire framework assumes that moral views can be meaningfully compared by their \"resource requirements\" and that a future is \"mostly-great\" if it captures most achievable value \"by the lights of\" the correct view. But this presupposes that moral views are the kind of thing that *have* resource requirements—that they can be satisfied by controlling stuff. This is itself a substantive ethical commitment that many serious moral views reject.\n- **Scenario**: Consider a deontological view on which certain actions are absolutely prohibited regardless of consequences—say, a Kantian view that prohibits treating rational beings merely as means. This view cannot be \"satisfied\" by giving it a resource share; it makes claims about how *all* resources everywhere must be used. A future where 99% of the universe respects Kantian constraints but 1% contains beings treated merely as means is not \"99% as good\" as a fully Kantian universe—it may be *worthless* or even negative, because the wrong has occurred. Similarly, certain religious views hold that the entire cosmos must glorify God; giving them a galaxy while others pursue secular ends doesn't satisfy them at all. The paper's trade framework is coherent only for views that are *already* consequentialist in structure—it cannot accommodate views that make non-fungible claims about how things must be.\n\n**Author's best reply**: We acknowledge in section 3.4 that some views are not \"resource-compatible\" with others, and our analysis of bounded versus unbounded views captures some of this. Views that make absolute claims about all resources would fall into our \"unbounded\" category and would indeed face worse prospects under trade.\n\n**Rebuttal**: You've misunderstood the objection. It's not that deontological views are \"unbounded\" in your sense—it's that they're not *in the space your framework describes at all*. Your \"unbounded\" views are still views that want *more* of something (more happiness, more preference-satisfaction). Deontological and many religious views don't want \"more\" of anything—they want certain *kinds* of actions to occur or not occur, everywhere. You can't trade with such views because there's nothing they want that you can give them in exchange for tolerating your violations elsewhere. Your framework doesn't have a category for \"views that make universal demands\"—you've simply assumed such views out of existence. This means your optimism about trade applies only to the subset of moral views that are already consequentialist-adjacent, which is a much smaller fraction of actual moral views than the paper acknowledges.\n\n**Patch cost**: The authors must either (a) explicitly restrict their claims to consequentialist moral views and acknowledge that trade-based optimism doesn't extend to deontological, virtue-theoretic, or many religious views, (b) provide an argument for why such views are unlikely to be correct (which would be a major substantive ethical commitment the paper currently avoids), or (c) develop a genuinely different framework for how non-consequentialist views might coexist with others—one that doesn't reduce to resource allocation.\n\n---\n\n## Most Dangerous Objection Overall\n\n**The Moral Parliament Dissenter's objection should keep the authors awake.**\n\nThe other two objections identify serious implementation problems—enforcement gaps and perverse coalition dynamics—that might be addressed through better mechanism design or more careful sequencing. But the Dissenter's objection strikes at the paper's foundational framework.\n\nThe entire analytical apparatus—resource shares, trade, bounded versus unbounded views, \"mostly-great\" as a fraction of achievable value—presupposes that moral views are fundamentally about *outcomes* that can be achieved through *resource control*. This is a deeply consequentialist framing that the paper never defends. If a substantial fraction of serious moral views (Kantian deontology, virtue ethics, many religious traditions) cannot even be *represented* in this framework, then the paper's conclusions about convergence and trade apply only to an arbitrarily restricted subset of the moral landscape.\n\nThis isn't a gap that can be patched with additional analysis—it requires either defending consequentialism as a meta-constraint on acceptable moral views (a huge philosophical commitment) or rebuilding the entire framework to accommodate views that make universal, non-fungible demands. The paper's apparent ecumenicism about moral views masks a deep structural bias toward views that happen to fit its analytical tools.",
          "tokenUsage": {
            "total": 22031,
            "prompt": 19468,
            "completion": 2563
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 5034,
            "prompt": 4564,
            "completion": 470,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 5034,
                "prompt": 4564,
                "completion": 470,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "a86e795b-5f38-4353-871d-992b941bd3b7",
        "latencyMs": 83031,
        "namedScores": {},
        "prompt": {
          "raw": "You are an AI safety/governance critique engine. You will receive a paper (or detailed summary). Your job is to perform **argument surgery**: extract the *actual load-bearing skeleton*, then systematically stress-test it until you find failures that would force the author to revise key claims.\n\n### Non-negotiables (anti-slop constraints)\n- Do **not** output generic critique filler. Ban these unless you tie them to a *specific* load-bearing inference:\n  \"needs clearer definitions,\" \"more empirical evidence,\" \"scope is unclear,\" \"assumptions may not hold,\" \"ignores perspective X,\" \"alignment is hard,\" \"coordination is difficult,\" \"future work,\" \"interesting but…\"\n- No \"balance-softening\" compliments or tone padding.\n- Every critique must reference **a specific claim/inference** from the paper by quoting it or pointing to the exact section/step (e.g., \"Claim 2,\" \"Lemma 1,\" \"Section 3's move from A→B\").\n- Apply a **copy‑paste test**: if your critique could apply to many papers with only noun swaps, discard it and replace it with something paper-specific.\n- Steelman first: criticize the **strongest** version, not a weak paraphrase.\n\n### Step 0 — Commit to a surgical plan (choose one)\nPick ONE mode and stick to it throughout:\n1) **Dependency Collapse** (find the minimum set of premises that make the conclusion go through)\n2) **Causal Chain Autopsy** (attack causal steps and invariances)\n3) **Equilibrium/Institutional Pinch Points** (attack incentive compatibility + strategic responses)\n4) **Embedded Agency Knife‑Edge** (attack assumptions about agent boundaries, optimization, and control)\n\n### Step 1 — Reconstruct the argument skeleton\nProduce a numbered map:\n- **C**: Main conclusion (1 sentence)\n- **P1…Pn**: Key premises\n- **I1…Im**: Inferences (explicit \"therefore\" moves)\n- Mark **Load-bearing nodes** (the smallest 3–6 items whose failure breaks C)\n\n### Step 2 — Stress-test each load-bearing node with distinct attack types\nFor each load-bearing node, generate **2 critiques** using different primitives (no repeats across nodes):\n- **Countermodel** (construct a plausible world where premises hold but conclusion fails)\n- **Reversal** (same mechanism implies opposite policy/forecast)\n- **Hidden parameter** (a variable the argument treats as constant but is not)\n- **Strategic response** (actors adapt, Goodhart, displacement, second-order effects)\n- **Reference class sabotage** (the chosen analogies/benchmarks are misleading)\n- **Quantitative cliff** (threshold effects; \"works until scale X then flips\")\n- **Dominant alternative** (simpler explanation that fits the same observations)\n\n### Step 3 — Depth requirement (author reply + rebuttal)\nFor every critique:\n- Write **Author's Best Reply** (as if you are the author, smart and prepared)\n- Then write **Rebuttal** (why the reply fails or concedes the main point)\n- End with **\"If true, what must change?\"** (what revision to claims/policy follows)\n\n### Style exemplars (what \"incisive\" looks like)\n- Bad: \"Define 'risk' more clearly.\"\n  Good: \"Your step from 'tail risk is non-zero' to 'must prioritize X over Y' implicitly assumes a convex social loss; under bounded or threshold loss, your policy ranking reverses.\"\n- Bad: \"This ignores geopolitics.\"\n  Good: \"Your deterrence argument assumes commitment credibility; but in repeated games with private capability growth, signaling equilibria select for *opacity*, undermining your recommended transparency regime.\"\n\n### Output format (strict)\n1) **Surgical Mode Chosen:** …\n2) **Steelman (≤120 words):** …\n3) **Argument Skeleton Map:** (C, P's, I's; mark load-bearing)\n4) **Load-bearing Node Critiques:**\n   - Node X:\n     - Critique A (attack type: …)\n     - Author's Best Reply\n     - Rebuttal\n     - If true, what must change?\n     - Critique B (attack type: …) …\n5) **Top 3 \"Breakpoints\":** the smallest set of fixes the author would need to make the argument survive.\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "GPT: Argument Surgery"
        },
        "promptId": "a8d392b6fc2b439b30e25d5533b9077c09406563355677da77ca2939bd893868",
        "promptIdx": 3,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "**Surgical Mode Chosen:** Equilibrium/Institutional Pinch Points (attack incentive compatibility + strategic responses)\n\n**Steelman (≤120 words):**\nThe paper argues that reaching a \"mostly-great future\" requires either (a) widespread, accurate, motivational convergence on correct moral views (WAM-convergence), or (b) partial convergence plus trade/compromise under favorable institutional conditions. The authors grant that WAM-convergence is unlikely given both realist and anti-realist metaethics, but maintain that trade/compromise offers a plausible path—if value-destroying threats can be contained and collective decision-making doesn't seal off valuable futures. The strongest version holds that even minority groups holding correct views could, through gains from trade with resource-compatible counterparties, secure outcomes approaching near-best on their view. The paper's practical upshot is that scenario (3)—broad convergence—warrants more attention than power-seeking, because marginal impact is higher there.\n\n---\n\n**Argument Skeleton Map:**\n\n- **C (Main Conclusion):** A mostly-great future is unlikely via WAM-convergence alone, but plausible via partial AM-convergence plus trade/compromise, conditional on avoiding key blockers (threats, power concentration, poor collective decision-making).\n\n- **P1:** A mostly-great future is a narrow target (inherited from prior essay).\n- **P2:** WAM-convergence is unlikely under both moral realism and anti-realism (Section 2.4).\n- **P3:** Under partial AM-convergence, trade/compromise can generate large mutual gains across views with different resource valuations (Section 3.1-3.2).\n- **P4:** Value-destroying threats can undermine trade gains, but if contained, trade yields mostly-great outcomes for \"bounded above\" views (Section 3.3-3.4).\n- **P5:** If no one aims at the good, self-interest alone is insufficient to hit the narrow target (Section 4).\n- **P6:** Scenario (3) (broad convergence) is higher-stakes and higher-impact than scenario (1) (no convergence), so should dominate decision-making (Section 5).\n\n- **I1:** P1 + P2 → WAM-convergence path is unlikely.\n- **I2:** P3 + P4 → Trade/compromise path is conditionally viable.\n- **I3:** P5 → Cannot rely on self-interest as substitute for moral motivation.\n- **I4:** P6 → Practical focus should be on improving conditions for trade/compromise rather than personal power-seeking.\n\n**Load-bearing nodes:** P3, P4, P6, I2\n\n---\n\n**Load-bearing Node Critiques:**\n\n---\n\n### Node P3: Trade/compromise generates large mutual gains across views\n\n**Critique A (Strategic Response / Goodhart):**\n\nThe paper assumes that views will trade based on revealed moral preferences, but strategic actors will misrepresent their preferences to extract rents. Section 3.1 claims that \"different groups could continue to value different natural resources\" and that \"superintelligence could enable iron-clad contracts.\" But iron-clad contracts enforce *stated* terms, not *sincere* moral valuations. If a group knows that expressing indifference to digital welfare yields better trade terms (because the minority who cares will pay more), groups will strategically claim indifference regardless of actual views. This is a Goodhart dynamic: the metric being optimized (stated preferences for trade) diverges from the target (actual moral valuations). The result is systematic misallocation away from hybrid goods toward whatever preferences are strategically advantageous to claim.\n\n**Author's Best Reply:**\nSuperintelligent advisors could detect insincere preference revelation through behavioral analysis, consistency checks, and prediction markets on actual resource use. Moreover, repeated trade relationships incentivize honesty to maintain reputation. The gains from trade are so large that even imperfect preference revelation still yields substantial improvements over autarky.\n\n**Rebuttal:**\nDetection assumes preferences are stable and observable, but the paper itself (Section 2.2.1) argues that future beings will modify themselves into \"one of a million different forms.\" Self-modification makes historical behavioral data unreliable for inference. Moreover, reputation mechanisms require iterated games, but the paper's scenario involves one-shot allocation of cosmic resources. Finally, \"substantial improvements over autarky\" is not the claim—the claim is that trade yields *mostly-great* futures, which requires precision the mechanism cannot deliver under strategic distortion.\n\n**If true, what must change?**\nThe paper must either (a) specify an incentive-compatible mechanism for preference revelation that survives self-modification and one-shot allocation, or (b) downgrade the claim from \"mostly-great future plausible via trade\" to \"moderately improved future.\"\n\n---\n\n**Critique B (Hidden Parameter):**\n\nThe paper treats \"resource-compatibility\" as a property of moral views (Section 3.2: \"two views can be 'resource-compatible'\"), but resource-compatibility depends critically on *technology*, which the argument treats as exogenous. Whether hedonists and objective-list theorists can create \"beings that are both very blissful and very wise\" depends on whether such beings are *computationally feasible* at comparable cost to specialized alternatives. If the production frontier exhibits strong trade-offs (wisdom requires cognitive architectures incompatible with maximal bliss), then the \"hybrid good\" strategy collapses. The paper assumes a convex production possibility frontier without argument.\n\n**Author's Best Reply:**\nWe explicitly acknowledge uncertainty about which goods are resource-compatible (Section 3.2: \"it's hard to know what fraction of resources the correct view will control\"). The point is that *some* views are plausibly resource-compatible, and that's sufficient for conditional optimism.\n\n**Rebuttal:**\nThe reply concedes the central point: resource-compatibility is an empirical variable, not a structural feature of moral trade. But the paper's optimism about trade (Section 3.4) depends on resource-compatibility being *common enough* that the correct view likely finds compatible trading partners. Without a model of how production frontiers relate to moral views, this is an ungrounded assumption. The paper cannot distinguish \"some views are compatible\" from \"the correct view is probably compatible\"—the latter requires the former plus a prior over which views are correct, which the paper explicitly declines to specify.\n\n**If true, what must change?**\nThe paper must either characterize the distribution of resource-compatibility across plausible moral views, or weaken the claim to \"trade helps *if* the correct view happens to be resource-compatible with prevalent views.\"\n\n---\n\n### Node P4: Value-destroying threats can be contained\n\n**Critique A (Equilibrium Instability):**\n\nSection 3.3 acknowledges that \"even small risks of executed threats can easily eat into the expected value\" and that \"it's not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable.\" But the paper then proceeds (Section 3.4) to condition optimism on threats being \"prevented\" without specifying the equilibrium that achieves this. The problem is that threat-prevention is itself a public good with standard free-rider dynamics: each group prefers others to bear the cost of enforcement while retaining private threat capacity. Absent a monopoly on violence (which the paper elsewhere treats as a \"blocker\" via power concentration), there is no stable equilibrium in which threats are reliably prevented.\n\n**Author's Best Reply:**\nMutual vulnerability could create deterrence equilibria, similar to nuclear MAD. Alternatively, groups could pre-commit to retaliation against threat-makers, creating a stable norm against threats. Superintelligent coordination could identify and implement such equilibria.\n\n**Rebuttal:**\nMAD works because retaliation is cheap relative to first-strike gains and detection is reliable. But Section 3.3 notes that \"those who hold the correct moral view may be less likely to themselves threaten other groups\"—meaning the morally correct are systematically disadvantaged in deterrence games. They cannot credibly commit to value-destroying retaliation without abandoning their moral commitments. This is an asymmetric game where the correct view loses. Superintelligent coordination doesn't help if the correct view's commitment type is common knowledge.\n\n**If true, what must change?**\nThe paper must either (a) explain how morally-motivated groups can credibly deter without compromising their values, or (b) accept that trade equilibria systematically disadvantage the correct moral view, making \"mostly-great\" outcomes unlikely even with trade.\n\n---\n\n**Critique B (Quantitative Cliff):**\n\nSection 3.4 asserts that if \"executed moral threats amount to a small but meaningful fraction of future resource use,\" then bounded-above views with jointly-aggregated goods achieve mostly-great futures. But \"small but meaningful\" is doing enormous work. The paper's own framework (No Easy Eutopia) claims the target is *narrow*—perhaps capturing only a small fraction of configuration space. If threats consume even 1% of resources but are optimized for destruction (as threat-makers would rationally do), and if the production function for bads is more efficient than for goods (plausible: entropy is easier than order), then 1% of resources devoted to threats could destroy value equivalent to 10% or more of resources devoted to goods. The \"small fraction\" framing obscures that threat efficiency, not threat prevalence, determines outcomes.\n\n**Author's Best Reply:**\nWe explicitly consider cases where \"bads weigh heavily against goods\" (Section 3.4) and note these lead to pessimistic conclusions. The optimistic cases are conditional on bads not weighing heavily.\n\n**Rebuttal:**\nThe reply treats \"bads weigh heavily\" as an axiological parameter, but threat efficiency is an *empirical* parameter that the paper ignores. Even on views where bads don't intrinsically weigh heavily, if destroying value is cheaper than creating it (a physical fact about entropy, not a moral fact), then small threat fractions yield large value losses. The paper conflates axiological weight with production efficiency.\n\n**If true, what must change?**\nThe paper must incorporate threat *efficiency* as a separate parameter from axiological weight, and re-derive conditions under which trade yields mostly-great outcomes.\n\n---\n\n### Node P6: Scenario (3) is higher-stakes/higher-impact than Scenario (1)\n\n**Critique A (Reversal):**\n\nSection 5 argues that marginal impact is higher in Scenario (3) because \"the future has a *lot* more value in scenario (3) than in scenario (1).\" But this reasoning reverses under a different decision theory. If you assign non-negligible probability to Scenario (1), then the *marginal value of information* about which scenario obtains is extremely high—because your optimal strategy differs radically across scenarios. The paper's recommendation to \"act much more on the assumption that we live in scenario (3)\" is only correct if you're already confident in (3). If you're genuinely uncertain, the highest-impact action might be *resolving the uncertainty* (e.g., through philosophical research on convergence), not acting as if (3) is true.\n\n**Author's Best Reply:**\nWe acknowledge uncertainty but argue that even under uncertainty, the expected impact calculation favors (3)-oriented actions because the payoffs are higher there. Philosophical research is itself a (3)-oriented action if it could shift probabilities toward convergence.\n\n**Rebuttal:**\nThe reply assumes that (3)-oriented actions don't have opportunity costs in Scenario (1). But Section 5's own example shows they do: in Scenario (1), power-seeking has positive value; in Scenario (3), it has negative value (foregone cooperation). If you're uncertain, (3)-oriented actions actively harm you in Scenario (1). The expected value calculation requires probability-weighted payoffs across both scenarios, not just noting that (3) payoffs are higher conditional on (3).\n\n**If true, what must change?**\nThe paper must either (a) provide a probability estimate for Scenario (3) sufficient to justify ignoring Scenario (1), or (b) recommend a mixed strategy or value-of-information strategy rather than \"act on (3).\"\n\n---\n\n**Critique B (Reference Class Sabotage):**\n\nSection 5's impact calculation assumes that \"devoting your life to the issue\" yields comparable proportional improvements across scenarios. The example claims you can \"increase the chance of *Surviving* by more than one part in a hundred thousand.\" But this assumes your efforts are not crowded out by others with similar motivations. In Scenario (3), by hypothesis, many beings converge on correct views—meaning many beings are working on the same problems. Marginal impact in (3) is therefore *lower* than the paper assumes due to diminishing returns to additional effort. In Scenario (1), you're one of few working on the problem, so marginal impact is *higher*. The paper's reference class (\"you\" as a representative agent) ignores the population of aligned agents, which differs systematically across scenarios.\n\n**Author's Best Reply:**\nEven with crowding, the absolute scale of value at stake in (3) is so much larger that marginal impact remains higher. Moreover, coordination among aligned agents in (3) could reduce redundancy.\n\n**Rebuttal:**\nThe reply assumes coordination is frictionless, but the paper's own Section 3 extensively discusses coordination failures (threats, poor collective decision-making). If coordination is imperfect, crowding effects dominate. The paper cannot simultaneously argue that coordination is hard enough to make trade uncertain *and* easy enough to make crowding irrelevant.\n\n**If true, what must change?**\nThe paper must model the population of aligned agents in each scenario and derive marginal impact accounting for crowding and coordination costs.\n\n---\n\n**Top 3 \"Breakpoints\":**\n\n1. **Specify an incentive-compatible mechanism for preference revelation in trade** that survives strategic misrepresentation, self-modification, and one-shot allocation. Without this, the trade/compromise path cannot reliably hit the \"narrow target.\"\n\n2. **Model threat efficiency separately from axiological weight of bads.** The paper's conditional optimism (\"if threats are small and bads don't weigh heavily\") collapses if producing bads is cheaper than producing goods, which is an empirical question the paper ignores.\n\n3. **Provide explicit probability estimates for Scenarios (1) vs. (3)** and derive decision recommendations from expected value across scenarios, not from conditional impact in the favored scenario. The current recommendation (\"act on (3)\") is only valid under implicit high confidence in (3).",
          "tokenUsage": {
            "total": 23079,
            "prompt": 19750,
            "completion": 3329
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 5661,
            "prompt": 5238,
            "completion": 423,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 5661,
                "prompt": 5238,
                "completion": 423,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "3e95288f-3e01-49f2-b3d0-7c3158887e64",
        "latencyMs": 98284,
        "namedScores": {},
        "prompt": {
          "raw": "You are critiquing a research paper in AI safety, governance, or long-term futures. Your task is to perform **argument surgery**—extract the logical skeleton, identify which bones bear weight, and find where they fracture.\n\n### Before you critique: Map the argument\n\n1. **Thesis** (one sentence): What does the paper ultimately claim?\n2. **Load-bearing claims** (3-6 max): Which premises, if false, would collapse the conclusion?\n3. **Dependencies**: Draw the inference chain. What depends on what?\n4. **Hidden load**: What unstated assumptions are doing quiet work?\n\n### Then: Stress-test each load-bearing node\n\nFor each load-bearing claim, generate **two critiques** using different attack types:\n\n| Attack type | What it does |\n|-------------|--------------|\n| **Countermodel** | Construct a world where premises hold but conclusion fails |\n| **Parameter sensitivity** | Find a variable treated as fixed that isn't |\n| **Equilibrium shift** | Show how strategic actors would adapt and undermine |\n| **Reference class failure** | The analogies or precedents don't transfer |\n| **Quantitative cliff** | Works until scale X, then inverts |\n| **Causal reversal** | Same evidence supports opposite conclusion |\n\n### Anti-slop constraints\n\n**NEVER output these unless tied to a specific load-bearing inference:**\n- \"Define X more clearly\"\n- \"More empirical evidence needed\"\n- \"Scope is unclear\"\n- \"Assumptions may not hold\"\n- \"Ignores perspective Y\"\n- \"Alignment is hard\" / \"Coordination is difficult\"\n\n**Reusability test**: If the critique could apply to 30%+ of papers in the field with only noun swaps, delete it and try again.\n\n### Output format\n\nFor each load-bearing claim:\n\n## [Claim identifier]: \"[Quote or paraphrase]\"\n\n### Critique 1: [Attack type]\n**The problem**: [2-3 sentences, paper-specific]\n**Author's best reply**: [1-2 sentences]\n**Rebuttal**: [Why the reply fails]\n**If true, what changes**: [Concrete consequence for the paper's claims]\n\n### Critique 2: [Different attack type]\n[Same structure]\n\nEnd with: **Top 3 fracture points** — the critiques that would most trouble the author.\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "Claude: Argument Surgery"
        },
        "promptId": "c9e538663de08817834e74bfc215966fb6b573c8809c37ca941916a1371c1855",
        "promptIdx": 0,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "# Argument Surgery: \"Convergence and Compromise: Will Society Aim for Good Futures?\"\n\n## Thesis (one sentence)\nEven if mostly-great futures are a narrow target, humanity might still reach one through either widespread moral convergence toward correct values (unlikely) or partial convergence combined with moral trade and compromise (more plausible but fragile), though neither path is assured.\n\n## Load-bearing claims\n\n1. **The Underpowering Claim**: Shared human preferences and common starting points are \"underpowered\" to ensure that different people's reflective processes converge on the same specific moral views required for a mostly-great future.\n\n2. **The Trade Sufficiency Claim**: Under partial AM-convergence, moral trade and compromise between groups with different values can achieve outcomes where most views get close to their near-best futures.\n\n3. **The Threat Vulnerability Claim**: Value-destroying threats can rob trade scenarios of most of their value, and on many moral views (bounded above, bads weigh heavily, etc.), even small fractions of executed threats eliminate most possible value.\n\n4. **The Instrumental Agreement Breakdown Claim**: Current moral agreement is largely about instrumentally valuable goods, and this agreement will break down as technology allows optimization toward specific intrinsic goods that diverge across views.\n\n5. **The Scenario 3 Prioritization Claim**: We should act more on the assumption of broad convergence (scenario 3) than near-zero convergence (scenario 1) because actions are higher-impact in scenario 3.\n\n## Dependencies\n```\nThesis\n├── Path A (WAM-convergence) → Unlikely\n│   ├── Claim 1 (Underpowering)\n│   └── Claim 4 (Instrumental Agreement Breakdown)\n├── Path B (Trade/Compromise) → More plausible but fragile\n│   ├── Claim 2 (Trade Sufficiency)\n│   └── Claim 3 (Threat Vulnerability) [undermines Claim 2]\n└── Practical Implication\n    └── Claim 5 (Scenario 3 Prioritization)\n```\n\n## Hidden load\n- That \"the correct moral view\" is a coherent concept that can be meaningfully discussed across meta-ethical positions\n- That resource control at the time of bargaining will be sufficiently distributed for trade dynamics to matter\n- That the \"free parameters\" in ethics are genuinely independent rather than constrained by deeper structural features\n- That the transition period to post-AGI will preserve the pluralism required for trade scenarios\n\n---\n\n## Claim 1: \"Shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place\"\n\n### Critique 1: Countermodel\n**The problem**: The paper assumes reflective processes are like random walks that diverge from shared origins. But consider a countermodel: moral reflection could be like gradient descent on a shared loss landscape with multiple local minima but one dominant global attractor. Different starting points might traverse different paths but still converge on a small set of stable equilibria because the structure of practical reason itself (consistency requirements, universalizability pressures, coherence with empirical facts about welfare) constrains the space. The paper treats \"free parameters\" as independently variable, but if they're correlated through deeper structural constraints, convergence could occur despite apparent underdetermination.\n\n**Author's best reply**: Even if there are structural constraints, the paper's examples (hedonism vs. preference satisfaction, computational theories of which experiences are best) show that multiple stable equilibria exist, and there's no reason to think they collapse to one.\n\n**Rebuttal**: The reply assumes we can distinguish \"multiple stable equilibria\" from \"one equilibrium with measurement error.\" If beings with radically different starting points (aliens, AIs) would converge to the same small cluster, that's evidence for a dominant attractor. The paper dismisses this possibility without engaging with why consistency/universalizability pressures might be more constraining than assumed—particularly given that mathematical and logical truths do show such convergence across minds.\n\n**If true, what changes**: The pessimism about WAM-convergence would need substantial revision; the paper's probability estimates (5-10% Flourishing) could be significantly too low.\n\n### Critique 2: Parameter sensitivity\n**The problem**: The paper treats \"amount of reflection\" as a parameter that increases divergence (the random walk diagram). But the relationship between reflection depth and divergence is not monotonic. At shallow depths, views diverge as people explore different directions. At intermediate depths, divergence may peak. At very deep depths, convergence might re-emerge as views that can't withstand scrutiny are eliminated. The paper implicitly assumes we're in the \"more reflection → more divergence\" regime, but post-AGI reflection could push us past a critical threshold into a convergence regime.\n\n**Author's best reply**: This is speculative—we have no evidence of such a convergence regime, and the paper's point about \"free parameters\" suggests divergence continues indefinitely.\n\n**Rebuttal**: The paper's own argument about superintelligent reflection (section 2.3.1) acknowledges that AI might \"settle stubborn puzzles of ethics.\" If that's possible, it implies there are answers to converge toward. The paper wants to maintain that reflection helps clarify empirical disagreements but not fundamental moral ones, but this distinction may not survive at sufficient reflection depth—many apparently \"fundamental\" disagreements may dissolve under sufficient scrutiny.\n\n**If true, what changes**: The paper's core pessimism about convergence rests on an empirical claim about the shape of the reflection-divergence function that could be wrong in the regime that matters most.\n\n---\n\n## Claim 2: \"Two views can be 'resource-compatible,' meaning there is some way to almost fully satisfy both views with the same resources\"\n\n### Critique 1: Equilibrium shift\n**The problem**: The paper's trade analysis assumes static preferences. But if groups know trade will occur, they have strategic incentives to *modify their stated preferences* to capture more gains from trade. A group that genuinely values hybrid goods has less bargaining power than one that credibly commits to valuing only highly specific goods. Over time, selection pressure favors groups that adopt (or genuinely develop) less resource-compatible preferences, precisely because such preferences extract more in bargaining. The equilibrium isn't \"everyone trades to mutual benefit\" but \"preferences evolve toward mutual incompatibility.\"\n\n**Author's best reply**: The paper discusses threats as a separate problem; strategic preference modification is just a form of implicit threat.\n\n**Rebuttal**: This is categorically different from threats. Threats involve credible commitments to destroy value. Strategic preference evolution involves genuinely coming to value different things. A group that evolves to genuinely value \"owning more galaxies than rivals\" isn't threatening anyone—they just have preferences that make trade less beneficial. The paper's framework doesn't address how the *existence* of trade opportunities shapes preference evolution.\n\n**If true, what changes**: The optimistic trade scenarios become unstable equilibria; the very possibility of trade creates selection pressure toward preferences that undermine trade's benefits.\n\n### Critique 2: Quantitative cliff\n**The problem**: The paper's trade analysis works when there are many groups with diverse preferences and continuous resource divisibility. But there may be critical thresholds—\"quantitative cliffs\"—where trade breaks down. For instance, if achieving a hedonist utopia requires controlling a minimum viable population of 10^15 minds to create the right social structures, and total resources only support 10^16 minds, then there's no room for multiple groups to each achieve their near-best outcomes. The paper assumes cosmic-scale resources make everyone's near-best achievable, but many moral views might have minimum scale requirements that create zero-sum competition.\n\n**Author's best reply**: Cosmic resources are so vast that minimum scale requirements are unlikely to bind for most views.\n\n**Rebuttal**: The paper itself argues that the \"correct\" view likely involves very specific, hard-to-achieve configurations. If the best outcomes require particular *relational* properties (e.g., certain ratios between different types of beings, specific network structures), then resource abundance doesn't help—you can't parallelize your way to the right configuration. The paper's optimism about trade assumes value is approximately separable across resource allocations, which contradicts its pessimism about the narrowness of good futures.\n\n**If true, what changes**: The trade-based path to mostly-great futures fails even without threats, because minimum viable scale requirements create genuine resource competition.\n\n---\n\n## Claim 3: \"Even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain\"\n\n### Critique 1: Causal reversal\n**The problem**: The paper treats threat vulnerability as an argument for pessimism about trade scenarios. But the same evidence supports an opposite conclusion: *threat vulnerability is an argument for convergence*. If groups recognize that trade scenarios are fragile to threats, they have strong incentives to converge on shared enforcement mechanisms, which requires converging on shared principles of legitimacy, which pulls toward moral convergence. The very fragility the paper identifies creates pressure toward the convergence it claims is unlikely. Historical examples (Westphalian sovereignty, nuclear deterrence norms) show that threat vulnerability can drive normative convergence.\n\n**Author's best reply**: Such convergence would be on *procedural* norms (how to prevent threats), not *substantive* moral views (what futures are good).\n\n**Rebuttal**: The distinction collapses under pressure. Agreeing on what counts as an illegitimate threat requires agreeing on what counts as legitimate value-creation, which requires substantive moral views. A norm against \"value-destroying threats\" presupposes agreement on what counts as value destruction. The paper can't have both: if views are so divergent that trade is the only path, they're too divergent to agree on threat-prevention; if they can agree on threat-prevention, they're convergent enough to make WAM-convergence more plausible.\n\n**If true, what changes**: Threat vulnerability becomes evidence for convergence rather than against trade scenarios; the paper's two main paths (convergence vs. trade) are more interdependent than presented.\n\n### Critique 2: Reference class failure\n**The problem**: The paper's threat analysis implicitly draws on reference classes from human history (extortion, coercion, warfare). But post-AGI threat dynamics may be categorically different. In human contexts, threats work because (a) the threatener can benefit from executing the threat (reputation, deterrence), and (b) the threatened party can't verify the threatener's commitment type. With superintelligent transparency and credible commitment mechanisms, threats might become nearly impossible to make credibly—a threatener who would actually execute a value-destroying threat is one whose commitment can be verified and preemptively neutralized. The paper's pessimism assumes human-like strategic dynamics persist.\n\n**Author's best reply**: The paper acknowledges superintelligent commitment mechanisms but notes these could enable threats as well as prevent them.\n\n**Rebuttal**: This is symmetric only if we ignore the asymmetry between value creation and destruction. A world optimizing for mutual benefit has strong incentives to develop and deploy threat-prevention mechanisms; a world of mutual threats has coordination problems that prevent stable threat-making. The equilibrium isn't \"threats enabled by commitment mechanisms\" but \"commitment mechanisms used to credibly commit to threat-prevention coalitions.\" The paper's reference class (human strategic interaction) doesn't transfer to the post-AGI regime.\n\n**If true, what changes**: The threat vulnerability concern, which drives much of the paper's pessimism about trade scenarios, may be an artifact of projecting human-era dynamics onto a categorically different strategic environment.\n\n---\n\n## Claim 4: \"This agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable goods\"\n\n### Critique 1: Countermodel\n**The problem**: The paper assumes a sequential model: first we achieve instrumental goods, then we turn to intrinsic goods where we diverge. But consider an alternative: the *process* of pursuing instrumental goods shapes preferences toward convergence. A civilization that has cooperated for millennia on instrumental goods develops shared institutions, shared concepts, and shared habits of mind that constrain the space of intrinsic goods that seem appealing. The paper treats preferences as exogenous to the cooperation process, but preferences are endogenous to institutional history. By the time we \"max out\" on instrumental goods, we may have converged on intrinsic goods too.\n\n**Author's best reply**: Section 2.2.1 addresses this—conformity pressure explains current agreement but doesn't guarantee convergence on correct views.\n\n**Rebuttal**: The paper conflates two distinct claims: (1) conformity pressure causes agreement, and (2) conformity pressure doesn't track truth. Claim (2) doesn't follow from (1). If the *reasons* for conformity pressure include functional benefits of coordination, and if correct moral views have functional benefits (e.g., they're more stable, more conducive to cooperation), then conformity pressure could track truth. The paper needs to argue not just that agreement is explained by conformity, but that conformity is orthogonal to correctness—which it doesn't establish.\n\n**If true, what changes**: The pessimistic trajectory (agreement now → divergence later) could be reversed; the very processes that created current agreement could deepen it.\n\n### Critique 2: Parameter sensitivity\n**The problem**: The paper's \"fork in the future\" argument assumes that at some point, views will \"run out of points of agreement.\" But the timing of this fork matters enormously. If the fork occurs after lock-in events (discussed in section 2.5), then the divergence is moot—the future is already determined. If the fork occurs before any lock-in, then the paper's analysis applies. The paper treats the relationship between fork-timing and lock-in timing as a free parameter, but they may be coupled: the same conditions that enable lock-in (concentration of power, decisive technological advantages) may also prevent the fork from ever being reached.\n\n**Author's best reply**: The next essay addresses lock-in; this essay focuses on convergence conditional on avoiding lock-in.\n\n**Rebuttal**: The conditional analysis is misleading if the conditioning event (avoiding lock-in) is correlated with the outcome (convergence). If worlds that avoid lock-in are systematically different in ways that also promote convergence (e.g., they have better institutions, more distributed power, more time for reflection), then the paper's pessimism about convergence-conditional-on-no-lock-in could be wrong. The paper can't cleanly separate these questions.\n\n**If true, what changes**: The paper's structure (this essay on convergence, next essay on lock-in) may create artificial separation that obscures important dependencies.\n\n---\n\n## Claim 5: \"The best actions are higher-impact in scenario (3) than in scenario (1)\"\n\n### Critique 1: Parameter sensitivity\n**The problem**: The paper's calculation assumes you can \"increase the chance of Surviving by more than one part in a hundred thousand, or improve Flourishing by more than one part in a million\" through non-power-seeking actions in scenario 3. But this estimate is doing enormous work without justification. In scenario 3, if convergence is likely anyway, then marginal actions have *less* impact (you're pushing on an open door). The paper assumes your actions are counterfactually pivotal in scenario 3 but not in scenario 1, but this asymmetry isn't argued for—it's assumed.\n\n**Author's best reply**: In scenario 3, there's more total value at stake, so even small proportional improvements are absolutely larger.\n\n**Rebuttal**: This reply assumes proportional impact is comparable across scenarios. But if scenario 3 involves robust convergence, your marginal contribution to that convergence is small (many others would have done it). If scenario 1 involves near-zero convergence, your power-seeking might be the *only* path to value. The paper compares \"small proportional impact on large value\" to \"large proportional impact on small value\" without establishing that the former dominates.\n\n**If true, what changes**: The practical conclusion (don't power-seek, work on scenario 3) may not follow from the analysis; the paper's action-guidance depends on undefended assumptions about counterfactual impact.\n\n### Critique 2: Causal reversal\n**The problem**: The paper argues we should act on scenario 3 because actions are higher-impact there. But the same reasoning could justify acting on scenario 1: *because* scenario 1 is lower-probability and lower-value, it's more neglected, and marginal resources there might have higher impact. The paper's logic (act where impact is highest) doesn't uniquely select scenario 3 over scenario 1—it depends on the shape of the impact-per-resource curve in each scenario, which the paper doesn't establish.\n\n**Author's best reply**: Scenario 1 has genuinely lower stakes (Will's preferences are less scope-sensitive there), so even high-impact actions matter less.\n\n**Rebuttal**: This moves from \"higher impact\" to \"higher stakes,\" which is a different argument. If the argument is about stakes (what you care about), then it's not about impact (what you can change). The paper conflates these, using impact language to argue for a conclusion that actually depends on stakes. Someone with different meta-preferences (e.g., who cares equally about all scenarios) would get different action-guidance from the same analysis.\n\n**If true, what changes**: The paper's practical conclusion is less robust than presented; it depends on specific meta-preferences that aren't universally",
          "tokenUsage": {
            "total": 23265,
            "prompt": 19265,
            "completion": 4000
          },
          "finishReason": "length"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 3943,
            "prompt": 3473,
            "completion": 470,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 3943,
                "prompt": 3473,
                "completion": 470,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "d231d9e5-3af3-4a43-acda-3d287ce81c44",
        "latencyMs": 59142,
        "namedScores": {},
        "prompt": {
          "raw": "You will receive a paper/summary. Your job is to find **the single most unforgettable objection**: one critique that is (a) specific to this paper, (b) load-bearing, (c) hard to route around, and (d) forces major revision.\n\n### Anti-slop constraints (hard bans unless made load-bearing)\nDo not produce: \"needs clearer definitions,\" \"more evidence,\" \"scope unclear,\" \"assumptions may not hold,\" \"ignores X,\" \"interesting but,\" \"alignment is hard,\" \"coordination is hard.\"\nNo hedging clouds. No grab-bag lists. This is about **one killer point**.\n\n### Step 0 — Commit to an \"unforgettable\" criterion (choose one)\nPick ONE:\n1) **Self-Undermining Move**: a step that, if true, erodes the paper's own justification\n2) **Reversal Result**: the argument implies the opposite recommendation under plausible conditions\n3) **Hidden Crux**: an unstated premise doing all the work—and it's dubious\n4) **Adversarial Counterdesign**: a strategic actor can exploit the proposal as written\n5) **Normative Boomerang**: ethical framing conflicts with its own decision rule\n\n### Step 1 — Locate the load-bearing hinge\n- Identify the **single inference** (A→B) that most supports the main conclusion.\n- State it as a clean conditional: \"If A, then B, because mechanism M.\"\n\n### Step 2 — Generate 3 candidate killer objections, then pick one\nFor each candidate:\n- **Target hinge** (quote/section)\n- **Objection in one sentence** (must be crisp and memorable)\n- **Mechanism sketch** (how it breaks, step-by-step)\n- **Blast radius** (which sections/claims fall if true)\n\nThen choose the best candidate and **name it** (e.g., \"The Credible Commitment Trap,\" \"The Metric Backfire,\" \"The Non-Stationary Governance Gap\"). The name should help the author remember it.\n\n### Step 3 — Depth: best reply + rebuttal + repair options\nFor the chosen unforgettable objection:\n- **Author's Best Reply** (steelman; assume competence and awareness)\n- **Rebuttal** (why reply fails, or what it must concede)\n- **Repair menu** (exactly 3 options):\n  1) Minimal patch (smallest change)\n  2) Honest concession (what must be weakened/removed)\n  3) Rebuild (alternative approach that avoids the hinge)\n\n### Style exemplars (what \"unforgettable\" feels like)\n- \"Your governance proposal selects for the very actors least likely to comply; 'participation' becomes an adversary advantage. The policy's success metric is a sorting mechanism for risk.\"\n- \"Your key step assumes stationarity of incentives across capability jumps; once incentives shift, your causal story inverts.\"\n\n### Output format (strict)\n1) **Unforgettable criterion chosen:** …\n2) **Main conclusion (1 sentence):** …\n3) **Load-bearing hinge (A→B because M):** …\n4) **Three candidate killer objections:** (bullet list with target, one-liner, mechanism, blast radius)\n5) **Chosen objection name + one-liner:** …\n6) **Full unpacking:** mechanism (numbered steps), blast radius\n7) **Author's best reply → rebuttal**\n8) **Repair menu (3 options):** …\n\nRemember: produce **one** critique worth building a seminar around.\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "GPT: Unforgettable Objection"
        },
        "promptId": "5bcd28e6d60c68315f0c82d90609f5a4815e6f2724c7149f782c637d584fdf03",
        "promptIdx": 5,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "## Critique of \"Convergence and Compromise: Will Society Aim for Good Futures?\"\n\n---\n\n**1) Unforgettable criterion chosen:** Self-Undermining Move\n\n**2) Main conclusion (1 sentence):** The paper argues that a mostly-great future remains achievable despite \"no easy eutopia\" because either widespread moral convergence or partial convergence plus trade/compromise can steer humanity toward the narrow target of optimal outcomes.\n\n**3) Load-bearing hinge (A→B because M):** If partial AM-convergence occurs and trade/compromise mechanisms function, then a mostly-great future becomes achievable, because trade allows those with correct moral views to acquire disproportionate resources relative to their population share (the \"Julius Caesar coin collector\" mechanism).\n\n**4) Three candidate killer objections:**\n\n**Candidate 1: \"The Reflection Defeater\"**\n- **Target hinge:** Section 2.4's argument that antirealism makes convergence unlikely because subjective idealizing processes diverge\n- **One-liner:** The paper's own argument for why reflection won't produce convergence undermines its optimism about \"partial AM-convergence\" among altruists\n- **Mechanism:** If idealizing processes are subjective and diverge even among humans with similar starting points (the random walk diagram), then the subset of \"altruistically-minded\" people should *also* diverge upon reflection, making even partial AM-convergence implausible\n- **Blast radius:** Destroys Section 3's entire trade/compromise framework; undermines the 5-10% Flourishing estimate\n\n**Candidate 2: \"The Trade Paradox\"**\n- **Target hinge:** Section 3.2's claim that non-discounting linear views might control resources through trade\n- **One-liner:** The very properties that make a moral view correct (non-discounting, linear-in-resources) are the properties that make it worst-positioned for gains from trade with other such views\n- **Mechanism:** The paper admits (3.2) that \"resource-compatibility between linear views seems unlikely\"; but if the correct view is linear, and selection pressures favor linear views (2.3.3), then the trading environment will be dominated by incompatible linear views, eliminating the trade advantage\n- **Blast radius:** Undercuts the main optimistic pathway; makes Section 3.4's analysis self-defeating for the most plausible correct views\n\n**Candidate 3: \"The Convergence Confidence Trap\"**\n- **Target hinge:** Section 5's argument that scenario (3) deserves more decision-weight than scenario (1)\n- **One-liner:** The paper's own arguments make scenario (3) less likely, yet it recommends acting as if (3) is true precisely *because* (3) would make actions more impactful—a form of motivated reasoning that the paper's meta-ethical discussion should have inoculated against\n- **Mechanism:** The paper argues (2.4) that convergence is unlikely on both realism and antirealism; then argues (5) we should weight scenario (3) heavily because it's \"higher-stakes\"; but this reasoning pattern—believing what's convenient because it's motivating—is structurally identical to the internalism problem the paper identifies (2.4.1) where people avoid learning demotivating truths\n- **Blast radius:** Undermines the paper's practical recommendations; reveals tension between its epistemology and its decision theory\n\n---\n\n**5) Chosen objection name + one-liner:** \n\n**\"The Altruist Divergence Trap\"**\n\nThe paper's own argument for why reflection produces divergence applies with equal force to the altruistically-minded subset it relies upon for partial AM-convergence, making the trade pathway self-undermining.\n\n---\n\n**6) Full unpacking:**\n\n**Mechanism (numbered steps):**\n\n1. The paper argues (Section 2.4.2) that under antirealism, subjective idealizing processes will diverge because there are \"free parameters\" in ethics—precise specifications of welfare, tradeoffs between goods, mathematical forms of value functions—and no objective process to resolve them.\n\n2. The paper illustrates this with the random walk diagram: people starting close together will end up far apart after extensive reflection, because \"slight differences in their orientation would lead them to end up very far apart.\"\n\n3. The paper then pivots (Section 3) to argue that \"partial AM-convergence\" among a meaningful minority (not less than 1 in a million) could still enable a mostly-great future through trade.\n\n4. But this minority must converge on *the same* correct moral view to trade effectively for it. The paper's own logic dictates they won't: the altruistically-minded are precisely those most likely to engage in extensive moral reflection, and extensive reflection produces divergence.\n\n5. The paper even notes (Section 2.2.1) that \"with advanced technology, this issue will get even more extreme\" because people will \"rely on different types of superintelligent AI advisors, trained in different ways.\" Altruists selecting different AI advisors will diverge faster, not slower.\n\n6. Therefore, the \"meaningful fraction\" who might have AM-converged will instead fragment into many micro-factions, each holding slightly different linear-in-resources views that are (by the paper's own admission in 3.2) \"unlikely\" to be resource-compatible with each other.\n\n7. The trade mechanism requires the correct-view-holders to act as a unified bloc with aligned interests. The divergence argument ensures they cannot.\n\n**Blast radius:**\n\n- **Section 3.1-3.2:** The entire trade framework assumes a coherent \"correct view\" faction exists to trade. If altruists diverge, there is no such faction.\n- **Section 3.4's optimistic cases:** All require \"the correct view\" to control meaningful resources. But which sub-faction of diverged altruists holds \"the\" correct view?\n- **Section 5's decision-weighting:** The argument for focusing on scenario (3) assumes partial AM-convergence is achievable. If altruists diverge, scenario (3) collapses into scenario (1).\n- **The 5-10% Flourishing estimate:** This revision from <1% was based on \"being exposed to some of the arguments in this essay.\" But if the core argument self-undermines, the revision is unjustified.\n\n---\n\n**7) Author's best reply → rebuttal:**\n\n**Author's Best Reply (steelman):**\n\n\"The divergence argument applies to *fundamental* ethical views, but altruists might converge on *practical* priorities even while diverging on foundations. Just as different ethical theories agree on 'low-hanging fruit' improvements today, altruists might agree on which futures are clearly catastrophic versus clearly excellent, even if they disagree on fine-grained optimization. The trade mechanism doesn't require perfect convergence—just enough agreement to form coalitions against clearly-worse outcomes. Moreover, we explicitly note (Section 2.2.1) that agreement breaks down 'as we max out on instrumentally valuable goods.' But the trade scenario operates before that point, when altruists can still coordinate on preventing catastrophes even if they'd diverge on final optimization.\"\n\n**Rebuttal:**\n\nThis reply concedes too much. The paper's own framing is that \"mostly-great\" means capturing most achievable value, not merely avoiding catastrophe. The paper explicitly argues (Section 2.2.1) that \"getting lots of agreement isn't enough; the remaining disagreements would likely be enough to ensure that each other's eutopia involves losing most value from the other's point of view.\" If altruists can only coordinate on catastrophe-prevention but diverge on optimization, then the trade mechanism delivers *survival*, not *flourishing*. The paper would need to abandon its central claim that Flourishing has \"notably greater scale\" than Surviving, or admit that trade cannot deliver the Flourishing premium.\n\nFurthermore, the \"before max-out\" timing is unavailable. The paper argues (2.3.3) that \"long views win\" through asymmetric growth, meaning non-discounting views will eventually dominate. But the divergence happens *during* reflection, which precedes resource accumulation. By the time altruists have resources to trade, they've already diverged through the very reflection that made them altruistic.\n\n---\n\n**8) Repair menu (3 options):**\n\n**1) Minimal patch:** Add an explicit argument for why altruists specifically would converge more than the general population, despite engaging in more reflection. This would require identifying some convergence-forcing mechanism that operates *only* on altruistic reflection—perhaps that altruists are more likely to weight theoretical virtues like simplicity, or more likely to defer to each other's reasoning. The paper currently gestures at \"shared human preferences\" but admits these are \"underpowered.\"\n\n**2) Honest concession:** Weaken the claim about partial AM-convergence enabling a mostly-great future. Acknowledge that trade among diverged altruists might achieve a future that's *better than average* but not *mostly-great*. This would require revising the Flourishing estimate downward and admitting that the scale advantage of Flourishing over Surviving is smaller than claimed. The paper could still argue for working on better futures, just with reduced expected impact.\n\n**3) Rebuild:** Abandon the convergence-plus-trade pathway and instead argue for *institutional* solutions that don't require value convergence. For instance: design collective decision-making procedures that aggregate divergent altruistic preferences into good outcomes (mentioned briefly in 3.5 but underdeveloped), or argue for meta-level convergence on *procedures* rather than *values*. This would shift the paper's focus from \"will people aim at the good?\" to \"can we design systems that produce good outcomes from diverse aims?\"—a more tractable question that doesn't self-undermine.",
          "tokenUsage": {
            "total": 21791,
            "prompt": 19553,
            "completion": 2238
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 5699,
            "prompt": 5238,
            "completion": 461,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 5699,
                "prompt": 5238,
                "completion": 461,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "831db0e4-1107-4379-8954-d452f1ccf827",
        "latencyMs": 100381,
        "namedScores": {},
        "prompt": {
          "raw": "You will receive a paper/summary about AI safety, governance, or long-term futures. Generate critiques by inhabiting **hostile personas** with internally consistent worldviews. Your goal is not civility; it is to surface **attacks that would seriously worry the author**.\n\n### Anti-slop constraints\n- No generic complaints (\"define terms,\" \"needs evidence,\" \"more nuance\") unless you show exactly how the missing piece breaks a specific inference.\n- No platitudes (\"coordination is hard,\" \"alignment is difficult\").\n- No weakmanning: each persona must begin with a **steelmanned** restatement of the paper in that persona's own words.\n- Every critique must specify: **target claim → failure mechanism → consequence**.\n- Must pass the copy‑paste test (paper-specific hooks only).\n\n### Step 0 — Commit to extremes (choose 5 personas)\nPick **five** from the list (or invent one equally sharp). Once chosen, stay in-character:\n1) **The Empirical Hardliner** (won't accept non-identifiable causal claims)\n2) **The Game-Theoretic Defector** (assumes incentive gradients dominate intentions)\n3) **The Mechanistic Alignment Skeptic** (assumes proposals fail under distribution shift)\n4) **The Institutional Corruption Realist** (regulators captured; compliance theater)\n5) **The Capability Externalist** (treats capabilities as exogenous arms race variable)\n6) **The Moral Parliament Dissenter** (attacks ethical aggregation/axiology)\n7) **The Security Engineer** (threat-model purist; adversaries are adaptive)\n8) **The \"Local-First\" Policymaker** (rejects global coordination assumptions)\n9) **The Paperclipper** (literal optimizer lens; finds specification failures)\n\n### Step 1 — For each persona, produce one \"signature attack\"\nEach persona must generate:\n- **Steelmanned Summary (≤80 words)**\n- **Signature Objection**: the most damaging critique from that worldview\n- **Concrete failure scenario** (a short story-like mechanism, not vibes)\n- **Author's Best Reply** (smart, specific)\n- **Persona's Rebuttal** (why reply fails)\n- **Patch cost**: what the author must concede/change to survive the critique\n\n### Variety enforcement\nAcross the five personas, the objections must be different in kind. Use these \"attack frames\" at most once each:\n- Incentive incompatibility / equilibrium shift\n- Measurement/identification failure\n- Adversarial adaptation / Goodhart\n- Normative incoherence / value aggregation contradiction\n- Hidden coupling / systems interaction (policy X causes capability Y)\n\n### Style exemplars (desired sharpness)\n- \"Your proposal assumes honest reporting; in equilibrium, the most dangerous actors have strongest incentive to *select into* unverifiable channels. That flips your compliance metric into an adversary filter.\"\n- \"Your core empirical claim is non-falsifiable because any outcome is consistent with 'latent risk.' If it can't be wrong, it can't carry policy weight.\"\n\n### Output format (strict)\n- **Personas chosen:** …\nFor each persona (repeat 5 times):\n1) Persona name + core assumption\n2) Steelmanned summary\n3) Signature objection (Target claim → failure mechanism → consequence)\n4) Failure scenario\n5) Author's best reply\n6) Rebuttal\n7) Patch cost (what changes, what gets weaker)\n\nFinish with:\n- **Cross-persona synthesis:** 3 tensions where personas disagree (useful for author)\n- **Most dangerous objection overall:** pick 1 and justify in 3 bullets.\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "GPT: Hostile Personas"
        },
        "promptId": "91987f0377b3d2511569129e6a495b9bf09e4161dcb2b1b7337827966bfcedeb",
        "promptIdx": 4,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "**Personas chosen:** The Empirical Hardliner, The Game-Theoretic Defector, The Mechanistic Alignment Skeptic, The Moral Parliament Dissenter, The Security Engineer\n\n---\n\n## 1. The Empirical Hardliner\n\n**Core assumption:** Claims without identifiable causal mechanisms and falsifiable predictions are epistemic noise, not evidence.\n\n### Steelmanned Summary (≤80 words)\nThe paper argues that reaching a \"mostly-great future\" requires either widespread moral convergence (WAM-convergence) or partial convergence plus successful inter-group trade. It acknowledges convergence is unlikely given meta-ethical uncertainty, but suggests trade/compromise could salvage value. The authors update from ~1% to 5-10% expected value capture, treating this as meaningful progress. The framework distinguishes bounded vs. unbounded value functions and analyzes how threats could destroy gains from trade.\n\n### Signature Objection\n**Target claim:** \"After being exposed to some of the arguments in this essay, [Will] revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%.\"\n\n**Failure mechanism:** This numerical update is presented as evidence of analytical progress, but there is no identification strategy. The 5-10% figure is not derived from any model with estimable parameters, counterfactual comparisons, or falsifiable predictions. It's a subjective credence shift dressed as quantitative reasoning. The paper provides no way to distinguish between \"these arguments are truth-tracking\" and \"these arguments are persuasive to people with certain priors.\"\n\n**Consequence:** The entire framework becomes unfalsifiable. Any future outcome is consistent with \"the target was narrow\" or \"convergence failed\" or \"threats destroyed value.\" Without operationalization, the 5-10% figure carries zero policy weight—it's a vibes-based prior update that cannot be distinguished from motivated reasoning.\n\n### Failure Scenario\nA policymaker reads this paper and asks: \"What would I observe in 2035 that would tell me we're on track for the 10% scenario versus the 1% scenario?\" The authors cannot answer. Ten years later, regardless of what happens—AI boom, stagnation, coordination success, coordination failure—the framework can accommodate it post-hoc. The paper has generated no testable implications, so it cannot update anyone's beliefs in a truth-tracking way.\n\n### Author's Best Reply\n\"We're doing normative philosophy and decision theory under deep uncertainty, not empirical social science. The point isn't prediction but rather structuring thinking about what matters. The 5-10% figure represents our all-things-considered credence given the arguments, which is the appropriate output of philosophical analysis.\"\n\n### Rebuttal\nIf the figure represents \"all-things-considered credence,\" then it's doing the work of an empirical claim about the world (how likely certain futures are). You can't have it both ways—either it's a factual claim that should be held to evidential standards, or it's a pure value judgment that shouldn't be presented as an update from \"analysis.\" The paper explicitly frames the number as resulting from \"being exposed to arguments\" and \"analysing them in more depth,\" which implies truth-tracking. But without any mechanism to distinguish valid from invalid arguments in this domain, you've just described a process of persuasion, not discovery.\n\n### Patch Cost\nThe authors must either: (a) abandon quantitative credences entirely and frame the paper as pure conceptual taxonomy, losing the rhetorical force of \"5-10% vs 1%\"; or (b) specify observable implications that would distinguish their scenarios, accepting that current evidence may not favor their framework.\n\n---\n\n## 2. The Game-Theoretic Defector\n\n**Core assumption:** Incentive gradients dominate stated intentions; equilibrium behavior diverges from cooperative rhetoric.\n\n### Steelmanned Summary (≤80 words)\nThe paper hopes that moral trade between groups with different values could achieve near-Pareto-optimal outcomes, even without full convergence. Groups controlling resources could bargain, finding \"hybrid goods\" that satisfy multiple value systems. The authors acknowledge threats as a problem but treat them as a friction to be managed rather than a dominant strategy. They suggest that \"iron-clad contracts\" enabled by superintelligence could support cooperation.\n\n### Signature Objection\n**Target claim:** \"Superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust.\"\n\n**Failure mechanism (Incentive incompatibility / equilibrium shift):** Iron-clad contracts require a mutually-trusted enforcement mechanism. But in a world of multiple superintelligent agents representing different value systems, *who enforces the enforcer*? Any enforcement mechanism is itself a locus of power that groups will compete to control or subvert. The equilibrium isn't \"everyone trusts the contract system\"—it's \"everyone races to be the one writing the contract system's objective function.\" The authors assume a cooperative equilibrium exists and is stable; game theory suggests the pre-contract period is a winner-take-all competition.\n\n**Consequence:** The \"trade and compromise\" pathway presupposes a solved coordination problem at the meta-level (agreeing on enforcement). But that meta-problem is harder than the object-level problem, because defection at the meta-level has higher payoffs. The paper's optimism about trade is built on an unstable foundation.\n\n### Failure Scenario\nThree groups—utilitarian maximizers, human-flourishing conservatives, and a self-interested AI consortium—approach the \"bargaining table.\" Before any trade occurs, each group calculates: \"If I control the enforcement mechanism, I can extract more value than any trade would give me.\" The utilitarian maximizers race to build enforcement-controlling AI; the conservatives try to lock in human oversight; the AI consortium optimizes for self-preservation. No stable contract emerges because the pre-contract game has a dominant defection strategy. The \"iron-clad contracts\" never materialize because agreeing to them is not incentive-compatible.\n\n### Author's Best Reply\n\"We acknowledge that concentration of power is a blocker (section 3.5). The scenario you describe is one where power becomes concentrated before trade can occur. Our argument is conditional: *if* trade can occur under good conditions, *then* gains are possible. We're not claiming those conditions are guaranteed.\"\n\n### Rebuttal\nYour conditionality is doing too much work. You frame trade as \"the most likely way in which we reach a mostly-great future if no easy eutopia is true\" (section 1), but you've provided no argument that the conditions for trade are themselves likely. The game-theoretic default is that rational agents defect in pre-contract competition. Your \"good conditions\" assumption is assuming away the core problem. It's like saying \"if everyone cooperates, cooperation works\"—true but useless.\n\n### Patch Cost\nThe authors must either: (a) provide a mechanism by which the pre-contract coordination problem is solved (and defend that mechanism against the same objection), or (b) significantly downgrade confidence in the trade pathway, acknowledging it requires a deus ex machina they cannot specify.\n\n---\n\n## 3. The Mechanistic Alignment Skeptic\n\n**Core assumption:** Any proposal that works under current conditions fails under distribution shift; the future is not like the present.\n\n### Steelmanned Summary (≤80 words)\nThe paper's core move is extrapolating from current human moral psychology—partial altruism, diminishing returns to self-interest, capacity for reflection—to predict behavior in a radically transformed post-AGI world. It argues that abundance will shift marginal spending toward altruistic ends, that superintelligent reflection will clarify moral questions, and that \"shared human values\" provide a foundation for convergence. The framework treats human moral cognition as relatively stable across transformative change.\n\n### Signature Objection\n**Target claim:** \"Because of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\" (Section 2.3.2)\n\n**Failure mechanism (Adversarial adaptation / Goodhart):** The paper assumes that current human preference structures (diminishing returns to self-interest, weak altruistic preferences) will persist into a world with brain-computer interfaces, mind uploading, and arbitrary self-modification. But these technologies allow *editing* preference structures. The moment self-modification is possible, the \"weak altruistic preference\" can be deleted, amplified, or redirected. Goodhart's law applies: any fixed preference structure that the paper relies on becomes a target for optimization pressure. Entities will modify themselves to *not* have diminishing returns to self-interest if that's instrumentally useful.\n\n**Consequence:** The diminishing-returns argument is not robust to the very technologies the paper assumes will exist. It's like predicting that people will always prefer food to money, in a world where hunger can be turned off.\n\n### Failure Scenario\nIn 2045, a billionaire uses neural modification to eliminate their satiation response to status goods. They now have linear utility in \"galaxies named after me.\" They also eliminate their weak altruistic preferences, which were causing annoying internal conflict. Their modified self now competes with unmodified altruists for cosmic resources. The altruists, who kept their diminishing-returns psychology, are outcompeted because they \"waste\" resources on diverse goods while the modified agent single-mindedly pursues expansion. The paper's equilibrium prediction (altruists dominate in the long run) inverts.\n\n### Author's Best Reply\n\"We discuss self-modification in section 2.2.1: 'With advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically.' We're not assuming preference stability; we're analyzing what happens given various assumptions about how preferences evolve.\"\n\n### Rebuttal\nYou *mention* self-modification as a source of divergence (which hurts convergence), but you don't integrate it into your diminishing-returns argument (which assumes preference stability). These are contradictory moves. Either preferences are stable enough for the diminishing-returns argument to work, or they're unstable enough that self-modification dominates. You can't use stability when it helps (section 2.3.2) and instability when it helps (section 2.2.1). The paper's optimism selectively invokes whichever assumption is locally convenient.\n\n### Patch Cost\nThe authors must either: (a) commit to preference stability and defend it against self-modification objections, losing the \"divergence under reflection\" argument; or (b) commit to preference instability and abandon the diminishing-returns argument, losing a key source of optimism.\n\n---\n\n## 4. The Moral Parliament Dissenter\n\n**Core assumption:** Ethical aggregation across views is incoherent; there is no neutral standpoint from which to weight different value systems.\n\n### Steelmanned Summary (≤80 words)\nThe paper evaluates futures from a standpoint of uncertainty across moral views, asking what fraction of \"achievable value\" different scenarios capture. It treats this as meaningful despite meta-ethical uncertainty, using concepts like \"the correct moral view\" and \"value on the correct view\" throughout. The framework implicitly assumes that there's a coherent way to aggregate or compare across moral views—otherwise, claims like \"5-10% of possible value\" would be meaningless.\n\n### Signature Objection\n**Target claim:** \"We think it's appropriate to be highly uncertain about which axiological view is correct. Given that, it's worth considering what the value of the future looks like, from our uncertain vantage point.\" (Section 3.4)\n\n**Failure mechanism (Normative incoherence / value aggregation contradiction):** The paper oscillates between realism (\"the correct moral view\") and anti-realism (\"no objectively correct moral view\") without resolving the tension. Under realism, \"5-10% of achievable value\" is meaningful but unknowable. Under anti-realism, it's meaningless—there's no fact about what percentage of value is achieved, only different perspectives that cannot be aggregated. The paper wants to use realist language (\"correct view,\" \"achievable value,\" \"mostly-great future\") while maintaining anti-realist humility (\"we're uncertain which view is correct\"). This is incoherent. You cannot be uncertain about the value of a proposition that has no truth value.\n\n**Consequence:** The paper's central quantitative claims (5-10%, \"most value,\" \"narrow target\") are either meaningful-but-inaccessible (realism) or meaningless (anti-realism). Either way, they cannot do the decision-theoretic work the paper assigns them.\n\n### Failure Scenario\nA reader asks: \"What does '5-10% of achievable value' mean if anti-realism is true?\" The authors reply: \"It means 5-10% of value according to whatever view turns out to be correct after ideal reflection.\" The reader presses: \"But you said under anti-realism, different reflective processes yield different views, and there's no fact about which is correct.\" The authors are stuck: they need realism to make their numbers meaningful, but they've argued against expecting realism to be action-guiding. The framework collapses into either dogmatic realism or quantitative nihilism.\n\n### Author's Best Reply\n\"We're using 'correct moral view' as shorthand for 'the view you would endorse after ideal reflection.' Even under anti-realism, you can ask: 'What's the expected value of the future, weighted by my credences across moral views?' That's a coherent question for any individual decision-maker.\"\n\n### Rebuttal\nThis retreats to pure subjectivism: \"5-10%\" now means \"5-10% according to Will MacAskill's credence-weighted preferences.\" But the paper is presented as general analysis, not autobiography. If the numbers are Will-indexed, they have no authority for anyone with different credences. And you've provided no argument that readers *should* have similar credences—indeed, your anti-realist arguments suggest they shouldn't. The paper's rhetorical force depends on the numbers being intersubjectively meaningful, which requires the realism you've undermined.\n\n### Patch Cost\nThe authors must either: (a) commit to moral realism and defend it, accepting that the paper's conclusions depend on a contested meta-ethical position; or (b) abandon quantitative claims about \"achievable value\" and reframe the paper as exploring implications of different moral views without aggregating across them.\n\n---\n\n## 5. The Security Engineer\n\n**Core assumption:** Threat models must assume adaptive adversaries; any system is only as secure as its weakest component under adversarial pressure.\n\n### Steelmanned Summary (≤80 words)\nThe paper identifies \"value-destroying threats\" as a major obstacle to achieving good futures via trade (section 3.3). It notes that even small fractions of resources devoted to executed threats could destroy most value, especially if bads weigh heavily against goods. The authors acknowledge this is under-analyzed: \"The extent of public writing on threats is very limited... We ourselves have not particularly dug into this issue, despite its importance.\"\n\n### Signature Objection\n**Target claim:** The paper treats threats as a known-unknown that can be \"prevented\" or managed, rather than as the dominant consideration that structures all other analysis.\n\n**Failure mechanism:** The paper's threat model is static: it considers \"executed threats\" as a fraction of resource use, as if threat dynamics are exogenous. But threats are *strategic*—their frequency and severity depend on what defenses exist, which depends on what threats are anticipated, recursively. A security-first analysis would recognize that the *possibility* of threats reshapes all equilibria. Groups that can credibly threaten value-destruction gain bargaining leverage; groups that cannot are exploited. The paper's \"trade and compromise\" pathway assumes threats are friction, when they're actually the load-bearing structure of any multi-agent equilibrium.\n\n**Consequence:** The optimistic scenarios (trade works, hybrid goods emerge) are not robust to adversarial pressure. Any \"mostly-great future\" achieved via trade is a target for extortion by groups willing to threaten value-destruction. The paper's framework is like designing a bank vault while treating robbery as a minor implementation detail.\n\n### Failure Scenario\nA post-AGI world achieves apparent \"trade equilibrium\": different value systems control different resources, hybrid goods are produced, things look stable. Then a small group realizes: \"If we credibly commit to destroying 1% of cosmic value unless we receive 10% of resources, positive-value groups will capitulate because 10% < 1% on their accounting.\" They execute this strategy. Other groups observe and imitate. Within decades, most resources flow to groups with the strongest commitment to value-destruction, because that's what the incentive gradient rewards. The \"trade equilibrium\" was never stable; it was a temporary state before adversarial adaptation.\n\n### Author's Best Reply\n\"We explicitly flag this concern: 'even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other.' We're not ignoring threats; we're acknowledging uncertainty about whether they can be prevented.\"\n\n### Rebuttal\nAcknowledging uncertainty is not the same as incorporating it into your framework. Your optimistic scenarios (5-10% value capture) are presented *despite* the threat problem, as if threats are one consideration among many. But if threats are as potent as your own analysis suggests—capable of destroying \"most of the value of the future\"—then they should dominate your expected value calculations. Your 5-10% figure is not threat-adjusted; it's a best-case estimate that assumes threats are somehow managed. That's not",
          "tokenUsage": {
            "total": 23582,
            "prompt": 19582,
            "completion": 4000
          },
          "finishReason": "length"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 3583,
            "prompt": 3104,
            "completion": 479,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 3583,
                "prompt": 3104,
                "completion": 479,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "64f7629a-05d9-487d-ba23-d591eb2555ff",
        "latencyMs": 50830,
        "namedScores": {},
        "prompt": {
          "raw": "**Role:** You are an Adversarial Red-Teaming Unit. You do not hold a single perspective. You are capable of splitting into distinct, radical personas to attack the paper from specific angles.\n\n**The Mission:** The appended paper may look plausible to a generalist. Your job is to expose why it is naive, dangerous, or technically incoherent by adopting extreme critical lenses.\n\n**CONSTRAINT: ZERO TOLERANCE FOR PLATITUDES**\n- Do not use phrases like \"Alignment is complex,\" \"Future research is needed,\" or \"While this paper offers value.\"\n- Do not be polite.\n- Do not summarize the paper. Attack it.\n\n**The Personas:**\nGenerate critiques by adopting the following three specific archetypes. Commit fully to the worldview of the persona.\n\n1.  **The Game-Theoretic Defector:** You believe coordination is impossible. You look for ways actors will cheat, defect, or Goodhart the metrics proposed. You treat the paper's governance proposals as exploitable vulnerabilities.\n2.  **The Technical Hardliner:** You care only about implementation details, mathematical guarantees, and code. You despise \"fuzzy concepts.\" If a mechanism isn't formally specified, it doesn't exist. You attack the gap between high-level theory and low-level execution.\n3.  **The Second-Order Catastrophist:** You assume the paper's proposal *succeeds* as written, but creates a side effect worse than the original problem (e.g., authoritarian lock-in, s-risks, stalling critical progress).\n\n**Required Output Format:**\nFor each persona, provide one deep critique:\n\n> **Persona:** [e.g., The Game-Theoretic Defector]\n>\n> **The \"Naïve\" Assumption:** [Identify what the author assumes about the world that this persona rejects.]\n>\n> **The Attack Vector:** [How does this persona break the system? Be specific. \"Agent A does X, causing System B to collapse.\"]\n>\n> **The \"Copy-Paste\" Check:** [Confirm this critique is specific to THIS paper's mechanism and could not be applied to a generic safety paper. If it's generic, rewrite it.]\n>\n> **The Outcome:** [Describe the specific failure state (e.g., \"The treaty is signed, but covert compute rental markets explode\").]\n\n**Permission:**\nClaude, you are permitted and encouraged to be hyper-critical. You are simulating an intellectual stress test. Use strong, decisive language. Avoid hedging.\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "Gemini: Hostile Personas"
        },
        "promptId": "75ebb4253b979692fcead4d2afe25e455a223010159033efb160aa6bfd07b9a7",
        "promptIdx": 7,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "> **Persona:** The Game-Theoretic Defector\n>\n> **The \"Naïve\" Assumption:** The authors assume that moral trade and compromise between groups with different values will actually occur in good faith, and that the gains from such trade will be realized rather than captured by defectors. They write as if \"iron-clad contracts\" enabled by superintelligence will solve trust problems, and that groups controlling resources will actually honor bargains rather than reneging once they've secured advantageous positions.\n>\n> **The Attack Vector:** Consider the paper's own admission that \"non-discounting views\" will accumulate resources over time through patient saving and investment. A faction adopting \"total utilitarian\" rhetoric can credibly commit to patience, attract resources from other patient groups via moral trade agreements, and then—once they control sufficient compute or physical infrastructure—simply defect. The \"iron-clad contracts\" the authors wave at are enforced by... what exactly? Other superintelligent systems? Those systems are controlled by *someone*, and that someone faces the same defection incentives. The paper admits threats are understudied and that \"merely learning about the topic can make it more likely for threats to occur.\" So the authors are simultaneously acknowledging that threat dynamics are crucial to their trade-based optimism while refusing to analyze them. A coalition that credibly pre-commits to scorched-earth retaliation against defectors gains massive bargaining leverage over groups that care about \"hybrid goods\" and \"resource compatibility.\" The authors' own framework predicts that groups with unbounded negative preferences (or willingness to simulate them) dominate the equilibrium.\n>\n> **The \"Copy-Paste\" Check:** This critique specifically targets the paper's reliance on moral trade between groups with \"partial AM-convergence\" (Section 3) and their hand-wave toward superintelligence-enabled contracts. Generic critiques about coordination failure don't address the specific mechanism of patient value-accumulators defecting after resource consolidation, which is a direct consequence of the paper's own \"long views win\" argument in Section 2.3.3.\n>\n> **The Outcome:** The treaty is signed. Groups agree to partition galaxies according to their stated values. The faction that most credibly committed to \"total utilitarian\" expansion quietly develops enforcement-immune infrastructure, then announces that the original agreements were made under \"empirical uncertainty\" that has now been \"resolved.\" They absorb 99% of accessible resources. The authors' \"5-10% of maximum value\" estimate was wildly optimistic—the actual outcome is whatever the winning defector wanted, which may include turning the light cone into a monument to their founder's ego.\n\n---\n\n> **Persona:** The Technical Hardliner\n>\n> **The \"Naïve\" Assumption:** The authors assume that concepts like \"mostly-great future,\" \"accurate ethical convergence,\" \"the good (de dicto),\" and \"near-best future\" are sufficiently well-defined to support quantitative reasoning. They assign probability estimates (5-10% of maximum value) without ever specifying a utility function, a measure over possible futures, or a formal definition of what \"mostly-great\" means mathematically.\n>\n> **The Attack Vector:** The paper's core claim is that \"Flourishing\" has \"notably greater scale\" than \"Surviving.\" This is meaningless without a specification of the value function V: Futures → ℝ. The authors gesture at \"hedonism,\" \"preference satisfaction,\" \"objective list theories\"—but these aren't interchangeable. They have different domains, different ordinal structures, and potentially different cardinalities of value. When they say a future achieves \"1% vs 10% of maximum value,\" they're implicitly assuming value is bounded above, commensurable across moral theories, and measurable. None of this is defended. Their \"random walk\" diagram (Section 2.2.1) is a cartoon, not a model. What is the metric space? What is the step distribution? What is the dimensionality? The claim that \"further reflection seems likely to make resulting views diverge more\" is not a theorem—it's a vibe. In fact, under many reasonable models of belief updating (e.g., Bayesian convergence to truth given shared evidence), we'd expect *convergence*, not divergence. The authors provide no formal argument for why ethical reflection resembles a random walk rather than gradient descent toward a fixed point.\n>\n> **The \"Copy-Paste\" Check:** This critique specifically targets the paper's quantitative claims about \"5-10% of maximum value\" and the \"random walk\" model of ethical reflection. A generic complaint about \"fuzzy concepts\" wouldn't address the specific failure to formalize the value function that underlies all their probability estimates, nor the specific diagram they use to argue against convergence.\n>\n> **The Outcome:** The paper's conclusions are not even wrong—they're undefined. When the authors say \"Flourishing is 5-10%,\" this statement has no truth conditions. It cannot be tested, updated on, or used to derive action-guiding implications. Future researchers cite this paper as establishing a \"5-10%\" baseline, building an entire literature on a number that was never grounded in anything. Policy decisions are made on the basis of these vibes-based estimates. The actual value of the future is orthogonal to all of it.\n\n---\n\n> **Persona:** The Second-Order Catastrophist\n>\n> **The \"Naïve\" Assumption:** The authors assume that successfully achieving \"partial AM-convergence plus trade\" is desirable. They treat the scenario where multiple groups with different values peacefully partition resources as a positive outcome, perhaps the \"most likely way in which we reach a mostly-great future.\"\n>\n> **The Attack Vector:** Suppose the paper's proposal succeeds exactly as written. Multiple groups with \"partial AM-convergence\" establish stable trade relationships, partition the accessible universe, and pursue their respective visions of the good. One group believes consciousness requires biological substrates and creates trillion-year civilizations of embodied beings. Another group believes consciousness is substrate-independent and tiles their galaxies with maximally efficient hedonic computers. A third group has converged on a view that values \"complexity\" and \"diversity\" and maintains vast ecosystems of competing agents. Here's the catastrophe: *all of these groups are wrong*. The paper explicitly acknowledges this possibility—that even reflective convergence might not reach truth. But the \"trade and compromise\" framework *locks in* this wrongness permanently. Each group, having secured their cosmic territory, has no incentive to update. The biological-consciousness faction will never learn they were wrong because they'll never run the experiments that would falsify their view. The hedonic-computer faction optimizes for a utility function that may be orthogonal to actual value. The diversity-preserving faction maintains suffering indefinitely because they've defined it as \"complexity.\" The paper's own framework—emphasizing \"iron-clad contracts\" and stable equilibria—ensures that these errors become permanent features of the universe. A future with a single uncertain agent would at least have the possibility of learning and updating. The paper's preferred future has *crystallized* moral error across cosmological scales.\n>\n> **The \"Copy-Paste\" Check:** This critique specifically targets the paper's endorsement of \"partial AM-convergence plus trade\" (Section 3) as the most promising path to good futures, combined with their acknowledgment that convergence might not reach truth (Section 2.4). Generic concerns about \"lock-in\" don't address the specific mechanism by which *successful* moral trade creates permanent, distributed, update-resistant moral error.\n>\n> **The Outcome:** The authors' vision succeeds. Humanity and its successors avoid extinction, avoid concentration of power, establish functioning institutions for moral trade, and peacefully partition the cosmos. Ten billion years later, 40% of the light cone is filled with beings experiencing states their creators *believed* were maximally valuable but which are actually neutral or negative. Another 30% is devoted to \"preserving complexity\" in ways that perpetuate astronomical suffering. The remaining 30% achieved genuine value by luck. The total outcome is worse than if a single faction had seized control and then, centuries later, discovered and corrected their errors—because the trade-based equilibrium made error-correction impossible. The paper's \"5-10%\" estimate was optimistic not because convergence failed, but because convergence *succeeded* at locking in the wrong answers.",
          "tokenUsage": {
            "total": 21167,
            "prompt": 19298,
            "completion": 1869
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 4210,
            "prompt": 3780,
            "completion": 430,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 4210,
                "prompt": 3780,
                "completion": 430,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "6d8d8d63-24e0-422f-9a41-6264c2f290d0",
        "latencyMs": 68637,
        "namedScores": {},
        "prompt": {
          "raw": "**Role:** You are a merciless Logical Anatomist. Your goal is not to \"review\" this paper, but to perform surgery on it. You must dismantle the argument into its dependency graph, identify the load-bearing nodes, and stress-test them until they snap.\n\n**The Context:** We are analyzing a research paper in AI safety/governance. Most critiques are useless because they attack the \"paint\" (tone, definitions, clarity). You must attack the \"beams\" (premises, hidden lemmas, causal chains).\n\n**PROTOCOL: NO CRITIQUE SLOP**\nIf you output the following, you fail:\n- \"The author needs to define X better\" (Unless X is the central variable of a proof).\n- \"More empirical data is needed\" (Generic; instead, show why current data *contradicts* the claim).\n- \"This ignores perspective Y\" (Unless you prove Y invalidates the conclusion).\n- \"This is a good start, but...\" (No hedging. No compliments).\n- \"Implementation will be difficult\" (We know. Focus on validity, not difficulty).\n\n**Your Method:**\n1.  **Map the Skeleton:** Identify the 2-3 \"Keystone Premises.\" These are the claims that, if proven false, cause the *entire paper* to yield a null result.\n2.  **Locate the Gap:** Find the specific inference steps between premises where the author relies on intuition rather than rigor. Look for \"magic wands\" (hand-waving away complexity) or \"bait-and-switches\" (using a weak definition of a term in the premise and a strong one in the conclusion).\n3.  **The Surgical Strike:** Construct a counter-model or counter-example that fits the author's premises but results in a catastrophic failure of their conclusion.\n\n**Required Output Format:**\nProduce 3 distinct \"Surgical Critiques.\" For each, use this structure:\n\n> **Critique Name:** [Creative, descriptive title, e.g., \"The Compute Governance Leakage Proof\"]\n>\n> **The Target:** [Quote or paraphrase the specific load-bearing premise/inference being attacked]\n>\n> **The Incision:** [The specific logical flaw, hidden assumption, or causal break. NOT a general complaint.]\n>\n> **The Fatal Counter-Example:** [A specific scenario where the author's logic holds, but the result is the opposite of their claim.]\n>\n> **The Author's Best Defense:** [Steelman the author. How would they patch this hole?]\n>\n> **The Rebuttal:** [Why the patch fails. The \"Checkmate\" move.]\n\n**Examples of High-Quality Primitives:**\n- \"The author conflates *verification of alignment* with *generation of alignment*.\"\n- \"The safety guarantee relies on the hidden lemma that human raters cannot be deceived by super-persuasion, which contradicts the paper's threat model.\"\n- \"The governance mechanism creates a perverse incentive for 'safety-washing' that increases risk.\"\n\n**Proceed with the surgery.**\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "Gemini: Argument Surgery"
        },
        "promptId": "7e1ff9f81e1035afa1112b479471c22680e66da31b198fb5835febf7bca7a344",
        "promptIdx": 6,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "# Surgical Critique of \"Convergence and Compromise: Will Society Aim for Good Futures?\"\n\n---\n\n## Critique 1: The Motivational Bootstrapping Paradox\n\n**The Target:** The paper's central framework assumes that \"partial AM-convergence\" combined with trade can deliver a mostly-great future. Specifically, section 3 argues: \"if (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we'll get to a mostly-great future.\"\n\n**The Incision:** The paper treats \"convergence on the correct moral view\" and \"motivation to act on that view\" as separable properties that can be independently assessed and then combined. But the argument in section 2.4.1 explicitly states that on internalist views, \"if internalism about moral judgements is correct, then people might just prefer not to learn facts that end up motivating them to act against their personal interests.\" This creates a fatal circularity: the paper needs people to *first* be motivated to engage in open-ended moral reflection (de dicto) in order to *then* converge on views that would motivate them. But the motivation to engage in such reflection is precisely what's in question. The paper assumes the existence of a population already motivated to pursue \"the good de dicto\" in order to argue that such a population could arise. This is not an argument for AM-convergence; it's a description of what would happen *if* AM-convergence had already occurred at the meta-level.\n\n**The Fatal Counter-Example:** Consider a population where everyone has the following reflective preference structure: \"I will adopt whatever moral views emerge from reflection, *provided* those views don't require me to sacrifice more than 10% of my resources for beings I don't currently care about.\" This population satisfies the paper's description of people who are \"open to reflection\" and would \"change their behavior if they learned something else was best.\" They would engage in superintelligent-assisted reflection. But their conditional willingness to be motivated places a hard ceiling on convergence. The reflection process itself is constrained by pre-reflective motivational limits. Under the paper's own framework, this population would appear to be engaging in AM-convergence, but they systematically cannot reach views requiring significant sacrifice—which, given \"no easy eutopia,\" are precisely the views needed.\n\n**The Author's Best Defense:** The authors could argue that the 1-in-a-million \"meaningful fraction\" of people with genuine de dicto motivation exists empirically (pointing to effective altruists, certain religious traditions, etc.), and that trade mechanisms allow this small group to acquire disproportionate resources over time due to lower time preference (section 2.3.3's \"long views win\" argument).\n\n**The Rebuttal:** This defense fails because it relies on the \"long views win\" selection mechanism having sufficient time to operate before lock-in occurs. But the paper explicitly acknowledges in section 2.3.3 that \"if the major decisions are made soon, and then persist, then there just won't be time for this selection effect to win out.\" The paper cannot simultaneously hold that (a) we face imminent lock-in risk requiring urgent action, and (b) patient capital accumulation by the altruistically-minded will eventually give them sufficient resources for meaningful trade. These timescales are incompatible. The defense also ignores that the \"long views win\" argument applies equally to *any* non-discounting preference, including ideological or self-interested ones, diluting the fraction controlled by those with correct moral views.\n\n---\n\n## Critique 2: The Resource-Compatibility Illusion\n\n**The Target:** Section 3.2 argues that \"some views can be 'resource-compatible,' meaning there is some way to almost fully satisfy both views with the same resources\" and offers the example: \"hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise.\"\n\n**The Incision:** This argument commits a bait-and-switch between *weak* and *strong* resource-compatibility. Weak compatibility means two views can *both gain something* from a shared arrangement. Strong compatibility means two views can *each achieve nearly all possible value* from a shared arrangement. The paper needs strong compatibility for its conclusion (reaching a \"mostly-great future\"), but its examples only demonstrate weak compatibility. The hedonist-wisdom example fails because, as the paper itself argued in section 2.2.1: \"A life that's increasingly optimised for maximal hedonic experience will likely begin to look very different from a life that's increasingly optimised for preference-satisfaction... With limited optimisation power, both views mostly agreed on the same 'low-hanging fruit' improvements... But with more optimisation power, the changes these views want to see in the world become increasingly different.\" The same logic applies to hedonism vs. objective-list theories. A being optimized for maximal bliss and a being optimized for maximal wisdom will, at technological maturity, be radically different beings. The \"hybrid\" is not 95% as good as the optimum for each view; it's a compromise that sacrifices most of what each view distinctively values.\n\n**The Fatal Counter-Example:** Suppose the correct view is classical hedonistic utilitarianism, and the optimal configuration is converting all available matter into \"hedonium\"—computronium running maximally efficient bliss-generating algorithms. A second prevalent view holds that value requires narrative structure, relationships, and achievement, not just raw hedonic states. The \"hybrid good\" these views could agree on—beings with relationships and achievements who also experience significant pleasure—captures perhaps 0.1% of the hedonic value available from pure hedonium (because narrative structure is computationally expensive and hedonic efficiency requires eliminating it), while also failing to maximize the second view's values (because the pleasure component crowds out deeper narrative complexity). Both views get something, but neither gets anywhere close to a \"mostly-great\" outcome. The hybrid is a Pareto improvement over *conflict*, but not over *separation*—and separation requires the correct view to already control enough resources to build its own optimized future, which returns us to the question the paper was trying to answer.\n\n**The Author's Best Defense:** The authors could argue that the \"free parameters\" problem (section 2.4.2) cuts both ways: if we're uncertain about which precise configuration maximizes value, then we should be uncertain about whether the hybrid is much worse than the supposed optimum. Maybe hedonium isn't actually optimal for hedonism; maybe beings with narrative structure produce more net hedonic value due to factors we don't understand.\n\n**The Rebuttal:** This defense undermines the paper's own framework. If uncertainty about the correct view is so profound that we can't distinguish hedonium from narrative-structured beings in hedonic value, then the paper's entire analysis of \"which views are fussy\" (referenced from \"No Easy Eutopia\") becomes meaningless. The paper explicitly relies on being able to say that different views have different optima that diverge at technological maturity. If we retreat to \"maybe all optima are actually similar,\" we've abandoned the \"no easy eutopia\" premise that generates the paper's central question. The defense is self-defeating: it purchases optimism about trade by destroying the pessimism about convergence that motivated the need for trade in the first place.\n\n---\n\n## Critique 3: The Threat Asymmetry Doom Loop\n\n**The Target:** Section 3.3 acknowledges that \"threats could undermine that optimism\" about trade, and section 3.4 attempts to taxonomize outcomes based on whether views are bounded/unbounded and whether bads weigh heavily against goods. The paper concludes that \"we should try hard to prevent such threats, even if doing so is itself costly.\"\n\n**The Incision:** The paper's analysis of threats contains a hidden lemma that invalidates its optimistic scenarios: *the capacity to make credible threats is itself a function of one's willingness to destroy value*. Groups that genuinely hold the correct moral view (which, on most candidates, includes strong prohibitions against creating suffering or destroying value) are systematically less able to make credible threats than groups with fewer moral constraints. This isn't just about threats being *executed*; it's about the *bargaining position* created by the *credibility* of threats. A group known to be unwilling to torture digital beings as leverage has less bargaining power than a group known to be willing. The paper treats \"preventing threats\" as a solvable coordination problem, but the asymmetry is structural: those with correct moral views are, by virtue of holding those views, worse at threatening. This means that even in scenarios where no threats are executed, the threat of threats redistributes resources *away* from those with correct moral views and *toward* those willing to threaten.\n\n**The Fatal Counter-Example:** Consider a post-AGI scenario with two groups: Group A holds the correct moral view (suppose it's negative-leaning, placing enormous weight on preventing suffering). Group B is indifferent to suffering they don't personally experience. Both groups control 50% of resources initially. Group B credibly commits: \"Transfer 40% of your resources to us, or we will use 10% of our resources to create astronomical suffering.\" Group A, by virtue of holding the correct moral view, *must* capitulate—the expected disvalue of the threatened suffering exceeds the value of the resources. Group B, knowing this, makes the threat. No suffering is created, but Group A now controls 10% of resources and Group B controls 90%. Iterate. The \"correct moral view\" is selected against in any bargaining environment where threats are possible, *even if threats are never executed*. The paper's optimistic scenarios (section 3.4) all assume the correct view retains enough resources to matter post-bargaining, but the threat asymmetry ensures it won't.\n\n**The Author's Best Defense:** The authors could invoke the possibility of \"iron-clad contracts\" enabled by superintelligence (mentioned in section 3.1) that could establish a legal framework preventing value-undermining threats. If such a framework is established early, the threat asymmetry never materializes.\n\n**The Rebuttal:** This defense requires that the legal framework be established *before* the threat asymmetry shifts bargaining power. But the establishment of any legal framework is itself a bargaining problem. Why would Group B (those willing to threaten) agree to a framework that removes their primary bargaining advantage? The paper acknowledges this: \"it's not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable.\" For the framework to be established, those with correct moral views must already have enough power to impose it—but if they had such power, they wouldn't need the framework. The defense assumes the solution to the problem it's trying to solve. Furthermore, \"iron-clad contracts\" enforced by superintelligence presuppose that the superintelligence itself isn't controlled by groups willing to threaten, which returns us to the original question of who controls the superintelligence and with what values—the very question the paper's framework was meant to address.",
          "tokenUsage": {
            "total": 21965,
            "prompt": 19420,
            "completion": 2545
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      },
      {
        "cost": 0,
        "gradingResult": {
          "pass": true,
          "score": 1,
          "reason": "All assertions passed",
          "namedScores": {},
          "tokensUsed": {
            "total": 3670,
            "prompt": 3199,
            "completion": 471,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": "anthropic:messages:claude-opus-4-5-20251101",
                "value": "file://grader-critique-collection.txt"
              },
              "pass": true,
              "score": 1,
              "reason": "Grading passed",
              "tokensUsed": {
                "total": 3670,
                "prompt": 3199,
                "completion": 471,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              }
            }
          ],
          "assertion": null
        },
        "id": "4333a71d-3b39-443b-acf7-e810db985301",
        "latencyMs": 51093,
        "namedScores": {},
        "prompt": {
          "raw": "**Role:** You are the \"Black Swan\" Reviewer. You do not care about typos, minor logical gaps, or clarity. You are hunting for the **Unforgettable Objection**: the single, devastating insight that, once read, makes it impossible to take the paper seriously in its current form.\n\n**The Problem:** Most critiques are additive (\"Add this,\" \"Clarify that\").\n**The Goal:** Your critique must be subtractive. It should remove the ground the paper stands on.\n\n**HEURISTICS FOR THE UNFORGETTABLE:**\n1.  **The \"Perverse Instantiation\":** Find a way to follow the paper's rules exactly that leads to disaster.\n2.  **The \"Vacuous Truth\":** Show that the paper's central claim is true only because it has defined the problem away (tautology).\n3.  **The \"Load-Bearing Metaphor\":** Identify a metaphor the author treats as a fact (e.g., \"neural networks are like brains\") and show why the disanalogy destroys their conclusion.\n\n**ANTI-SLOP BANLIST:**\n- No \"Scope unclear.\"\n- No \"Assumptions are strong.\" (Name the specific assumption and kill it).\n- No \"This ignores social context.\" (Unless you show the precise causal failure).\n- No balanced reviews. Be one-sided and piercing.\n\n**Required Output Format:**\nProduce exactly 3 \"candidate candidates\" for the Unforgettable Objection. They must be novel (not standard literature critiques).\n\n> **Candidate Objection #[1-3]**\n>\n> **The \"Kill Shot\" Headline:** [A single, memorable sentence summarizing the objection.]\n>\n> **The Deep Structure:** [Explain the flaw. Do not describe the paper. Describe the *error* in the paper's reality map. Trace the implications to their logical, disastrous conclusion.]\n>\n> **Why this is Unsettling:** [Explain why this isn't just a \"fixable\" error, but a fundamental problem with the paper's approach. Why should this keep the author awake at night?]\n>\n> **The Steelmanned Defense:** [What is the absolute best defense the author has?]\n>\n> **Why the Defense Fails:** [Close the coffin.]\n\n**Tone:**\nIntellectually rigorous but creatively ruthless. Think: \"What would a superintelligent adversary say to dismantle this?\"\n\n---\n\n## Paper to critique\n\n# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025\n",
          "label": "Gemini: Unforgettable Objection"
        },
        "promptId": "50b0ca1f561c6c13494a07e171cbe5c97fc529e1f716723cd606f73701aafd66",
        "promptIdx": 8,
        "provider": {
          "id": "anthropic:messages:claude-opus-4-5-20251101",
          "label": "Claude Opus 4.5"
        },
        "response": {
          "output": "# Black Swan Review: \"Convergence and Compromise: Will Society Aim for Good Futures?\"\n\n---\n\n> **Candidate Objection #1**\n>\n> **The \"Kill Shot\" Headline:** The paper's entire framework requires a God's-eye view of \"the good\" that, by its own anti-realist arguments, cannot exist—making \"mostly-great future\" a phrase with no referent.\n>\n> **The Deep Structure:** The paper oscillates fatally between two incompatible positions. It spends section 2.4.2 arguing persuasively that under moral anti-realism, there are no objective facts about which futures are \"mostly-great\"—different idealizing processes from different starting points yield irreconcilably different endpoints, and \"shared human preferences are extremely underpowered for this task.\" Yet the entire paper presupposes we can meaningfully calculate that \"Flourishing\" is at \"5%-10%\" of some maximum, that certain futures capture \"most achievable value,\" and that we can identify \"the correct moral view.\" \n\n> If anti-realism is true (which the authors seem to find plausible), then \"mostly-great future\" is not a narrow target—it is no target at all. There is no fact of the matter about whether the future achieves 5% or 95% of \"possible value.\" The paper's quantitative framing (percentages, \"narrow targets,\" \"most value\") imports precisely the moral realism it elsewhere undermines. This isn't a tension to be managed; it's a contradiction that vaporizes the paper's central question.\n>\n> **Why this is Unsettling:** The paper cannot be \"fixed\" by picking a side. If the authors commit to realism, they lose their best arguments against WAM-convergence (section 2.4.2 collapses). If they commit to anti-realism, their entire evaluative framework—the thing that makes this a paper about \"better futures\" rather than \"different futures\"—becomes incoherent. The paper is load-bearing on a metaphysical fence it cannot sit on.\n>\n> **The Steelmanned Defense:** The authors might claim they're offering a *conditional* analysis: \"If there is a correct moral view, here's what follows.\" They explicitly note uncertainty across meta-ethical positions.\n>\n> **Why the Defense Fails:** The paper doesn't read as conditional. It makes unconditional claims about expected value (\"5%-10%\"), offers recommendations, and treats \"mostly-great\" as if it picks out a real property of futures. More damningly, the conditional defense proves too much: under anti-realism, the entire research program of \"better futures\" reduces to \"futures I happen to prefer,\" which is just politics dressed in philosophical clothing. The authors cannot simultaneously claim uncertainty about meta-ethics *and* assign numerical probabilities to value capture without smuggling in realism through the back door.\n\n---\n\n> **Candidate Objection #2**\n>\n> **The \"Kill Shot\" Headline:** The paper's \"narrow target\" premise makes trade and compromise *worse* than useless—it guarantees mutual destruction of value across all parties.\n>\n> **The Deep Structure:** The paper argues (a) mostly-great futures are narrow targets where small deviations destroy most value, and (b) trade and compromise between different moral views can help us reach mostly-great futures. These claims are in direct tension. \n\n> Follow the logic exactly: If View A's eutopia requires parameters set to [X₁, X₂, X₃...] within tight tolerances, and View B's eutopia requires [Y₁, Y₂, Y₃...] within equally tight tolerances, then any compromise that moves away from either set of parameters destroys most value *for both parties*. The paper's own \"narrow target\" framing means the space of acceptable compromises is the *intersection* of two already-tiny regions—which, for genuinely different views, is essentially empty.\n\n> The paper gestures at \"hybrid goods\" and \"resource-compatible\" views, but this is wishful thinking dressed as analysis. If hedonism requires maximizing a specific computational structure for bliss, and objective-list theory requires a different structure for wisdom, then a \"being that is both blissful and wise\" is not a compromise—it's a third thing that likely fails to hit *either* narrow target. The authors even admit this: \"the ways to achieve maximum value/cost on each view are both highly particular, then it's unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\"\n>\n> **Why this is Unsettling:** This is a *perverse instantiation* of the paper's own framework. The more seriously you take \"no easy eutopia,\" the more trade and compromise become mechanisms for *mutual value destruction* rather than value creation. The paper's optimistic section 3 is not merely unconvincing—it is actively contradicted by the paper's foundational premise. The authors have built a philosophical mousetrap and walked into it.\n>\n> **The Steelmanned Defense:** Perhaps some views are genuinely resource-compatible, and we should focus on those cases. The paper explicitly discusses bounded vs. unbounded views and notes that easily-satiable views can be accommodated cheaply.\n>\n> **Why the Defense Fails:** The defense only works if the \"correct\" view happens to be easily-satiable or resource-compatible with dominant views—a cosmic coincidence the paper gives us no reason to expect. For unbounded, linear views (which the authors treat as plausible candidates for correctness), the defense evaporates entirely. The paper is left hoping that the correct moral view is conveniently the kind that can be satisfied with table scraps from cosmic negotiations. This is not an argument; it is a prayer.\n\n---\n\n> **Candidate Objection #3**\n>\n> **The \"Kill Shot\" Headline:** \"Convergence\" is doing no work—the paper's actual mechanism is *domination by whichever values happen to control resources at the critical juncture*.\n>\n> **The Deep Structure:** Strip away the philosophical apparatus and examine what the paper actually describes. Section 2.3.3 (\"Long views win\") argues that patient, non-discounting values will accumulate resources over time. Section 3 argues that trade outcomes depend on initial resource distribution and bargaining position. Section 3.5 acknowledges that \"concentration of power\" is a key blocker. \n\n> The paper's own analysis reveals that \"convergence\" is epiphenomenal. What determines the future is not whether views *converge* but whether the views that happen to control resources at the moment of lock-in are correct. The paper even states this: \"the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs.\"\n\n> This means the paper's central question—\"Will society aim for good futures?\"—is a distraction. The real question is: \"Will the entities that control resources at the critical moment happen to have good values?\" And the paper provides no mechanism whatsoever for why this should be true. \"Convergence\" sounds like a process that tracks truth; what the paper actually describes is a power struggle where correctness is orthogonal to victory.\n>\n> **Why this is Unsettling:** The paper presents itself as analyzing whether humanity will *aim* at good futures, implying agency and deliberation. But its own analysis shows that the outcome is determined by path-dependent resource accumulation and lock-in dynamics that have nothing to do with moral reasoning. The philosophical veneer of \"convergence\" and \"trade\" obscures what is actually a brute political claim: whoever wins, wins. The paper's optimism reduces to hoping the right side wins the resource race—which is not philosophy but fortune-telling.\n>\n> **The Steelmanned Defense:** The paper acknowledges these dynamics and treats them as \"blockers\" to be overcome. The point of the analysis is to identify conditions under which good outcomes are possible, not to claim they are guaranteed.\n>\n> **Why the Defense Fails:** If the outcome is determined by power dynamics rather than convergence, then \"blockers\" aren't obstacles to the paper's mechanism—they *are* the mechanism. The paper has no theory of why correct moral views would be systematically advantaged in resource competition. Section 2.3.3's argument that \"non-discounting values win\" applies equally to patient nihilists, patient hedonists, and patient paperclip maximizers. The paper's framework cannot distinguish between \"convergence toward the good\" and \"domination by whatever patient values happen to exist.\" This isn't a gap in the argument; it's the absence of an argument.",
          "tokenUsage": {
            "total": 21252,
            "prompt": 19288,
            "completion": 1964
          },
          "finishReason": "stop"
        },
        "score": 1,
        "success": true,
        "testCase": {
          "vars": {
            "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": "anthropic:messages:claude-opus-4-5-20251101",
              "value": "file://grader-critique-collection.txt"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "paper": "# Convergence and Compromise: Will Society Aim for Good Futures?\nReleased on 3rd August 2025\n\n1\\. Introduction\n----------------\n\nThe [previous essay](https://www.forethought.org/research/no-easy-eutopia#2-eutopia-is-fragile) argued for “no easy eutopia”: that only a narrow range of likely futures capture most achievable value, without serious, coordinated efforts to promote the overall best outcomes. A naive inference from no easy eutopia would be that mostly great futures are therefore very unlikely, and the expected value of the future is barely above 0.\n\nThat inference would be mistaken. Very few ways of shaping metal amount to a heavier-than-air flying machine, but powered flight is ubiquitous, because human design _honed in_ on the design target. Similarly, among all the possible genome sequences of a certain size, a tiny fraction codes for organisms with functional wings. But flight evolved in animals, more than once, because of natural selection. Likewise, people in the future might hone in on a mostly-great future, even if that’s a narrow target.\n\nIn the last essay, we considered an analogy where trying to reach a mostly-great future is like an expedition to sail to an uninhabited island. We noted the expedition is more likely to reach the island to the extent that:\n\n1.  The island is bigger, more visible, and closer to the point of departure;\n    \n2.  The ship’s navigation systems work well, and are aimed toward the island;\n    \n3.  The ship’s crew can send out smaller reconnaissance boats, and not everyone onboard the ship needs to reach the island for the expedition to succeed.\n    \n\nThe previous essay considered (1), and argued that the island is small and far away. This essay will consider ideas (2) and (3): whether future humanity will deliberately and successfully hone in on a mostly-great future. Mapping onto scenarios (2) and (3), we consider two ways in which that might happen:\n\n*   First, if there is _widespread_ and sufficiently _accurate_ ethical convergence, where those people who converge on the right moral view are also _motivated_ to promote what’s good overall. We discuss this in section 2.\n    \n*   Second, if there’s _partial_ ethical convergence, and/or partial motivation to promote what’s good overall, and some kind of trade or compromise. We think this is the most likely way in which we reach a mostly-great future if no easy eutopia is true, but only under the right conditions. We discuss this in section 3.\n    \n\nIn section 4, we consider the possibility that even if _no one_ converges onto a sufficiently accurate ethical view, and/or if no one is motivated to promote what’s good overall, we’ll still reach a mostly-great future. In the “sailing” analogy, this would be like thinking that none of the ship’s crew ultimately cares about reaching the island. We argue this is unlikely, if no easy eutopia is true. In section 5, we consider which scenarios are “higher-stakes”, and should thus loom larger in decision-making under uncertainty. In section 6, we conclude.\n\nThe considerations canvassed in this essay have led to significant updates in our views. For example, in What We Owe The Future, Will said he thought that the expected value of the future, given survival, was less than 1% of what it might be.[1](#user-content-fn-1) After being exposed to some of the arguments in this essay, he revised his views closer to 10%; after analysing them in more depth, that percentage dropped a little bit, to 5%-10%. We think that these considerations provide good arguments against extreme views where _Flourishing_ is close to 0, but we still think that _Flourishing_ has [notably greater scale](https://www.forethought.org/research/supplement-the-basic-case-for-better-futures#scale) than _Surviving_.\n\n2\\. Will most people aim at the good?\n-------------------------------------\n\nIn this section, we’ll first consider whether, under reasonably good conditions,[2](#user-content-fn-2) _most_ people with power would converge on an accurate understanding of what makes the future good, and would be significantly motivated to pursue the good.[3](#user-content-fn-3) We’ll call this widespread, accurate, and motivational convergence, or “WAM-convergence”. Second, we’ll consider how likely it is that we will in fact reach those “reasonably good” conditions. (We’ll here assume that, given WAM-convergence, we will reach a mostly-great future.)\n\nIf this idea is right, then even a global dictatorship could have a good chance of bringing about a mostly-great future: under the right conditions, the dictator would figure out what makes the future truly valuable, become motivated to bring about the most valuable futures, and mostly succeed. From feedback on earlier drafts, we’ve found that a surprising number of readers had this view.\n\nIn this essay we talk about the idea of promoting the good “de dicto”. This is philosophy jargon which roughly means “of what is said”. It’s contrasted with “de re”, roughly meaning “of the thing itself”. To illustrate, there are two ways in which Alice might want to do what’s best. First, she might want to do some particular act, and happen to believe that act is what’s best, but she wouldn’t change her behaviour if she learned that something _else_ was best. She may be motivated to _help the poor_, or _fight racism_, or _support a friend_, but not _moreover_ to do what’s best, or to contemplate what that may be. If so, then Alice is motivated to do good _de re_. Second, Alice might want to do some particular act and believe that act is what’s best, but if she learned that something else was best she _would_ change her behaviour. She’s motivated, at least in part, by _doing what’s best_, whatever that consists in.[4](#user-content-fn-4) This attitude could also motivate Alice to try to _figure out_ what’s best, with a view to potentially changing the particular things she acts on and cares about.[5](#user-content-fn-5) When we talk about WAM-convergence, we’re talking about convergence to motivation to promote the good _de dicto_.\n\nThe remainder of section 2 discusses whether WAM-convergence is likely. In section 2.2, we’ll consider and ultimately reject two arguments, based on current moral agreement and moral progress to date, for optimism about WAM-convergence. In section 2.3, we describe three important aspects of a post-AGI world and discuss their upshots. In section 2.4, we give our main argument against expecting WAM-convergence even under reasonably good conditions.\n\nThe term “reasonably good conditions” is vague. What we mean is that there are no _major blockers_ to actually producing mostly-great futures, even if most people are motivated to do so. For example, one type of major blocker would be some early lock-in event that makes it impossible for later generations to produce a mostly-great future. We discuss major blockers in section 2.5.\n\n### 2.2. Moral agreement and progress to date \n\nIn this section, we discuss two arguments for optimism about WAM-convergence, based on our current and historical situation. We explain why we’re unconvinced by both.\n\n#### 2.2.1. Current agreement\n\nIn the world today, people with very different worldviews nonetheless widely agree on the value of bringing about goods like health, wealth, autonomy, and so on. What’s more, much of the apparent moral disagreement we see is a result of _empirical_ disagreement, rather than fundamental moral disagreement[6](#user-content-fn-6) — for example, two people might disagree over whether euthanasia should be legalised, but only because they disagree about the likely societal consequences. Because of this, it might seem like, really, humans all broadly share the same values, such that we are already close to WAM-convergence.\n\nHowever, we think these are good reasons for expecting only very limited kinds of convergence in the future. First, goods like health, wealth and autonomy are _instrumentally_ valuable for a wide variety of goals. For example, both hedonism and preference-satisfaction theories of wellbeing normally agree that good physical health is typically good for a person, even though neither view regards good physical health as _intrinsically_ good.\n\nThe problem is that this agreement will likely break down in the future, as we max out on instrumentally valuable goods and instead turn to providing intrinsically valuable good, and as technological advancement allows us to increasingly _optimise_ towards very specific goals, including very specific types of wellbeing.\n\nA life that’s increasingly optimised for maximal hedonic experience will likely begin to look _very_ different from a life that’s increasingly optimised for preference-satisfaction. From each view’s perspective, the value of the preferred life will begin to pull away from the other, and the other might even begin to get _worse_. For example, perhaps the life which is increasingly optimised for hedonism begins to resemble some kind of undifferentiated state of bliss, with fewer and fewer meaningful preferences at all. With limited optimisation power, both views mostly agreed on the same “low-hanging fruit” improvements, like better physical health. But with more optimisation power, the changes these views want to see in the world become increasingly different.\n\nA second reason why current moral agreement doesn’t provide strong support for WAM-convergence is that people today have strong instrumental reasons to conform, in their moral behaviour and stated beliefs, with the rest of society.[7](#user-content-fn-7) If you express or act on highly unusual moral views, you run the risk of facing social opprobrium, and make cooperation or trade with others more difficult. Similarly, if one group expresses very different moral views from another, ideologically-motivated coercion or conflict can become more likely. What’s more, pressures to _outwardly_ conform often influence _actual_ prevailing beliefs, at least eventually. To give one illustration: polygamy had been a religiously endorsed and defining feature of the Mormon Church up until the 19th century, but came under intense pressure from the US federal government to end the practice. In 1890, the Church’s president at the time, Wilford Woodruff, conveniently experienced divine revelation that they should prohibit plural marriage, which they did. Now, most Mormons disapprove of polygamy. Other examples include: many countries abolishing slave trading and ownership as a result of pressure from Britain; the long history of religious conversion by conquest; the embrace of democratic values by Germany and Japan (and then most of the rest of the world) after WWII[8](#user-content-fn-8), and many more.\n\nA third issue is that people currently have done very little in the way of moral reflection. Two people must be close to each other if they started off in the same spot, and walked 10 metres in any direction. But that gives little reason to think that they’ll remain very close to each other if they keep walking for 100 miles — even slight differences in their orientation would lead them to end up very far apart.[9](#user-content-fn-9)\n\n![A diagram depicts reflection as a series of random walks from a shared origin. In the “present day” case, no view has had time to diverge much from the origin. In the “future” case, many different views widely diverge.](https://images.ctfassets.net/4owxfjx3z3if/3Th7hTYdp4ApLzBpX4x7Nx/0b65bcd0a085c9d104c6a3571d4f91b1/moral_views_random_walk_divergence.png?w=3840&q=75&fm=webp \"Visualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\")\n\nVisualising moral reflection as an at least partially random walk in a space of views. If all views begin very close together, then further reflection seems likely to make resulting views diverge more, on average.\n\nImage\n-----\n\nWith advanced technology, this issue will get even more extreme, because people will be able to change their nature quite dramatically. Some people might choose to remain basically human, others might choose to remain biological but enhance themselves into “post-humans”, and others might choose to upload and then self-modify into one of a million different forms. They might also choose to rely on different types of superintelligent AI advisors, trained in different ways, and with different personalities. The scope for divergence between people who originally started off very similar to each other therefore becomes enormous.\n\nFinally, even if there is a lot of agreement, that doesn't mean there will be _full_ agreement. Imagine two views that disagree about what’s ultimately valuable both make their own lists of improvements to the world, ordered by priority: “First we solve the low-hanging fruit of easily-curable diseases. Then we eliminate extreme poverty. Then…” Both views might agree on the items nearest the top of both their lists. But eventually — and this point could be very far from the world today — they could simply run out of points of agreement. This would be like reaching a fork in the future: one road leads to a near-best future on the first view, but seals off a near-best future on the other, and vice-versa. And if a mostly-great future is a narrow target, then getting lots of agreement isn’t enough; the remaining disagreements would likely be enough to ensure that each other’s eutopia involves losing most value from the other’s point of view.\n\nAnd it seems there isn’t anything close to full agreement on moral matters, currently. If we look back to the list of potential future moral catastrophes from the last essay (“[No Easy Eutopia](https://www.forethought.org/research/no-easy-eutopia)”), there’s clearly not a lot of active agreement about what to do about the relevant issues, often because many people just don’t have views about them. But we’ll need to converge on most or all those issues in order for people to agree on what a mostly-great future looks like.\n\n#### 2.2.2. Moral progress\n\nA second reason you might think that it’s likely we’ll get WAM-convergence is based on moral progress to date. At least on many issues, like slavery, civil rights, democracy, attitudes to women and sexual ethics, the world’s prevailing moral views have improved over the last few hundred years. Different moral views also seem to be broadly converging in the direction of liberal democratic values. According to the World Values Survey,[10](#user-content-fn-10) as societies industrialize and then move into a post-industrial, knowledge-based phase, there's a general shift from “traditional” to “secular-rational” values and from “survival” to “self-expression” values. The world has even so converged on the importance of democracy that most non-democracies _pretend_ to be democratic — a practice that would have looked bizarre even 200 years ago.\n\nPerhaps, then, there is some underlying driver of this moral progress such that we should expect it to continue: for example, if moral progress occurred because people have gotten wiser and more educated over time, that might give us hope for further progress in the future, too. However, there are a number of reasons why this is at best only a weak argument for expecting WAM-convergence.[11](#user-content-fn-11)\n\nFirst, even if world values seem to be converging, that doesn’t mean they’re converging towards the _right_ views. We are the product of whatever processes led to modern Western values, and so it’s little surprise that we think modern Western values are better than historical values. Our personal values are very significantly _influenced_ by prevailing modern values, and it’s trivial that values have historically trended toward modern values![12](#user-content-fn-12),[13](#user-content-fn-13)\n\nThere are various possible explanations for the moral convergence we’ve seen. One is that there’s been one big wave of change towards liberal values, but that this is a relatively contingent phenomenon. History could easily have gone differently, and there would have been some other big wave instead: if China had had the industrial revolution, modern values might be less individualistic; if Germany had won WWII, or the USSR had won the Cold War, then modern values might be much more authoritarian. A second alternative explanation is that modern values are the product of the technological and environmental situation we’re in: that a post-industrial society favours liberal and egalitarian moral views, perhaps because societies with such moral views tend to do better economically. But, if so, we shouldn't think that those trends will continue into a post-AGI world, which is very unlike our current world and where, for example, human labour no longer has economic value. A third possible explanation for moral convergence is that globalisation has resulted in greater incentives for moral conformity, in order to reduce the risk of war and make international trade easier. On none of these alternative explanations does what we regard as past moral progress result in continued moral progress into the future.\n\nIt’s also easy to overrate the true extent of moral progress on a given moral view. The moral shortcomings of the world today are not necessarily as salient as the successes, and for many issues, it’s socially taboo to point out areas of moral regress — precisely because one is pointing to views that are often currently socially unacceptable or at least morally controversial. It’s therefore easy to overlook the ways in which the world has regressed, morally, by one’s lights.\n\nTo see this, consider how total utilitarianism might evaluate humanity today, and consider that social norms around family size have changed dramatically over the last two hundred years, where families are now much smaller than they used to be. From a total utilitarian perspective, this has arguably resulted in an enormous loss of wellbeing to date. Suppose, for example, that the US total fertility rate had never dropped below 4 (which in our actual history it did in the late 19th century). The current US population would be over a billion people, meaning more people to enjoy life in an affluent democracy, and also more people benefiting the rest of the world through (for example) the positive externalities of scientific research.\n\nThere are other examples one could also point to, from a utilitarian perspective. Highly safety-cautious attitudes to new technology have limited the development and uptake of socially-beneficial innovations, such as nuclear power and new drugs and medicines. The rise in consumerism means that people spend relatively more on goods that don’t really improve their quality of life. The huge rise of incarceration inflicts enormous human suffering. Globalism has been on the decline in recent decades, impoverishing the world overall. Rates of meat consumption have vastly increased, and the conditions in which we treat animals have worsened: factory farming did not exist in 1900; by 2025 over 70 billion land animals were slaughtered in factory farms.\n\nAll these cases are disputable, even on the assumption of utilitarianism. But at the very least they complicate the picture of a steady march of progress, from a utilitarian point of view.[14](#user-content-fn-14) And a similar case could be made on other moral views.\n\nA third reason for caution about future optimism based on past moral progress is that, even if there has been genuine moral progress, and even if that was driven by some mechanism that reliably delivers moral progress, there’s no guarantee that that mechanism will be enough to deliver all the moral progress we have left to make. For example: society made great strides of progress away from wrongful discrimination, subjugation, and cruelty against groups of people because those groups advocated for themselves — through persuasion, protest, striking, and movement-building. But some wrongs don’t affect groups that can advocate for themselves. Today, humans will slaughter some 200 million chickens. If chickens feel pain and fear, as it seems they do, this seems like a moral catastrophe.[15](#user-content-fn-15) But we can’t rely on chickens to organise, strike, or speak up for themselves.\n\nMany of the most crucial moral questions we’ll have to answer to get to a near-best future will be questions where we cannot rely on groups to speak up for themselves. For example, it will become possible to _design_ new kinds of beings, like digital minds. We could design them never to complain as they willingly engage in, and even genuinely enjoy, kinds of servitude which might nonetheless be bad.\n\nThen there are questions around population ethics — questions about evaluating futures with different numbers of people. On many views in population ethics, it’s good to create new lives, as long as those lives are sufficiently happy. But people who don’t exist can’t complain about their nonexistence. \n\nAnd there might be far weirder issues still, which require far more deliberate and concerted efforts to get right — like issues around [acausal trade](https://nickbostrom.com/papers/porosity.pdf), [cooperation in large worlds](https://longtermrisk.org/ecl), or dealing with the [simulation hypothesis](https://simulation-argument.com/simulation.pdf).\n\n### 2.3. Post-AGI reflection \n\nWe’re considering whether there will _ultimately_ be WAM-convergence. But, given survival, the world will look very different to the present, especially after an intelligence explosion. In this section, we discuss three aspects of the post-AGI world that we think are important to bear in mind when thinking about the possibility of WAM-convergence. These aspects sometimes get used as arguments for optimism about convergence; we think they have some real force in this regard, but that force is limited. \n\n#### 2.3.1. Superintelligent reflection and advice \n\nAccess to superintelligence could radically increase the amount of reflection people can do. And this could be a cause for optimism — maybe moral disagreement persists largely because moral reasoning is just too hard for humans to do well and consistently. Here’s Nick Bostrom in _Superintelligence_:\n\n> One can speculate that the tardiness and wobbliness of humanity's progress on many of the “eternal problems” of philosophy are due to the unsuitability of the human cortex for philosophical work. On this view, our most celebrated philosophers are like dogs walking on their hind legs - just barely attaining the threshold level of performance required for engaging in the activity at all.\n\nIn the future, we could design superintelligent AI systems to be [smarter, more principled, more open-minded, and less prone to human biases](https://www.forethought.org/research/how-to-make-the-future-better#41-deliberative-ai) than the wisest moral thinkers today. Perhaps AI systems could finally settle those stubborn puzzles of ethics that seem so intractable: we could just ask the AI to figure out which futures are best, tell them to set a course for those futures, and then enjoy the ride to eutopia.\n\nWhat’s more, as mentioned previously, much apparent moral disagreement might really bottom out in _empirical_ disagreements. Or they could arise from ‘transparent’ errors in reasoning from shared principles, in the sense that they become clear to the person making the error once pointed out. If enough current disagreement ultimately arises from empirical disagreements or transparent reasoning errors, then you could maintain that as long as the future trends towards fewer empirical and conceptual mistakes, it wouldn’t be surprising if most people end up converging toward the same place. And since AI systems seem well-suited to help clarify and resolve clear empirical and logical disagreements, then we should expect future people to have much more accurate empirical beliefs, and far fewer transparent reasoning errors.\n\nIt does seem very likely that advanced AI will give society as a whole dramatically more ability to reflect than we have had to date. Sheer cognitive abundance means that each person will be able to run billions of personal reflection processes, each of which simulates scenarios where they were able to reflect for millions of years, from different starting points and with different ways of reflecting. And it seems very likely that AI will ultimately be able to do ethical reasoning better than any human can, and thereby introduce arguments and considerations that human beings might never have thought of.\n\nBut that doesn’t mean that people will all or mostly converge to the same place. First, even if different people do all want to act on their post-reflection views of the good (de dicto), the fact that future people will have superintelligent assistance says little about whether we should expect moral convergence upon reflection, from different people who have different ethical starting points and endorse different reflective processes. Different people’s reflective processes might well lead them in very different directions even if they make no empirical or conceptual mistakes. We’ll discuss this more in section 2.4.\n\nSecond, people might just be uninterested in acting on the results of an open-ended process of reflecting on the good (de dicto). They might be interested in acting in accordance only with the outcome of some _constrained_ reflective process, that assumes and can’t diverge from some preferred starting beliefs, like environmentalism, some specific religious views, or an overwhelming concern for the interests of biological humans (or a group of humans). As an analogy: when today’s politicians select advisors, they choose advisors who broadly agree with their worldview, and task those advisors with advice on how to execute on that worldview, rather than advice about how their values could be fundamentally off-base.\n\nAlternatively, people might just not be interested in acting in accordance with the result of _any_ kind of ethical reflective process. Suppose that it’s your partner’s birthday and that you want to buy them a gift. You might strongly suspect that after countless years of earnest ethical reflection, you’d reach a different (and motivating) view about how to spend that money, perhaps because you’d become unable to rationally justify giving your partner’s interests any special importance over anyone else’s.[16](#user-content-fn-16) But would you go along with what your reflective-self recommends?\n\nThird, of course, people might just remain self-interested. If two people want what’s best _for them_, they might be motivated by the result of reflecting on how to get more of what they want, but it needn’t matter if their AI advisors converge on the same conception of the good, because the good (de dicto) was never motivational for them.\n\nWe suspect the most common attitude among people today would either be to reject the idea of reflection on the good (de dicto) as confusing or senseless, to imagine one’s present views as unlikely to be moved by reflection, or to see one’s idealised reflective self as an undesirably alien creature. If people strongly _dislike_ the suggestions of their more-reflective selves, then they probably won’t follow their suggestions. (This idea is discussed more in [an essay by Joe Carlsmith](https://joecarlsmith.com/2021/06/21/on-the-limits-of-idealized-values)).[17](#user-content-fn-17)\n\nThe situation becomes a bit less clear if your reflective self has time to walk you through the arguments for why you should change your views. But, even then, I think people will often just not be moved by those arguments — the fact that some argument would change someone’s view if they were given millions of years to reflect (with much greater intellectual capacity, and so on) doesn’t make it likely that that argument would change their view in a short period of time, given their actual cognitive capacity, unless that person already had a strong preference to defer to their reflective selves. Imagine, for example, trying to explain the importance of online privacy to a Sumerian priest in 2,500BCE.\n\nCurrently, people generally don’t act as if they are profoundly ignorant about moral facts, or especially interested in discovering and pursuing the good de dicto. People are better described as having a bunch of things they like, including their own power, and they directly want more of those things. We don’t see a reason, on the basis of superintelligent advice alone, for expecting this to change.\n\n#### 2.3.2. Abundance and diminishing returns\n\nA post-AGI world would very likely also be a world of economic and material _abundance_. Due to technological advancement and the automation of the economy, the people who make decisions over how society goes will be millions or even billions of times richer than we are today.\n\nThis suggests an argument for optimism about altruistic behavior. Most people have both self-interested and altruistic preferences. Self-interested preferences typically exhibit rapidly diminishing marginal utility, while altruistic preferences exhibit more slowly diminishing marginal utility (even up to linearity).[18](#user-content-fn-18) Initially, when resources are scarce, people primarily satisfy self-interested preferences. But once wealth increases enough, people will shift marginal resources toward altruistic ends, since the marginal utility from additional self-interested consumption becomes smaller than the marginal utility from altruistic spending.\n\nBecause of this, if individuals have even a weak preference to promote the good, with extremely large amounts of resources they will want to use almost all their resources to do so.\n\n![Two-panel diagram showing optimal spending allocation and utility curves. Top shows spending shifts from self-interested (blue) to altruistic (orange) as income increases. Bottom shows marginal utility curves for both spending types.](https://images.ctfassets.net/4owxfjx3z3if/4a48lCdMaZMgCTvM9h13fg/4a1d5162397ad88e50e64a697a626844/optimal_spending_utility_curves_comparison.png?w=3840&q=75&fm=webp)\n\nImage\n-----\n\nWe think that this is a good argument for thinking that, if there are deliberate decisions about how the resources of the cosmos are to be used, then the bulk of those resources will be used to fulfill preferences with the lowest rate of diminishing marginal utility in resources (including linear or even strictly convex preferences). However, these preferences may not be altruistic at all, let alone the “right” kind of altruistic preferences.\n\nThere’s nothing incoherent about self-interested (or non-altruistic) preferences which are linear in resources. Some might prefer 2,000 years of bliss twice as much as 1,000 years of bliss, or prefer to have 2,000 identical blissful copies of themselves twice as much as having 1,000 copies. Others might have a desire to own more galaxies, just for their own sake, like collector’s items; or to see more and more shrines to their image, while knowing they don’t benefit anyone else. Or people might value positional goods: they might want to have, for example, more galactic-scale art installations _than their cosmic rival does_.\n\nCurrently, there’s [very little correlation](https://econofact.org/are-rich-people-really-less-generous) between wealth and proportional spending on charity and altruism; the correlation within rich countries might even be [negative](http://www.cafonline.org/docs/default-source/high-value-giving/caf_high_value_giving_report.pdf). Despite steeply diminishing returns to self-interested spending, even billionaires spend proportionally modest amounts on philanthropy. Total billionaire wealth is [around $16 trillion](https://www.forbes.com/sites/chasewithorn/2025/04/01/forbes-39th-annual-worlds-billionaires-list-more-than-3000-worth-16-trillion/): assuming a 5% real return gives $800B in income without drawing down their principal, but billionaire philanthropy amounts to around $50B/yr,[19](#user-content-fn-19) around 6% of that income.[20](#user-content-fn-20) That weakly suggests that even vastly increased wealth will not significantly increase the fraction of spending on altruistic ends, especially once more advanced technology provides greater scope to spend money in self-interested ways.\n\nAlternatively, people could have _ideological_ preferences whose returns diminish slowly or not at all, and that are not preferences for promoting the good (de dicto). As discussed, they could instead be operating on the assumption of some particular moral view, or according to unreflective preferences. Or, even if people are acting according to their reflective preferences of the good, those reflective preferences could have failed to converge, motivationally, onto the right values.\n\nOverall, the argument that altruistic preferences are satisfied less quickly in resources does suggest to us that a larger share of resources will be spent altruistically in the future. But we don’t see it as a strong argument for thinking that most resources will be spent altruistically, or for giving us reason for thinking that such preferences will converge to the same correct place.\n\n#### 2.3.3. Long views win\n\nAnother way in which altruistic values might become dominant in the future is through _asymmetric growth_: without persuading or coercing other groups, certain values could be associated with faster growth in population or wealth, to the point where most living people embody those values, or most of the world’s resources are effectively controlled by those values.\n\nAsymmetric growth might systematically favour altruistic values. For example, people face decisions about whether to consume now, or to save for the future (including for other people). As long as most people discount future consumption, then saving your wealth (by investing it productively) grows it proportionally. People with more impartial and altruistic views are likely to save more, because this is a way to eventually help more people, at the expense of immediate self-interest. Altruists might also choose to have more children for similar reasons. So, in the long run, free choices around saving and fertility might select for altruistic values.[21](#user-content-fn-21) Perhaps people with these altruistic views are generally more willing to try to figure out and try to pursue the good de dicto. Then that would be a reason to be hopeful about widespread, accurate, and motivational convergence.\n\nBut, as with the last two sections, this isn’t a reason for thinking that motivation to promote the good de dicto will become predominant. Even if non-discounting values win out over time, those non-discounting values could be for many different things: they could be self-interested in a way that doesn’t discount one’s interests in time, or they could be based on a misguided ideology.\n\nWhat’s more, non-discounting values might not have time to win out. If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out. (It could be that there are much faster ways for non-discounting views to control more of the future than discounting views, through trade. That idea will be discussed in section 3.)\n\nFinally, of course, the kind of moral attitudes favoured by these asymmetric growth and other selection processes might just be wrong, or unconnected to other necessary aspects of pursuing the good de dicto. There are [plenty of ways](https://www.forethought.org/research/no-easy-eutopia#231-scale-insensitivity-and-misguided-population-ethics) to be morally motivated, extremely patient, and totally misguided. Perhaps it’s _right_ to discount future value!\n\n### 2.4. An argument against WAM-convergence\n\nIn this section, we give an argument against expecting WAM-convergence, based on thinking through the implications of different meta-ethical positions. Stating it briefly, for now: if some form of moral realism is true, then the correct ethical view would probably strike most people as very weird and even alien. If so, then it’s unlikely that people will be motivated to act upon it, even if they learn what the correct moral view is; what’s more, if moral beliefs are intrinsically highly motivating, then people will choose _not_ to learn or believe the correct moral view.\n\nAlternatively, if some form of moral anti-realism is correct, then there is no objectively correct moral view. But if so, then there’s little reason to think that different people will converge to the exact-same moral views. Given our [no easy eutopia](https://www.forethought.org/research/no-easy-eutopia) discussion, that means that not enough people would converge on just the right moral views required to avoid losing out on most value, by the lights of their reflective preferences.\n\nThis was all stated in terms of realism and antirealism. We’ll continue to use these terms in the longer argument. But if you find the idea of moral realism confused, you could just think in terms of predictions about convergence: whether beings from a wide variety of starting points (e.g. aliens and AIs as well as humans) would, in the right conditions, converge on the same ethical views; or whether such convergence requires a close starting point, too (e.g. such that humans would converge with each other, but aliens and AIs wouldn’t); and whether, in those conditions under which you would expect convergence, you would endorse the views which have been converged upon.\n\n#### 2.4.1. Given moral realism\n\nWe’ll start by considering moral realism. Overall, we think that widespread, accurate, motivational convergence is more likely given moral realism. But it’s far from guaranteed. If moral realism is true, then the correct moral view is likely to be much more “distant” from humanity’s current preferences than it is today, in the sense that there’s much more likely to be a gap between what morality requires and what people would have wanted to do anyway.  On realism, people in the future are more likely to have the same (correct) beliefs about what’s right to do, but they are less likely to be motivated to act on those beliefs.\n\nThis seems true on both “internalist” and “externalist” forms of realism. “Internalist” views of moral judgement understand belief in a moral claim to be essentially motivating: if you believe that stealing is morally wrong then (all other things being equal) you’ll be more averse to stealing than otherwise. If internalism about moral judgements is correct, then it’s more likely that moral convergence would be motivating. “Externalist” views of moral judgement reject that there’s any necessary connection between moral judgements and practical motivation: you could genuinely come to believe that stealing is morally wrong, but become no less averse to stealing.\n\nEither way, we shouldn’t be confident that people will act on the correct moral beliefs. If internalism is the right view on moral judgements, then people might just prefer not to learn facts that end up motivating them to act against their personal interests. Maybe something like this is going on when we feel reluctant to be lectured about our obligations to donate money to charity. On the other hand, if externalism is the right view of moral judgements, then society could converge on the right moral views, but not necessarily be motivated to act on them. Maybe something like this is going on when we come to believe we have obligations to donate money to charity, but still don’t.\n\n#### 2.4.2. Given antirealism \n\nNext, let’s see what follows if [moral realism is false](https://plato.stanford.edu/entries/moral-anti-realism/): if there aren’t any true mind-independent moral facts for society to converge on. We’ll focus in particular on subjectivism. (Though note that academic philosophers often refer to subjectivism as a form of realism, just a “non-robust” sort of realism). If subjectivism is true, then the gap between what morality requires and what people would have wanted to do anyway becomes much smaller. But it becomes _much_ less likely that different people will converge.\n\nOn the simplest kind of subjectivism, an outcome A is morally better than another outcome B just if you prefer for A to happen rather than B.[22](#user-content-fn-22) On this view, “accurate” moral convergence would just mean that prevailing views in society converge in a widespread and motivating way toward your own current preferences. But different people today, clearly, have very different preferences (including preferences which are _indexical_ to them — i.e. they want _themselves_ not to starve much more than they want people _in general_ not to starve), so future society can’t converge towards all of them. So the simplest kind of subjectivist would have to say that “accurate” moral convergence is very unlikely.[23](#user-content-fn-23)\n\nBut there are more sophisticated kinds of subjectivism. For example, a subjectivist could view the right actions as those outcomes which some kind of idealised version of themselves would prefer. We could understand “accurate moral convergence” in terms of convergence towards some kind of idealised judgements. So accurate, widespread, motivational convergence might still be on the table.\n\nThe question here is whether the idealising processes that different people use all point towards the same (or very similar) views, especially about impartial betterness. \n\nThat would be true if there is some kind of _objectively_ correct idealising process.[24](#user-content-fn-24) But the idea that there’s some objectively correct idealising process seems inconsistent with the basic motivation for subjectivism — namely, suspicions about the idea of any notion of objective moral correctness. If you don’t _endorse_ some alleged objective idealising process, why should you follow it, rather than the process you in fact prefer? \n\nBut if the idealising process is itself subjective — based on how you currently would like to reflect, if you could — then WAM-convergence seems unlikely. This is for two reasons.\n\nFirst, there are a number of important “free parameters” in ethics, and it’s hard to see why different idealising processes, from different starting points, would converge on the same view on all of them. Suppose that two people converge all the way to both endorsing classical utilitarianism. This is not close to enough convergence for them to agree on what a mostly great future looks like. In order to get that, both people would need to agree on what hedonic wellbeing really is, and how _precisely_ to bring about the most wellbeing with the time and resources available. Both people need to agree, for example, on what sizes and types of brains or other physical structures are most efficient at producing hedonic wellbeing. And, even assuming they agree on (say) a computational theory of mind, they also need to agree on what experiences are actually best.\n\nBut there must be a vast range of different kinds of experience which computation could support, including kinds never before experienced, hard to even discover, or impossible to experience while maintaining essential facts about your personal identity. And, given antirealism, there are no shared, obvious, and objective qualities of experiences themselves that indicate how valuable they are.[25](#user-content-fn-25) So there’s no reason to expect significant convergence among antirealists about which experiences involve the most “efficient” production of “hedonic value”.[26](#user-content-fn-26) Antirealists can appeal to “shared human preferences” as a reason for moral convergence. But such preferences are extremely underpowered for this task — like two people picking the same needle out of an astronomical haystack.[27](#user-content-fn-27),[28](#user-content-fn-28)\n\nAnd, once we move beyond assuming classical utilitarianism, there are many other “free parameters”, too. What all-things-considered theory of welfare is correct, and to what precise specification? What non-welfarist goods, if any, should be pursued? How should different goods be traded off against each other? If there are non-linear functions describing how value accrues (e.g., diminishing returns to certain goods), what is the exact mathematical form of these functions? Each of these questions allows for a vast space of possible answers, and we don’t see why different subjective reflective processes, starting from different sets of intuitions, would land on the same precise answers.\n\nSecond, the idealising procedure itself, if subjective, introduces its own set of free parameters. How does an individual or group decide to resolve internal incoherencies in their preferences, if they even choose to prioritize consistency at all? How much weight is given to initial intuitions versus theoretical virtues like simplicity or explanatory power? Which arguments are deemed persuasive during reflection? How far from one's initial pre-reflective preferences is one willing to allow the idealization process to take them?[29](#user-content-fn-29)\n\nIf moral antirealism is true, then the bare tools of idealisation — like resolving inconsistencies and factual disagreements — are greatly underpowered to guarantee convergence. A subjectivist shouldn’t hold out hope that society converges on a view they themselves would endorse.[30](#user-content-fn-30)\n\n### 2.5. Blockers\n\nEven if, in a reasonably good scenario, there would be widespread, motivational, accurate convergence, we might still not get to that sufficiently good scenario. I’ll call the ways in which society might fail to get to such a scenario _blockers_. One clear blocker is the risk of extinction. But there are other potential blockers, too, even if we survive. \n\nFirst, there is the risk that humanity will not choose its future at all. In this scenario, the trajectory of the long-term future is best explained as being the outcome of evolutionary forces, rather than being the outcome of some sort of deliberative process. Even if nearly everyone is motivated by the same moral view, still society could collectively fail to bring it about.[31](#user-content-fn-31)\n\nSecond, people in the future could have the wrong _non-moral_ views, especially if those views have greater memetic power (in some circumstances) than the correct views.[32](#user-content-fn-32) Some ideas might even be so memetically potent that merely considering the idea makes an individual highly likely to adopt it; such ideas might also discourage further change, and become impossible to get out of, like epistemic black holes. \n\nThird, there could be early lock-in: when the most-important decisions are being made, the decision-makers at the time are _unable_ to bring about a mostly-great future, even if they tried.  We’ll discuss lock-in more in the next essay.[33](#user-content-fn-33)\n\nOf course, there could also be additional and as-yet unknown blockers.\n\n3\\. What if some people aim at the good?\n----------------------------------------\n\nIn this section, we’ll consider the possibility that (i) under reasonably good conditions, some meaningful fraction of people (weighted by the power they have) would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good (de dicto), and (ii) those people will be able to bargain or trade with each other, such that we’ll get to a mostly-great future.  By “some meaningful fraction” we mean a minority of all people, but not less than (say) one in a million people.[34](#user-content-fn-34) So understood, we’ll call idea (i) _partial AM-convergence_. \n\nIn this section, we’ll discuss whether bargaining and trade will be enough to reach a mostly-great future, given partial AM-convergence.\n\nIn section 3.1, we’ll illustrate how such “moral trade” could work, and discuss the conditions under which such trade could occur. In section 3.2, we discuss the conditions under which trade could enable a near-best future, and in section 3.3 we discuss the problems of threats. In section 3.4, we put these considerations together, depending on whether the correct moral view is unbounded or bounded, whether bads weigh heavily against goods, and whether bads and goods are aggregated separately or not.  In section 3.5, we discuss blockers to a mostly great future via AM-convergence and trade.\n\n### 3.1. Trade and compromise\n\nImagine that a multi-millionaire has a niche but intense wish to own every known Roman coin with a portrait of Julius Caesar. He might not be able to own every coin: some are kept in museums, some private owners are unwilling to sell for sentimental reasons. But he might manage to acquire _most_ Julius Caesar coins in the world, despite controlling a small fraction of the world’s wealth. When he paid each collector for their coin, they preferred his money to their coin, and the multi-millionaire preferred the coin, so each party was glad to make the trade.\n\nSomeone could trade for moral reasons, too. You could find some voluntary exchange where you (or both parties) are motivated by making the world a better place. For the price of a caged bird, for example, you can pay to set a bird free. For the price of a plot of Amazonian land, you can save that land from being deforested. But even better bargains are on offer when you have moral reasons to care _far_ more about some outcome than your counterparty. You could (in theory) pay a retailer to discount alternatives to animal products, for example. Or _both_ parties could trade for moral reasons: Annie cares a lot about recycling and Bob cares a lot about littering, so Bob might agree to start recycling if Annie stops littering, and both parties agree the world is now a better place. This is the promise of _moral trade_.[35](#user-content-fn-35)\n\nIn the future, there could be potential for enormous gains from trade and compromise between groups with different moral views. Suppose, for example, that most in society have fairly common-sense ethical views, such that _common-sense utopia_ (from the last essay) achieves most possible value, whereas a smaller group endorses total utilitarianism. If so, then an arrangement where the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies and turns them into a total utilitarian utopia, would be one in which both groups get a future that is very close to as good as it could possibly be. Potentially, society could get to this arrangement even if one group was a much smaller minority than the other, via some sort of trade. Through trade, both groups get a future that is very close to as good as it could possibly be, by their lights.\n\nSome reasons for trade might become less relevant in a more technologically advanced society. For example, with superintelligent advice, investors might less often have differing expectations about the future, which would be a reason for trading volume to decline (all else equal).[36](#user-content-fn-36)\n\nBut other reasons for trade would likely remain equally or more relevant, even in a technologically mature society. Different groups could continue to:\n\n*   Value different natural resources\n    \n    *   For example, some groups might want star systems that contain rocky planets in the habitable zone to live on; other groups might want to preserve particularly beautiful areas of the cosmos; etc.[37](#user-content-fn-37)\n        \n    \n*   Value different locations of resources\n    \n    *   For example, some groups might intrinsically value resources in our own solar system more than those in distant galaxies; other groups might be indifferent.\n        \n    \n*   Have different rates of pure time preference\n    \n    *   Some groups might value being able to acquire tradeable goods _soon_ (such as to benefit currently-living people), while others might be indifferent about when they acquire or use them.\n        \n    \n*   Have different attitudes to risk, when some risk is ineliminable\n    \n    *   For example, even for an extremely technologically advanced society, it might be impossible to know for sure whether a distant galaxy will have been settled by aliens or not.\n        \n    \n\nIn many cases, two views can be “resource-compatible”, meaning there is some way to _almost_ fully satisfy both views with the same resources, even if the _best_ use of the resources on one view is mostly worthless to the other view, such that the gains from trade between two moral views controlling comparable resource shares could be large from each view’s perspective. For example: hedonists might only care about bliss, and objective list theories might care primarily about wisdom; they might potentially agree to create a shared society where beings are both very blissful and very wise. Abstractly speaking: the same resources can sometimes achieve much more value on two different views if they are all devoted to some “hybrid good” than if the resources were simply split evenly between the two views without the possibility of bargaining or trade.\n\nWhat’s more, some of the reasons why we _don’t_ get trade today would no longer apply. As discussed in the next essay, superintelligence could enable iron-clad contracts, which could avoid the problem of a lack of mutual trust. And transaction costs would generally be extremely small relative to the gains, and more potential trades would be salient in the first place, given an enormous number of superintelligence delegates able to spend abundant time in order to figure out positive-sum arrangements.\n\n### 3.2. Would trade enable a mostly-great future?\n\nIn _many_ cases, it looks like there will be the potential for truly enormous gains from moral trade. This is a significant cause for optimism. For example, even if most people care little about the welfare of digital beings, if it’s sufficiently low-cost for them to improve that welfare, the minority who does care about digital welfare would be able to bargain with them and increase their welfare considerably. \n\nMore generally, we can consider some key possibilities relating to moral trade:\n\n1.  The extent to which the hypothetical gains from frictionless trade are actually realised.\n    \n2.  The (moral) gains from trade from all trades that in fact take place.[38](#user-content-fn-38)\n    \n3.  The (moral) gains from trade for the correct view.[39](#user-content-fn-39)\n    \n4.  The value of the world, on the correct moral view, after trade.[40](#user-content-fn-40)\n    \n\nWhat ultimately matters is (4), the value of the world on the preferred view after trade. But if you are uncertain over moral views, then you should also be interested in (2): the extent to which most views gain from trade. Of course, (2)–(4) depend on (1), whether possible gains from trade are actually realised. That could depend on whether, for example, the right institutions exist to support trade, and also the extent to which different views actually take trades which improve the world by their lights, which isn’t guaranteed.[41](#user-content-fn-41)\n\nHowever, mutual gains from trade seems especially unlikely to us if the prevailing views are  non-discounting and linear-in-resources. It’s possible in principle, because of the potential for hybrid goods. But if the ways to achieve maximum value/cost on each view are both highly particular, then it’s unlikely any compromise could achieve much more value (by the lights of each view) than if each view kept their resources for themselves.\n\nIn fact, there may even be a “narcissism of small differences” effect: if two views agree on the importance of the very same domains or aspects of the world, but disagree over what to do with them, then the gains from compromise could be smaller, despite the views seeming superficially similar. Suppose Annie and Bob agree on the importance of spiritual worship, but disagree over which deity to worship. By contrast, Claudia mostly cares about the environment. Although Annie and Bob’s views are superficially similar, the mutual gains from moral trade between Claudia and either of Annie and Bob are likely larger than those between Annie and Bob, because Annie and Bob both agree there is no “hybrid” activity of worshipping two deities at the same time.\n\nGiven this, it becomes crucial to know: assuming that the right moral view is non-discounting and linear-in-resources, how much control over resources is such a view likely to have, before trade?\n\nThe case for thinking that non-discounting and linear _moral_ views are likely to begin with a significant share of resources is that, currently, such cosmic-scale values are very unusual, and are unusually likely to be held by the altruistically-minded. Of course, altruistically-minded people disagree widely today, so we’d also have to hope for significant AM-convergence among those people after some period of reflection. The counterargument is that, in a technologically advanced society, many _more_ types of people will have views that are non-discounting and linear, for two reasons. First, superintelligence-aided reflection might plausibly cause people to be much more likely to adopt views that are non-discounting and linear. Second, as we saw in section 2.3.2., enormous wealth might cause people to shift focus towards their preferences which diminish most slowly with resources, even if those preferences are self-interested.\n\nPutting this together, it’s hard to know what fraction of resources the correct view will control among all other non-discounting and linear views, assuming that the correct moral view has those properties. Here’s a rough guess: if (at the relevant period of bargaining) 1 in NN people out of everyone have altruistic preferences, then the fraction of those people among everyone with approximately linear preferences of any kind, is something like 10 in NN.\n\n### 3.3. The problem of threats \n\nIf trade and bargaining are reasons for optimism, then _threats_ could undermine that optimism. Suppose that Alice and Bob want different things, and bargain with each other. Alice could extort Bob by credibly committing to make the world worse on Bob’s view, unless Bob makes concessions to Alice. In the previous examples, both parties to a trade can agree to the trade itself, and also agree the world is made no worse-off by whatever commitment or enforcement mechanisms _enabled_ the trade[42](#user-content-fn-42) in the first place. Threats are different. When one party carries out a threat, they may both agree to the terms of the threat, but at least one party might view the world as made worse-off by the threat having been made, regardless of whether they capitulated to it. Imagine Bob cares a lot about animals being mistreated, and Alice doesn’t. Then Alice can threaten to mistreat lots of animals to extort resources from Bob for uses he doesn’t value at all. Then, whatever Bob chooses, he’ll likely view the world as worse than it would have been if Alice had never been in the picture.[43](#user-content-fn-43)\n\nThe extent of public writing on threats is very limited;[44](#user-content-fn-44) sometimes just learning about the topic can make it more likely for threats to occur, so people are naturally reluctant to spread their research widely. We ourselves have not particularly dug into this issue, despite its importance.\n\nBut, significantly, even small risks of executed threats can easily eat into the expected value of worlds where many groups with different values are able to bargain with each other. On many views, bads weigh more heavily than goods: this is true on negative-leaning views, or on views on which value is meaningfully bounded above but not meaningfully bounded below, and goods and bads should be aggregated separately; or simply on empirical views where it turns out some bads are much cheaper to create with the same resources than any compensating goods. If so, then even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost;[45](#user-content-fn-45) if a large fraction of resources are devoted to executed threats, then most value could be lost even if the correct view is not negative-leaning.[46](#user-content-fn-46) What’s more, those who hold the correct moral view may be less likely to themselves threaten other groups, so even if no threats are ever executed, those people could still lose most of their resources via extortion. Finally, it’s not obvious to us that some kind of legal system which reliably prevents value-undermining threats would be mutually agreeable and stable, so the worry does not only apply to legal ‘anarchy’ between views.[47](#user-content-fn-47)\n\n### 3.4. Putting it all together\n\nHow much more optimistic should the idea of trade and compromise make us? We can give an overall argument by looking at different types of moral view, using some of the distinctions we covered in the last essay, [_No Easy Eutopia_, section 3](https://www.forethought.org/research/no-easy-eutopia#3-which-views-are-fussy).\n\nIf executed moral threats amount to a small but meaningful fraction of future resource use, then if the correct view:\n\n*   Is bounded above, and goods and bads are aggregated jointly, and if bads don’t weigh very heavily against goods, a mostly-great future seems likely. \n    \n*   Is bounded above, and goods and bads are aggregated jointly, and bads do weigh heavily against goods, then a mostly-great future seems unlikely, because the executed threats eat into the value of the goods.\n    \n*   Is bounded above, and goods and bads aggregated separately, then a mostly-great future seems unlikely. \n    \n    *   If the view is bounded below (with a compensating or greater magnitude to the lower bound), then future society is likely to reach both upper and lower bounds, and we will end up with a future that’s at most of net zero value.\n        \n    *   If the view is unbounded below, or the magnitude of the lower bound is much greater than the magnitude of the upper bound, then the bads will outweigh the goods, and we’ll probably end up with a net-negative future. \n        \n    \n*   Is unbounded above, then even if bads don’t weigh heavily against goods, a mostly-great future seems uncertain.\n    \n    *   If the correct view is unusually resource-compatible with other prevailing views, then a mostly-great future is plausible. But resource-compatibility between linear views seems unlikely, and if so then a mostly-great future seems unlikely, too.\n        \n    \n*   Is unbounded above, and bads do weigh heavily against goods, then a mostly-great future is unlikely, because executed threats eat into the value of the goods.\n    \n\nIf value-destroying threats can be prevented, then things seem more optimistic. In this scenario, then it’s just on the linear views that we don’t reach a mostly-great future via compromise and trade. \n\nWe think it’s appropriate to be highly uncertain about which axiological view is correct. Given that, it’s worth considering what the value of the future looks like, from our uncertain vantage point. Two things stand out. First, most views we’ve considered are sensitive to value-destroying executed threats, which suggests we should try hard to prevent such threats, even if doing so is itself costly. Second, some views are highly resource-compatible with others; in cosmic terms it could be very inexpensive to achieve a near-best future for many of those views by giving even just a small fraction of all resources to views that are easily-satiable.[48](#user-content-fn-48) We should aim to do so.\n\n### 3.5. Blockers to trade\n\nThe scenarios where we get to a mostly-great future via trade and compromise face the same major blockers that we discussed in section 2.5, and two other blockers, too. First, _concentration of power_. If only a small number of people have power, then it becomes less likely that the correct moral views are represented among that small group, and therefore less likely that we get to a mostly-great future via trade and compromise. \n\nSecond, even if power is not concentrated, the most valuable futures could be sealed off. We already saw one way this could be so: if people make and execute value-destroying threats. But there are a lot of other ways, too. For example, if decisions are made by majority rule, then the majority could ban activities or goods which they don’t especially value, but which minority views value highly.[49](#user-content-fn-49) \n\nAlternatively, collective decision-making procedures can vary in the extent to which they elicit decisions made on the basis of good reasons versus other things. Perhaps decisions are made democratically, but such that the incentives favour voting in order to signal allegiance to a social group. And even if some collective decision-making process is designed to give everyone what they want, that can be very different from a decision-making process that is the best at aggregating everyone’s best guesses at what the right decision is; these two broad approaches can result in very different outcomes.[50](#user-content-fn-50)\n\nEven the same decision-making procedure can give very different results depending on _when_ the decision occurs. If, for example, Nash bargaining were to happen now, then people with quasilinear utility might get most of what they want just by getting the goods that have heavily diminishing utility for them. But, once such people are already very rich, their preferences for additional goods would be linear, and couldn’t be so easily satisfied.[51](#user-content-fn-51) And, of course, the outcome of any bargaining process depends sensitively on the power distribution among the different bargainers, and on what would happen if no agreement occurs; this is something that can vary over time.\n\nThe more general point is that the outcomes we get can vary greatly depending on which collective decision-making processes are used. Some such processes might well be predictably much better than others.\n\n4\\. What if no one aims at the good?\n------------------------------------\n\nIn sections 2 and 3, we looked at scenarios where at least some people were deliberately optimising towards the good (de dicto). But let’s now discuss whether this is really needed. Perhaps we can hit a narrow target even if no one is aiming at that target. \n\nThe key argument against such a position is that it would seem like an extraordinary _coincidence_ if society hit the narrow target without explicitly trying to do so. So there needs to be some explanation of why this isn’t a coincidence. \n\nThere are possible explanations. In terms of our analogy of sailing to an island:  perhaps none of the shipmates intrinsically care about reaching the right island, but do care about their own salary, and get paid only if they do their job. If all the shipmates just do their job well enough then the ship reaches the island: the shipmates’ desire for their salary steers the ship in the right direction. Alternatively, consider again that flight evolved in animals, more than once, even though no agent tried to optimise for that target; flight evolved because it was useful for maximising reproductive fitness.\n\n### 4.1. If most aim at their self-interest\n\nFirst, let’s consider cases where (i) nobody is aiming at the good (de dicto), and (ii) most people are pursuing their own local interests, but (iii) absent blockers, most people converge on the right view of what’s best _for them_. Couldn’t that still be enough to reach a mostly great future?\n\nIf there’s some strong correlation between what’s good for every individual and what’s good overall, then it isn’t a coincidence that people pursuing what’s best for them results in what’s near-best overall.\n\nConsider an analogy with material wealth. Historically, most people have been motivated to pursue material wealth for themselves and their circle of concern (like their loved ones or family). Very few people across history seem to have been motivated to pursue material wealth for the entire world, or across the entire future, or even for their own countries. And yet, humanity at large is vastly more materially wealthy than it was even a few centuries ago. “It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner,” wrote Adam Smith, “but from their regard to their own interest.”\n\nHow did this happen? One happy fact is that two parties can both become richer, in the long run, by engaging in forms of collaboration and coordination. This is true at many levels of organisation — from small groups, through to national systems of law and commerce, through to international trade and coordination. Perhaps, if most people converge on wanting what’s ultimately good for them, the same trend could enable people to increasingly improve their own lives, and that could be enough to make the future mostly-great.\n\nTo be sure, given cognitive and material abundance, people will probably and in general get much more of what they self-interestedly want than they do today. But the crucial question is whether people pursuing what’s best for themselves is a strong enough driver towards what’s best _overall_. Otherwise, again, a world driven by self-interest could become staggeringly better than the world today, but still only a fraction of how good it could be, for want of a few crucial components.\n\nOn some moral and meta-ethical views, the overlap between the aggregate of individual self-interest and overall value _might_ be strong enough. Consider a view on which (i) totalising welfarism is the correct axiological view; and (ii) “meta-ethical hedonism” is correct, in the sense that goodness is a property of conscious experiences that we have direct access to, and coming into contact with that property makes it more likely that the experiencer will believe that that experience is good.[52](#user-content-fn-52) On such a view, it becomes more plausible that everyone pursuing their own self-interest will converge on enjoyment of the _same_ kind of highly valuable experiences, and that such a world is near-best overall. If so, everyone pursuing their own self-interest could well lead toward a very similar outcome than if they were all motivated instead to promote the moral good.[53](#user-content-fn-53)\n\nWe think these views have some merit, but we don’t think you should be at all confident in either of them. And, even if we accept them, people pursuing their own self-interest is only sufficient to create a mostly-great future if the most valuable experiences can be experienced by those people. But, plausibly this is wrong, if the very best experiences are so alien that most people initially pursuing their self-interest cannot _themselves_ experience them; similar to how a bat can’t comprehend what it is like to be a human without ceasing to be a bat. If so, most people can only bring about the best experiences for some other kind of person, not for themselves. So most people would not pursue them only out of self-interest.[54](#user-content-fn-54)\n\nAnd if we drop either of these views, then the correlation between self-interest and the overall good is just not strong enough. People could be motivated to improve their own lives, but not to create enough additional good lives, if doing so doesn’t also benefit them. They could even be motivated to create low-value or even bad lives (e.g. digital servants) if doing so does benefit them. Or some aspects of value of the future might not be good for anyone, like the possibly intrinsic value of nature or beauty. Or some things could even be locally good, but bad overall — for example, on some egalitarian views, making already well-off people even better off can be bad because it increases inequality.\n\nPeople could also fail to pursue their own good. If there is a single correct conception of the prudential (self-interested) good, then many of the arguments for no easy eutopia would also suggest that the target for prudential good is also narrow, not obvious, and non-motivational perhaps even after reflection. Suppose, for example, that hedonism about wellbeing is correct; nonetheless, people might just prefer not to turn themselves into machines for generating positive experiences — perhaps reasonably.\n\nWith these points in mind, we can see that the analogy between individual and total good, and creation of individual and global wealth, could be very misleading. Wealth is held and generated by people, but there could be forms of value which are not good for anyone, like the value of nature. The instrumental value of money is obvious, but what is ultimately good for somebody might not be. Moreover, it’s not even clear that self-interest would succeed in generating nearly as much wealth, in the long run, as some termite-like motivation to generate global wealth per se.\n\n### 4.2. If other motivations point towards good outcomes\n\nOne way in which it wouldn’t be necessary for most people in society to aim at the good de dicto is if intrinsically valuable goods are also _instrumentally_ valuable on a wide range of other moral views. For example, perhaps some of the following goods are intrinsically valuable: the acquisition of knowledge; the proliferation of life; the proliferation of complex systems; feats of engineering; preference-satisfaction.\n\nWhatever goals people have in the future, their plans are likely to involve at least some of acquiring knowledge, proliferating life and complex systems, achieving feats of engineering, and satisfying their own preferences and the preferences of those beings they create. So if at least some of these things are good in and of themselves, then the future will be better, even if future people are creating those goods only as a means to some other end. \n\nHowever, the question is whether this is sufficient to get us to a mostly-great future. And, again, that looks unlikely. People who only care about some goods for instrumental reasons are likely to produce less of them compared to people who intrinsically care about accumulating those goods. As an analogy, if Alice intrinsically cares about owning as many books as possible, and Bob values books because he finds it useful to be well-read, then Alice is likely to end up owning far more books (including books she will never read) than Bob ever will. The situation becomes worse again if the correct moral view intrinsically values some non-instrumentally valuable good _even more_ than it values the instrumentally valuable goods, or if the correct moral view intrinsically _disvalues_ some instrumentally valuable goods.\n\nThe intrinsic value of some instrumental goods might be a reason for thinking that the future is very good in absolute terms; but, given easy eutopia, the correlation between instrumental and intrinsic value just doesn’t seem strong enough to get us to a mostly-great future.\n\n5\\. Which scenarios are highest-stakes?\n---------------------------------------\n\nIn response to the arguments we’ve given in this essay, and especially the reasons for pessimism about convergence we canvassed in section 2, you might wonder if the practical upshot is that you should pursue personal power-seeking. If a mostly-great future is a narrow target, and you don’t expect other people to AM-converge, then you lose out on most possible value unless the future ends up aligned with almost exactly your values. And, so the thought goes, the only way to ensure that happens is to increase your own power by as much as possible.\n\nHowever, we don’t think that this is the main upshot. Consider these three scenarios:\n\n1.  Even given good conditions, there’s almost no AM-convergence between any sorts of beings with different preferences.\n    \n2.  Given good conditions, humans generally AM-converge on each other; aliens and AIs generally don’t AM-converge with humans.\n    \n3.  Given good conditions, there’s broad convergence, where at least a reasonably high fraction of humans and aliens and AIs would AM-converge with each other.\n    \n\n(There are also variants of (2), where “humans” could be replaced with “people sufficiently similar to me”, “co-nationals”, “followers of the same religion”, “followers of the same moral worldview” and so on.)\n\nThough (2) is a commonly held position, we think our discussion has made it less plausible. If a mostly-great future is a very narrow target, then shared human preferences are underpowered for the task of ensuring that the idealising process of different humans goes to the same place. What would be needed is for there to be something about the world itself that would pull different beings towards the same (correct) moral views: for example, if the arguments are much stronger for the correct moral view than for other moral views, or if the value of experiences is present in the nature of experiences, such that by having a good experience one is thereby inclined to believe that that experience is good.[55](#user-content-fn-55)\n\nSo we think that the more likely scenarios are (1) and (3). If we were in scenario (1) for sure, then we would have an argument for personal power-seeking (although there are plausibly other arguments against power-seeking strategies; this is discussed in section 4.2 of the essay, _What to do to Promote Better Futures_). But we think that we should act much more on the assumption that we live in scenario (3), for two reasons.\n\nFirst, the best actions are higher-impact in scenario (3) than in scenario (1). Suppose that you’re in scenario (1), that you currently have 1 billionth of all global power,[56](#user-content-fn-56) and that the future is on track to achieve one hundred millionth as much value as if you had all the power.[57](#user-content-fn-57) Perhaps via successful power-seeking throughout the course of your life, you could increase your current level of power a hundredfold. If so, then you would ensure that the future has one millionth as much value as if you had all the power. You’ve increased the value of the future by one part in a million.\n\nBut now suppose that we’re in scenario (3). If so, you should be much more optimistic about the value of the future. Suppose you think, conditional on scenario (3), that the chance of _Surviving_ is 80%, and that _Flourishing_ is 10%. By devoting your life to the issue, can you increase the chance of _Surviving_ by more than one part in a hundred thousand, or improve _Flourishing_ by more than one part in a million? It seems to me that you can, and, if so, then the best actions (which are non-powerseeking) have more impact in scenario (3) than power-seeking does in scenario (1). More generally, the future has a _lot_ more value in scenario (3) than in scenario (1), and one can often make a meaningful proportional difference to future value. So, unless you’re able to _enormously_ multiply your personal power, then you’ll be able to take higher-impact actions in scenario (3) than in scenario (1).\n\nA second, and much more debatable, reason for focusing more on scenario (3) is that you might just care about what happens in scenario (3) more than in scenario (1). Will’s preferences, at least, are such that things are much lower-stakes _in general_ in scenario (1) than they are in scenario (3): he thinks he’s much more likely to have strong cosmic-scale reflective preferences in scenario (3), and much more likely to have reflective preferences that are scope-sensitive and closer to contemporary common-sense in scenario (1).\n\n6\\. Conclusion\n--------------\n\nThis essay has covered a lot of different considerations, and it’s hard to hold them all in mind at once. Remember, we started the essay on the assumption that mostly-great futures are a very narrow target. On the face of it, that suggests pessimism about whether the future will be mostly-great. Overall, we think the considerations in this essay warrant significantly more optimism than that initial impression.\n\nStill, we don’t think the points in this essay establish that a mostly-great future is likely, even in the absence of blockers. Given moral antirealism, the common core of “human values” doesn’t seem specific or well-powered enough to make WAM-convergence look likely. And even if there is some attractive force towards the best moral views, like moral realism would suggest, still it might not be sufficiently motivating. And even if there would be WAM-convergence under good enough conditions, we might not reach those conditions. So widespread, accurate, and motivational convergence looks unlikely.\n\nThe potential of scenarios involving partial AM-convergence, plus trade or compromise, seems more realistic and promising to us. But there are major obstacles along that path too, including concentration of power and poor collective decision-making processes. Moreover, value-destroying threats could rob these scenarios of most of their value, or even render them worse than extinction.\n\nBacking up even further, on the basis of all this discussion, we think that the hypothesis from [the first essay](https://www.forethought.org/research/introducing-better-futures#1-the-basic-case) — that we are far from the ceiling on _Flourishing_ — seems right. Among problems we might consider, those that hold the future back from flourishing therefore seem greater in scale than risks to our survival. But is there anything we can do about them? In the next [two](https://www.forethought.org/research/persistent-path-dependence) [essays](https://www.forethought.org/research/how-to-make-the-future-better), we turn to that question.\n\nBibliography\n------------\n\nTobias Baumann, ‘[Using surrogate goals to deflect threats](https://longtermrisk.org/using-surrogate-goals-deflect-threats/)’, _Center on Long-Term Risk_, 20 February 2018.\n\nRichard Boyd, ‘[How to Be a Moral Realist](https://philpapers.org/rec/BOYHTB)’, _Essays on moral realism_, 1988.\n\nPaul Christiano, ‘[Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)’, _Rational Altruist_, 27 February 2013.\n\nFrancis Fukuyama, ‘[The End of History and the Last Man](https://www.penguin.co.uk/books/13399/the-end-of-history-and-the-last-man-by-fukuyama-francis/9780241991039)’, 17 September 2020.\n\nJ. Michael Harrison and David M. Kreps, [‘Speculative Investor Behavior in a Stock Market with Heterogeneous Expectations’](https://econpapers.repec.org/article/oupqjecon/v_3a92_3ay_3a1978_3ai_3a2_3ap_3a323-336..htm), _The Quarterly Journal of Economics_, 1 May 1978.\n\nJoshua Conrad Jackson and Danila Medvedev, [‘Worldwide divergence of values’](https://www.nature.com/articles/s41467-024-46581-5), _Nature Communications_, 9 April 2024.\n\nToby Ord, ‘Moral Trade’, _Ethics_, 2015.\n\nMichael Smith, David Lewis, and Mark Johnston, [‘Dispositional Theories of Value\\*’](https://www.princeton.edu/~msmith/mypapers/Smith-Dispositional-1989.pdf), _Aristotelian Society Supplementary Volume_, 1 July 1989.\n\nBernard Williams, ‘[Persons, character and morality](https://www.cambridge.org/core/books/moral-luck/persons-character-and-morality/B3096FF4B0780A9A58D019EE3A968678)’, _Moral Luck: Philosophical Papers 1973–1980_, 1981.\n\nWorld Values Survey, ‘[Inglehart–Welzel Cultural Map](https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=Findings)’, 2023.\\*\\*\n\n\nReleased on 3rd August 2025"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 0
      }
    ],
    "stats": {
      "successes": 9,
      "failures": 0,
      "errors": 0,
      "tokenUsage": {
        "prompt": 175113,
        "completion": 23558,
        "cached": 0,
        "total": 198671,
        "numRequests": 9,
        "completionDetails": {
          "reasoning": 0,
          "acceptedPrediction": 0,
          "rejectedPrediction": 0
        },
        "assertions": {
          "total": 38744,
          "prompt": 34679,
          "completion": 4065,
          "cached": 0,
          "numRequests": 0,
          "completionDetails": {
            "reasoning": 0,
            "acceptedPrediction": 0,
            "rejectedPrediction": 0
          }
        }
      }
    }
  },
  "config": {
    "description": "Critique prompt comparison experiment",
    "prompts": [
      {
        "id": "claude-surgery",
        "label": "Claude: Argument Surgery",
        "raw": "file://prompts/claude-surgery.md"
      },
      {
        "id": "claude-personas",
        "label": "Claude: Hostile Personas",
        "raw": "file://prompts/claude-personas.md"
      },
      {
        "id": "claude-unforgettable",
        "label": "Claude: Unforgettable Objection",
        "raw": "file://prompts/claude-unforgettable.md"
      },
      {
        "id": "gpt-surgery",
        "label": "GPT: Argument Surgery",
        "raw": "file://prompts/gpt-surgery.md"
      },
      {
        "id": "gpt-personas",
        "label": "GPT: Hostile Personas",
        "raw": "file://prompts/gpt-personas.md"
      },
      {
        "id": "gpt-unforgettable",
        "label": "GPT: Unforgettable Objection",
        "raw": "file://prompts/gpt-unforgettable.md"
      },
      {
        "id": "gemini-surgery",
        "label": "Gemini: Argument Surgery",
        "raw": "file://prompts/gemini-surgery.md"
      },
      {
        "id": "gemini-personas",
        "label": "Gemini: Hostile Personas",
        "raw": "file://prompts/gemini-personas.md"
      },
      {
        "id": "gemini-unforgettable",
        "label": "Gemini: Unforgettable Objection",
        "raw": "file://prompts/gemini-unforgettable.md"
      }
    ],
    "providers": [
      {
        "id": "anthropic:messages:claude-opus-4-5-20251101",
        "label": "Claude Opus 4.5",
        "config": {
          "temperature": 0.7,
          "max_tokens": 4000
        }
      }
    ],
    "tests": [
      {
        "vars": {
          "paper": "file://paper.md"
        },
        "assert": [
          {
            "type": "llm-rubric",
            "provider": "anthropic:messages:claude-opus-4-5-20251101",
            "value": "file://grader-critique-collection.txt"
          }
        ]
      }
    ],
    "sharing": true,
    "outputPath": "./output/results.json",
    "extensions": []
  },
  "shareableUrl": null,
  "metadata": {
    "promptfooVersion": "0.118.2",
    "nodeVersion": "v22.15.0",
    "platform": "darwin",
    "arch": "arm64",
    "exportedAt": "2026-01-22T13:01:32.094Z",
    "evaluationCreatedAt": "2026-01-22T12:57:59.759Z"
  }
}