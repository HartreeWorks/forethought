1. "The Multiplicative Model Smuggles In What It Purports to Discover" attacks the paper's core inference that value is well-modeled as a product of independent factors (Section 2.4). The authors present the multiplicative model as a *discovery* about why eutopia is fragile, but the model itself presupposes that single dimensions can zero out overall value—which is precisely the "no easy eutopia" conclusion. A model where value is the *weighted average* of factors (or where factors have diminishing marginal returns at low scores) would generate the opposite conclusion from identical inputs. The paper provides no principled argument for why multiplication rather than averaging better captures moral reality; it simply observes that multiplication produces the result they want. If this objection holds, the paper needs to justify why multiplicative aggregation is the correct functional form—not merely illustrate that it generates fragility.

2. "The Fat-Tailed Value Distribution is Self-Refuting" targets the claim that the distribution of value-per-resource is fat-tailed, making near-optimal resource use astronomically unlikely (Section 3.2). The paper invokes fat-tailed distributions in citations, subjective experience, and economic value as evidence. But the very same fat-tailed logic applies to *our probability distribution over moral theories*: a fat-tailed distribution over theories means we should expect the true theory to be one we haven't even conceived of yet, and that theory could easily be easygoing. The paper's own reasoning about expecting extremes undermines confidence in any particular fussy theory. If the authors accept fat tails generally, they cannot assume the true theory resembles any theory they've analyzed. The paper would need to explain why fat-tailed logic applies to value distributions but not to moral uncertainty itself.

3. "The Bounded-Asymmetric Domino Problem" challenges the paper's treatment of bounded views that aggregate goods and bads separately (Section 3.3). The paper argues such views are fussy because even one part in 10²² of bads destroys eutopia. But this proves too much: if any non-zero bads prevent a mostly-great future, then *every physically possible future* fails to be mostly-great, since thermodynamic constraints guarantee some suffering somewhere. The paper's criterion doesn't distinguish between achievable futures at all—it makes the entire category of "mostly-great futures" empty by physical necessity. This isn't fussiness; it's vacuity. If the objection holds, the paper must either abandon separate aggregation views or provide a threshold for bads that's achievable rather than mathematically zero.

4. "The Navigation Metaphor Undermines the Argument" exposes a tension between the paper's framing and its conclusion. The authors use the sailing-to-an-island metaphor to set up three factors: island size, navigation, and multiple attempts. They announce they'll address only factor (1), the island's size. But the entire Section 2.3 list of "future catastrophes" is actually about failures of *navigation*—wrong population ethics, wrong wellbeing theory, wrong discount rates. These aren't features of the target; they're ways the crew could aim wrong. The paper's evidence for a narrow target is actually evidence that navigation is hard, which they explicitly bracket. If this confusion is corrected, the paper has much less to say about target size than it claims, and the sequel essay on convergence must bear far more argumentative weight.

5. "The Common-Sense Utopia Benchmark Is Gerrymandered" attacks the paper's choice of reference point for evaluating fussiness. The paper defines common-sense utopia (Section 2.2) as 100 billion happy people with freedom, diversity, and minimal suffering—then argues most views wouldn't count this as mostly-great. But this benchmark is carefully selected to fail on *scale-sensitive* views (only 100 billion people, only possible future civilization) while passing on *easygoing liberal* views. A different "common-sense utopia" featuring a trillion galaxies of moderately good lives would reverse which views are fussy. The paper's conclusion that "most views are fussy" depends entirely on benchmarking against a deliberately small-scale scenario. The paper needs to justify why this particular benchmark reveals something general rather than being constructed to produce the desired result.

6. "The Hedonism-Treadmill Analogy Cuts Both Ways" undermines the psychological debunking argument in Section 2.5. The paper argues that hedonic adaptation explains why eutopia always seems just out of reach—our expectations reset, so we're biased toward thinking the target is close when it isn't. But the same psychological mechanism generates the opposite prediction: after extensive reflection, each newly discovered "flaw" would feel maximally important precisely because it's the current frontier, while previously discovered flaws would fade in apparent significance. Hedonic treadmills don't systematically bias us toward *under*estimating distance to targets; they bias us toward treating current position as neutral. If anything, this suggests we systematically *overweight* the most recently discovered moral concern. The debunking argument could support either conclusion, so it supports neither.

7. "The Intertheoretic Comparison Shell Game" attacks the moral uncertainty analysis in Section 3.5. The paper walks through multiple normalization methods, shows they yield different recommendations, then tentatively endorses the "pairwise" approach that makes unbounded views loom largest. But the pairwise approach requires direct judgments about how views compare—for instance, that utilitarianism and strict negative utilitarianism "agree on the disvalue of bads." This is false: utilitarians think 1 unit of suffering trades against 1 unit of pleasure, while strict negative utilitarians think no amount of pleasure compensates any suffering. The "agreement" is fabricated by stipulating the comparison the authors want. The paper's conclusion that "unbounded views should effectively loom larger" rests entirely on this manufactured intertheoretic bridge.

8. "The Resource-Control Assumption Begs the Question" challenges the inference that linear unbounded views require controlling "most accessible resources" for a mostly-great future. The paper assumes that resources not under human control contribute nothing (or random value) to overall goodness. But many moral views—especially those concerned with naturalness, wild animal welfare, or the intrinsic value of unmanaged ecosystems—would evaluate resources *better* when left alone. On such views, humanity controlling all 10²² stars would be a moral catastrophe, not a requirement for eutopia. The paper treats "resources under civilization's control" as a proxy for "resources contributing to value," but this is precisely what's contested. If control isn't instrumentally necessary for value on many views, the linear-views-require-scale argument collapses.

9. "The Extinction Stipulation Is Doing Hidden Work" attacks the paper's definitional choice that extinction has value 0 (Section 3.1). This stipulation makes extinction better than any negative-value future and worse than any positive-value future—a substantive ethical claim, not a harmless normalization. Many views (Buddhist, antinatalist, some environmentalist) would assign extinction positive value; many religious views would assign it negative value. By stipulating extinction = 0, the paper excludes views where humanity's absence is intrinsically good, which are precisely the views most likely to be easygoing about futures that don't maximize human-controlled value. The entire fussiness analysis is conditional on a particular ranking of extinction that already excludes the most plausible easygoing alternatives.

10. "The 90%/50% Thresholds Are Arbitrarily Punitive" targets the paper's definitions of eutopia (>90% of best feasible value) and mostly-great (>50%). The paper acknowledges "there's nothing magical about the mostly-great threshold" but then builds its entire argument around showing most views make these thresholds hard to reach. But at any non-arbitrary threshold, *some* views will make it easy and others hard—that's what having a threshold means. The paper's conclusions are artifacts of threshold choice: at 10%, most views would be easygoing; at 99.9%, virtually all views would be fussy. The interesting empirical question—what fraction of futures most views would endorse as "good enough"—cannot be answered by stipulating numeric cutoffs. If the thresholds were derived from something principled (decision-relevant indifference points, revealed preferences under uncertainty), this objection dissolves, but no such derivation appears.