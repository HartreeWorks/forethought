Suppose the paper's framework succeeds and humanity develops institutions for coordinating on "correct" moral views about digital welfare, population ethics, and resource allocation. The very existence of such coordination capacity creates a new catastrophic risk the paper doesn't consider: a mechanism powerful enough to steer civilization toward one target can be captured to steer it toward another. If the paper is right that a narrow range of futures contain most possible value, then the same apparatus that could navigate to eutopia could navigate to something arbitrarily bad—and the actors who control that apparatus face overwhelming temptation to define "correct" views as whatever serves their interests. The historical pattern with technologies of social control—from mass media to surveillance to algorithmic recommendation—is that they're first justified as tools for beneficial coordination and then captured for extraction and domination. The paper's entire project of specifying what a "mostly-great future" requires is therefore not neutral analysis but a roadmap for value lock-in by whoever controls the deliberative processes it envisions.