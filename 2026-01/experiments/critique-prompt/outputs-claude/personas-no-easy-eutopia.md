1. [The Empirical Hardliner] The paper's central claim that "mostly-great futures" require achieving at least 50% of the value difference between extinction and eutopia rests entirely on stipulated definitions without any identified causal mechanism for why value would actually aggregate multiplicatively across independent dimensions. The authors assert that factors like population ethics, digital welfare, and resource allocation combine as products rather than sums, but provide no empirical evidence that moral value actually compounds this way rather than adding linearly or following some other functional form. Without a falsifiable prediction about how we could detect whether value is multiplicative versus additive in real-world moral assessments, this is metaphysical speculation dressed as analysis. The concrete consequence is that the entire "narrow target" conclusion could collapse if value aggregates differently—for instance, if doing well on 4 of 5 dimensions yields 80% of maximum value rather than the 6% their multiplicative model implies.

2. [The Game-Theoretic Defector] The paper treats future resource allocation decisions as if they will be made by unified agents pursuing coherent value functions, but ignores that any actual space settlement regime will involve strategic actors who can defect from cooperative agreements. When the authors discuss "initial periods of settlement and resource appropriation," they fail to recognize that first-mover advantages in capturing extrasolar resources create overwhelming incentives for racing behavior—whoever moves fastest gains near-permanent control, regardless of what allocation system society ostensibly endorses. The "compromise" mechanisms proposed in the companion essay cannot bind actors who can unilaterally claim resources before negotiations conclude. This means the paper's framing of moral catastrophe as stemming from "wrong views" misdiagnoses the problem: even if everyone holds correct views about ideal allocation, the game-theoretic structure ensures defection, making their entire framework of convergence toward correct moral views decision-theoretically naive.

3. [The Institutional Corruptionist] The paper assumes that if "some people have the right view and compromise between them and the rest of society is sufficient," this could steer toward eutopia—but this ignores how deliberative processes are systematically captured by concentrated interests. The authors acknowledge that resource allocation during space settlement could "end up concentrated among a tiny number of hands," yet treat this as merely one possible moral error rather than the predictable outcome of any institutional arrangement. Historical precedent from colonial resource extraction, spectrum auctions, and carbon markets shows that nominally neutral allocation mechanisms are reliably captured by whoever has the most resources at the time of implementation. The paper's entire framework of "convergence and compromise" as a path to eutopia therefore rests on institutional assumptions that have never held in practice when stakes were high—the very actors best positioned to influence "truth-seeking deliberative processes" are those with the most to gain from non-eutopian outcomes.

4. [The Historical Parallelist] The paper's claim that humanity could "squander" its opportunity for a near-best future mirrors the exact reasoning structure of 19th-century utilitarian reformers who believed that material abundance plus correct moral education would automatically produce optimal social arrangements. The Benthamite program assumed that once scarcity was overcome and correct views disseminated, society would converge on happiness-maximizing institutions—yet industrial abundance actually enabled new forms of exploitation, alienation, and warfare that pre-industrial reformers couldn't anticipate. Similarly, the Soviet experiment assumed that eliminating capitalist property relations would allow humanity to achieve a near-best future through correct social organization, yet produced outcomes that most moral frameworks would classify as catastrophic. The paper provides no account of why their framework of "moral convergence plus compromise" would succeed where structurally identical reasoning repeatedly failed, suggesting their confidence in deliberative solutions reflects philosophical naivety about how large-scale social optimization actually unfolds.

5. [The Complexity Theorist] The paper's multiplicative model of value assumes that factors like "population ethics," "digital welfare," and "attitudes to wellbeing" are independent dimensions that can be scored separately and then combined. But in any actual future civilization, these factors will interact in complex, nonlinear ways that make the product decomposition meaningless. Attitudes toward digital beings will shape population ethics through replacement fertility effects; wellbeing theories will determine which beings count in population calculus; resource allocation will constrain which wellbeing theories are even implementable. The authors treat these as separate "mistakes" that each independently erase value, but emergent properties of complex systems mean that getting one factor "right" may be impossible without simultaneously addressing others—or that apparent solutions on one dimension create cascading failures elsewhere. Their framework cannot model path dependencies, feedback loops, or phase transitions in social values, making their probability estimates over "mostly-great futures" systematically overconfident about the decomposability of the target.

6. [The Cognitive Scientist] The paper claims we should "hope for a 60–40 gamble between eutopia and extinction rather than a guarantee of many futures that intuitively seem truly wonderful," but treats the resulting intuitive resistance as something to be explained away rather than as evidence against the framework. The authors invoke "hedonic treadmill" effects to explain why eutopia "seems" achievable when it isn't, but this same reasoning undermines their entire methodology: if human moral intuitions systematically misjudge value comparisons at cosmic scales, then the von Neumann-Morgenstern axioms they use to generate cardinal value functions are calibrated on cognitively distorted inputs. The paper acknowledges that scope insensitivity affects how we perceive future value, yet proceeds to build a formal apparatus on scope-sensitive comparisons between galaxies and solar systems. This is incoherent—either our value intuitions are reliable enough to anchor the formal framework, or they're systematically biased in ways that invalidate the framework's conclusions about what counts as "mostly-great."

7. [The Moral Parliament Dissenter] The paper assumes that moral views can be meaningfully aggregated through von Neumann-Morgenstern utility representations, but this smuggles in consequentialist assumptions that many moral frameworks explicitly reject. Deontological views that treat certain actions as absolutely prohibited cannot be captured by any continuous value function over outcomes—the wrongness of torturing one person isn't diminished by any amount of happiness elsewhere in the distribution. The authors claim their framework doesn't "restrict us to consequentialist views," but their entire apparatus of "quantities of value" defined by willingness to accept gambles presupposes that outcomes can be traded off against each other at some rate. When they discuss "moral uncertainty" between bounded and unbounded views, they're not actually comparing moral theories—they're comparing consequentialist cousins while excluding the deontological and virtue-theoretic frameworks that would reject the gamble-based methodology entirely. The paper's conclusion that "most plausible moral views are fussy" is therefore circular: they've defined "plausible" to exclude views incompatible with their aggregation procedure.

8. [The Capability Accelerationist] The paper treats the question of whether humanity reaches eutopia as if it depends on our collective moral choices, but completely ignores that the relevant capabilities for shaping long-term futures are being developed competitively by multiple actors with incompatible goals. If the authors' own framework is correct that "initial periods of settlement and resource appropriation" will determine cosmic-scale outcomes, then the key variable is who develops transformative AI and space settlement technology first—not what moral framework they endorse. Safety-oriented actors who spend resources on moral deliberation will be systematically outcompeted by actors focused purely on capability development. The paper's entire premise that we can choose between "safety-focused" and "upside-focused" options assumes a coordination capacity that doesn't exist; in reality, the actors who shape the future will be selected for speed rather than correctness. The "no easy eutopia" conclusion is therefore true but for the wrong reasons: it's not that the target is narrow, but that the selection pressure on who gets to aim is orthogonal to whether they're aiming well.

9. [The Resource Economist] The paper's cost-benefit comparisons between different futures assume that "resources in the accessible universe" can be meaningfully allocated toward arbitrary ends, but never specifies what conversion rate connects physical resources to moral outcomes. The authors claim that "the distribution of value/cost, over likely uses of those resources, is probably sufficiently fat-tailed" based on analogies to consumer goods and health interventions, but these domains have well-defined production functions that don't exist for their proposed moral goods. Converting a star system's worth of matter into "digital welfare" versus "flourishing biological lives" versus "aesthetic experiences" requires technologies that don't exist and may have wildly different efficiency curves—or may be physically impossible. The entire framework of "value-efficient arrangements of resources" treats moral optimization as engineering problem with known constraints, when in fact we have no idea what the production possibility frontier for cosmic-scale wellbeing looks like. Their quantitative conclusions about how much value is "lost" to various moral errors are therefore not wrong but meaningless—denominated in units that cannot be measured or traded.

10. [The Second-Order Catastrophist] Suppose the paper's framework succeeds and humanity develops institutions for coordinating on "correct" moral views about digital welfare, population ethics, and resource allocation. The very existence of such coordination capacity creates a new catastrophic risk the paper doesn't consider: a mechanism powerful enough to steer civilization toward one target can be captured to steer it toward another. If the paper is right that a narrow range of futures contain most possible value, then the same apparatus that could navigate to eutopia could navigate to something arbitrarily bad—and the actors who control that apparatus face overwhelming temptation to define "correct" views as whatever serves their interests. The historical pattern with technologies of social control—from mass media to surveillance to algorithmic recommendation—is that they're first justified as tools for beneficial coordination and then captured for extraction and domination. The paper's entire project of specifying what a "mostly-great future" requires is therefore not neutral analysis but a roadmap for value lock-in by whoever controls the deliberative processes it envisions.