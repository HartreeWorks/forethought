> The flight analogy breaks down because aircraft design has clear, shared feedback loops (fly or crash), while "mostly-great future" lacks any comparable consensus error signal—so optimisation may just lock in proxy targets like status or power.

**Load-bearing claim:**
In **§1–§2** you lean on the analogy “*powered flight is ubiquitous, because human design honed in on the design target*,” then map “honing in” to future society deliberately reaching a “mostly-great future.”

**Attack type:**
Reference class sabotage

The analogy smuggles in a crucial property: flight has tight, externally validated feedback loops (lift/drag tests, crash vs fly) that are legible and shared across designers, whereas “mostly-great” is exactly what you later argue is *not* legible, not agreed, and often not motivational (“de dicto” is rare). A plausible world exists where optimization capability explodes while value-feedback remains fragmented/strategic, yielding intense honing-in on *proxy targets* (status, identity, power, specific hedonic modes) rather than on “good overall,” so the flight analogy predicts the wrong direction. If this holds, you’d need to replace “honing” with a mechanism that generates *shared, incentive-compatible error signals about moral mistakes* (or explicitly weaken the conclusion to: optimization increases extremity, not “mostly-greatness”).
