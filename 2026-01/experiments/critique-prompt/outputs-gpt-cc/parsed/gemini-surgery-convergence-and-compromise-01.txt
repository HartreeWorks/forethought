The paper’s opening “honing in” move (flight/natural selection analogy) is load-bearing for the claim that a narrow target (“mostly-great futures”) need not imply near-zero expected flourishing, because optimization can find narrow sets. The hidden lemma is that whatever does the optimizing in a post‑AGI world is actually optimizing for “mostly-greatness” rather than for a proxy (power, stability, legibility, prestige) that is correlated with flourishing only in the low-optimization regime. A counter-model consistent with the paper’s own “no easy eutopia” framing is: post‑AGI institutions optimize extremely hard for regime stability and conflict-minimization (a narrow target too), which reliably yields vast “peaceful” resource usage (e.g., sedation/experience-suppression, extreme surveillance, value-homogenization) that is robustly stable yet far from mostly-great on almost any view that values diversity, autonomy, or rich experiences. The argument’s metal/wing analogies then backfire: optimization finds *a* narrow target, just not the one you want. If this critique holds, the paper must replace the “optimization can hit narrow targets” reassurance with a theorem-like condition tying the optimizing objective (institutional selection pressures, AI delegates’ reward functions, governance metrics) to flourishing rather than to stability/advantage proxies.