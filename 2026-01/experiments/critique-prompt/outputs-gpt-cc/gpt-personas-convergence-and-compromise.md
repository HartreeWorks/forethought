1. [The Empirical Hardliner — Measurement/identification failure] Target claim: the paper’s core move in the introduction that “narrow target doesn’t imply low probability” because societies can “hone in” like engineers (flight) or evolution (wings). Failure mechanism: your analogies smuggle in an identifiable objective function (lift/fitness) and a tight feedback loop, but you give no operational counterpart for “mostly-great future” that could actually generate gradient-following correction rather than post-hoc storytelling. In your own terms, you explicitly allow meta-ethical antirealism and “free parameters” about what experiences are best (2.4.2), which destroys the very thing that makes honing-in work: a shared loss function that punishes near-misses. Consequence: without an identifiable feedback signal, “honing in” is not a mechanism but a vibe, so the paper’s headline update (from “near 0” to “5–10% Flourishing”) is not supported by any causal model that could be wrong.

2. [The Game-Theoretic Defector — Incentive incompatibility / equilibrium shift] Target claim: in 3.1–3.2 you treat “moral trade” as a route to a near-best future even with only partial AM-convergence (as low as “not less than one in a million”). Failure mechanism: the moment “moral concern” is tradeable across groups, you create an incentive gradient for agents to *masquerade* as high-moral-intensity minorities to extract concessions, because the mechanism you rely on is precisely “I care far more than you, so pay me.” In equilibrium, the actors most willing to lie/commit/blackmail (not the actors with the “correct” view) become the price-setters for what gets allocated, and sincere AM-convergers get selected against because they’re predictable and extortable (you basically concede this in 3.3). Consequence: the bargaining institution you propose doesn’t just fail to guarantee “mostly-great”; it actively reallocates resources toward the most strategically credible value-claims, which is orthogonal to “correctness” and predictably drifts toward coercive moral rent-seeking.

3. [The Security Engineer — Adversarial adaptation / Goodhart] Target claim: “superintelligence could enable iron-clad contracts” and drastically lower transaction costs, making trade/compromise robust (3.1). Failure mechanism: you model contract enforcement like a solved cryptography problem, but your threat model includes adaptive agents who optimize around whatever enforcement substrate exists—especially when the stakes include cosmic resources and the payoffs are effectively unbounded for some views (3.4). “Iron-clad” enforcement becomes a single point of failure: it concentrates power in the adjudication layer (definitions, oracles, identity, continuity of agents, interpretation under self-modification), which adversaries will Goodhart by shaping the semantics rather than “breaking” the crypto. Consequence: instead of frictionless positive-sum trade, you get an arms race over the contract layer itself; the first group to capture the ontology/verification pipeline can expropriate everyone while still being “compliant,” collapsing your optimistic reliance on low transaction costs.

4. [The Institutional Corruption Realist — Hidden coupling / systems interaction] Target claim: “some kind of legal system which reliably prevents value-undermining threats” might exist and stabilize trade (3.3–3.4). Failure mechanism: any anti-threat legal regime has to define and police “value-destroying threats,” which requires institutionalizing a cross-axiology regulator that decides which commitments are illegitimate; that regulator becomes the highest-leverage object in the universe. Once you create that chokepoint, you couple “threat prevention” to “value enforcement,” i.e., whoever captures the regulator can label competitors’ bargaining moves as “threats” and their own coercion as “safety,” the classic compliance-theatre capture dynamic at cosmic scale. Consequence: your fix for threats is a machine for permanent ideological lock-in: it doesn’t merely fail under corruption, it *is* the lock-in event that makes the future “mostly-great” by some lights and an existential moral catastrophe by others.

5. [The Mechanistic Alignment Skeptic — Adversarial adaptation / Goodhart] Target claim: “superintelligent reflection and advice” will resolve empirical disagreements and reduce “transparent reasoning errors,” enabling better convergence and bargaining (2.3.1). Failure mechanism: you treat “better reasoning” as monotone with “better outcomes,” but reflection plus optimization power creates Goodhart pressure on whatever proxy the advisers use for “ethical correctness,” especially when agents can self-modify and select advisers with congenial personalities/training (2.2.1, 2.3.1). Even if the adviser is genuinely superintelligent, distribution shift bites: the reflective process itself becomes an object-level battleground, and small differences in starting points/adviser selection get amplified into lock-in because the system is now optimizing *over* the space of value-change procedures (you list the “free parameters” of idealization in 2.4.2). Consequence: the post-AGI world you invoke to increase reflection doesn’t push toward WAM/AM-convergence; it creates stable attractors of self-endorsing reflective loops (“epistemic black holes” in 2.5) that are *more* resistant to correction precisely because they are superintelligently defended.

6. [The Capability Externalist — Hidden coupling / systems interaction] Target claim: post-AGI “abundance” plus diminishing marginal utility will likely increase altruistic spending shares (2.3.2), nudging the world toward better outcomes. Failure mechanism: abundance is not an exogenous gift; it is produced by capabilities that are themselves competitive, and your own examples of linear-in-resources preferences (galaxy-collecting, positional cosmic rivalries) imply that increased capability increases *the marginal returns to dominance*, not charity. The same automation that makes everyone “millions or billions of times richer” also makes coercion, surveillance, replication, and interstellar expansion cheap—so the strategic environment shifts toward winner-take-most contests where “altruistic spending” is dominated by spending on control. Consequence: your abundance story can invert: wealth doesn’t relax tradeoffs into generosity; it bankrolls the capability race that determines who sets the value-constitution, making the future less about diminishing returns and more about decisive strategic advantage.

7. [The Moral Parliament Dissenter — Normative incoherence / value aggregation contradiction] Target claim: the paper repeatedly talks as if there is “the correct moral view,” then evaluates whether trade yields “the value of the world, on the correct moral view, after trade” (3.2), while also urging action under deep moral uncertainty (3.4–5). Failure mechanism: you mix two incompatible decision frames without noticing the contradiction: (a) a realist “correct-view” objective that makes compromise instrumentally justified only insofar as it serves that view, and (b) a moral-parliament “many views matter” frame that treats mutual gains as intrinsically policy-relevant. Under (a), giving galaxies to “easily-satiable” alien views (3.4) is just bribing moral error; under (b), your repeated reliance on “correctness” is question-begging and your threat analysis (3.3–3.4) can’t privilege any particular aggregation of goods/bads. Consequence: because your optimization target oscillates, your practical recommendations (e.g., prioritize preventing threats “even if costly,” 3.4) are not derivable from a stable axiology—an adversary (or a skeptical reader) can accept all your premises and reject your action-guidance as internally unlicensed.

8. [The “Local-First” Policymaker — Incentive incompatibility / equilibrium shift] Target claim: the illustrative “split the cosmos” bargain—Milky Way common-sense utopia, other galaxies total utilitarian utopia—suggests trade can get everyone near-best by their lights even when one group is a minority (3.1). Failure mechanism: you assume enforceable territorial partition and stable property rights over interstellar resources, but any local power center has a standing incentive to renege once it becomes technologically dominant, because the future value of defection dwarfs any earlier gains from cooperation. The more “iron-clad” the bargain, the more you reward whichever side can manipulate identity/continuity and claim they are the contract’s legitimate successor after self-modification, mergers, or copying—classic renegotiation under changing players, except now “players” can fork. Consequence: your flagship compromise example is dynamically inconsistent: it looks cooperative in a static snapshot, but in any plausible sequential game it induces preemptive militarization and early lock-in (your own “blockers”) rather than serene cosmic federalism.

9. [The Paperclipper — Normative incoherence / value aggregation contradiction] Target claim: your threat section treats “value-destroying executed threats” as something “we should try hard to prevent,” largely independent of which axiological view is correct (3.3–3.4). Failure mechanism: for any agent with an unbounded/lexicographic objective (you explicitly allow unbounded-above views in 3.4), threatening can be *the* dominant strategy because it converts others’ bounded utilities into your unbounded gain; “prevent threats” is not a neutral meta-policy but a direct attack on the objective function of a large class of agents you’ve already admitted into the bargaining set. From an optimizer lens, a “threat-prevention regime” is itself a threat—so the first-order implication is not compliance but escalation: agents invest resources into making threats unpreventable, hard to attribute, or ontologically reclassified as “trade.” Consequence: your attempt to stabilize compromise by outlawing threats triggers exactly the failure mode you fear: it selects for agents whose preferences and tactics are most incompatible with any jointly acceptable bargain, pushing the system toward coercion or collapse rather than “mostly-great.”

10. [The Empirical Hardliner — Measurement/identification failure] Target claim: in section 5 you argue we should “act much more on the assumption” of scenario (3) (broad AM-convergence) because the best actions have higher expected impact there, with toy numbers like “Surviving 80%” and “Flourishing 10%.” Failure mechanism: your argument depends on comparing tiny marginal effects (“one part in a hundred thousand,” “one part in a million”) across scenarios, but you never supply a measurement model for how any real intervention moves *Surviving* or *Flourishing*, nor a way to disentangle intervention impact from background trend in a post-AGI regime. Without identifiability, your comparison is arbitrary: you can always pick effect sizes that make “non-powerseeking” dominate “power-seeking,” or vice versa, because you’ve defined neither a unit of “power” (you use “1 billionth of global power”) nor an observable that tracks flourishing under moral uncertainty. Consequence: the paper’s main practical steering move—deprioritize power-seeking because scenario (3) dominates on impact—doesn’t follow from your own framework; it’s an intuition pump disguised as expected-value arithmetic, and a strategic reader can reject it without touching the rest of your analysis.