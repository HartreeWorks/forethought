1. In **§1–§2** you lean on the analogy “*powered flight is ubiquitous, because human design honed in on the design target*,” then map “honing in” to future society deliberately reaching a “mostly-great future.” **Attack type: Reference class sabotage.** The analogy smuggles in a crucial property: flight has tight, externally validated feedback loops (lift/drag tests, crash vs fly) that are legible and shared across designers, whereas “mostly-great” is exactly what you later argue is *not* legible, not agreed, and often not motivational (“de dicto” is rare). A plausible world exists where optimization capability explodes while value-feedback remains fragmented/strategic, yielding intense honing-in on *proxy targets* (status, identity, power, specific hedonic modes) rather than on “good overall,” so the flight analogy predicts the wrong direction. If this holds, you’d need to replace “honing” with a mechanism that generates *shared, incentive-compatible error signals about moral mistakes* (or explicitly weaken the conclusion to: optimization increases extremity, not “mostly-greatness”).  

2. In **§2** you state: “*We’ll here assume that, given WAM-convergence, we will reach a mostly-great future*,” and even suggest “*even a global dictatorship could have a good chance*” under WAM-convergence. **Attack type: Countermodel.** Consider a world where the dictator and key technologists sincerely converge on the “right” moral view and are motivated de dicto, but governance is *epistemically brittle* under concentration: a few mistaken institutional choices (metric selection, delegation to advisors, security doctrine) create irreversible path dependence that blocks key components of the “mostly-great” target despite aligned intent. This breaks your inference that WAM-convergence is sufficient; it treats “major blockers” as exogenous while dictatorship itself is a blocker generator (reduced error-correction, suppressed dissent, adversarial selection of information). If this holds, you must either (i) make WAM-convergence include robust institutional properties (pluralism, corrigibility, redundancy), or (ii) stop using WAM-convergence as a near-sufficient condition and track the conditional failure probability from governance structure.  

3. In **§2.2.1** you argue present agreement is mostly “low-hanging fruit” and will “*break down… as we max out on instrumentally valuable goods*,” with hedonism vs preference-satisfaction diverging under optimization. **Attack type: Hidden parameter (psychological/technological plasticity).** The divergence story assumes people will keep endorsing their *current* welfare theories under post-AGI choice, but the post-AGI setting you describe also allows direct editing of preferences, meta-preferences, and reflectivity—so the key parameter is whether agents choose to stabilize welfare-theory pluralism or *converge on a negotiated common currency* to reduce conflict/transaction costs. There’s a plausible world where, precisely because optimization makes divergence costly, actors voluntarily adopt “interoperability standards” for value (e.g., mutually legible preference representations, welfare-construction protocols), keeping agreement high even at the top of the list. If this holds, your step from “optimization power increases” to “moral agreement breaks down” fails unless you model the incentive to standardize/merge values. You’d need to add an explicit argument that value-standardization is infeasible or dominated by factional lock-in, rather than presuming divergence as the default.  

4. In **§2.3.2** you present an abundance-driven argument that weak altruistic preferences could dominate marginal spending because self-interested utility diminishes faster, then you rebut with billionaire philanthropy (~6% of income) as evidence against large shifts. **Attack type: Dominant alternative explanation.** The billionaire data can be explained by *institutional and reputational constraints* (tax strategy, signaling equilibria, limited high-trust channels, fear of political backlash), not by stable preference curvature; so it’s a poor proxy for what agents do when (as you assume) “transaction costs are extremely small” and “iron-clad contracts” exist. In a post-AGI world with cheap verification and low coordination cost, the same underlying preferences could yield much higher altruistic fractions without any change in curvature. If this holds, your use of today’s philanthropy to downweight the abundance-to-altruism channel is not load-bearing evidence; it’s confounded by missing institutions you elsewhere assume away. You’d need to either (i) justify why those constraints persist post-AGI, or (ii) drop the billionaire comparison and replace it with a model of altruistic allocation under your own post-AGI assumptions.  

5. In **§2.3.3 (“Long views win”)** you argue patient/altruistic agents may outgrow others via saving/fertility, but then note it may be too slow if “major decisions are made soon.” **Attack type: Quantitative cliff.** Your mechanism has a sharp threshold: if lock-in occurs before the compounding advantage accumulates, the effect is ~0; if after, it can dominate. Yet you treat it qualitatively and then pivot to trade in §3 without quantifying the timing condition that decides whether “long views win” matters at all. A plausible world has rapid, early constitutional lock-in of cosmic property rights (or AI-controlled enforcement) within decades of AGI, making demographic/wealth-selection irrelevant; another world has centuries of competitive expansion where it dominates. If this holds, your optimism/pessimism updates can swing by orders of magnitude based on an unmodeled timeline parameter. You’d need to introduce explicit timing scenarios (lock-in half-life, institutional mutability) and show how your conclusions change across them.  

6. In **§2.4.1 (realism)** you argue: if realism/internalism, people “*prefer not to learn facts that end up motivating them*,” and if externalism, they may learn but not act—either way WAM-convergence is unlikely. **Attack type: Reversal.** The same internalism premise can imply the opposite: if moral belief is intrinsically motivating, actors may *precommit to exposure* (or build institutions that force exposure) precisely to harness motivation as a self-control technology, analogous to binding oneself to diets or addiction treatment—especially in a world of advanced commitment devices you later rely on for “iron-clad contracts.” Moreover, “not wanting to learn” is unstable when competitive selection rewards groups that adopt motivating truth-tracking moral beliefs (they coordinate better, reduce internal defection, and may win conflicts). If this holds, your inference “internalism → avoidance → low convergence” is not directionally robust; internalism could increase both convergence and motivation. You’d need to specify conditions under which avoidance dominates precommitment/selection (e.g., privacy, low intergroup competition, easy self-deception) rather than treating avoidance as the default.  

7. In **§2.4.2 (antirealism)** you claim antirealists face “astronomical haystack” non-convergence because there are no “objective qualities of experiences” indicating value, so even utilitarians won’t converge on “best experiences.” **Attack type: Countermodel (shared structure despite antirealism).** Even under antirealism, convergence can arise from common architecture: many agents may share evolved reward circuitry, computational constraints, and preference-learning dynamics that strongly favor a narrow band of experience-types (stability, novelty balance, avoidance of wireheading-like collapse, narrative coherence). That creates de facto “objective-ish” attractors (not truth, but equilibrium) that your argument denies; the haystack can be sharply pruned by universal constraints on minds that remain agentic over long horizons. In that world, antirealism doesn’t imply broad divergence; it implies coordination on a few psychologically stable value-constructs. If this holds, you must either argue why post-AGI minds won’t share these constraints (radical self-modification breaks universals) or incorporate mind-design constraints as the main determinant of convergence rather than metaethics.  

8. In **§3.1–§3.2** you propose cosmic “partition” trade (Milky Way commonsense utopia; other galaxies utilitarian utopia) and suggest “enormous gains from trade” with low transaction costs and enforceable contracts. **Attack type: Strategic response (boundary manipulation / bargaining power).** Once values are tradable at cosmic scale, actors have incentives to *reshape their expressed utility functions* to improve bargaining position (commit to lexicographic preferences, claim extreme disutility for others’ projects, or cultivate “non-negotiable sacred values”) because doing so extracts concessions—especially if contracts are enforceable. This turns “moral trade” into a selection process for agents who can credibly commit to intransigence, reducing the feasibility of near-Pareto partitions and pushing outcomes toward threat-like equilibria even without explicit “threats” as you define them. If this holds, your optimism about compromise depends on an unstated assumption: bargainers reveal honest, smooth, comparable preferences and can’t cheaply self-modify to become harder negotiators. You’d need to model mechanism design for preference revelation (or institutional constraints on self-modification/commitment) rather than assuming frictionless cooperative bargaining.  

9. In **§3.3 (“problem of threats”)** you treat threats as a mostly negative overlay on otherwise-positive trade, and imply preventing them is a key lever: “*even small risks of executed threats can easily eat into the expected value*.” **Attack type: Reversal (threat-suppression increases covert coercion / worse equilibria).** In a world with “iron-clad contracts” and powerful enforcement, an anti-threat legal regime can create incentives for *stealthy, deniable, hard-to-classify harms* (e.g., creating beings whose suffering is epistemically occluded, or designing harms that look like normal production externalities) because overt threats are punishable but covert leverage isn’t. That can make expected disvalue *higher* than in an open-threat world where threats are legible and deterrable via countercommitments, insurance pools, or preemptive bargaining. If this holds, “prevent threats” is not monotone-good; the policy lever can shift harm into channels that are cheaper to execute and harder to police. You’d need to specify the enforcement/observability assumptions under which anti-threat institutions reduce rather than displace coercion, and adjust §3.4’s “mostly-great if threats prevented” conclusion accordingly.  

10. In **§5** you argue we should “act much more on the assumption that we live in scenario (3)” because marginal impact is higher there, and you give a toy calculation comparing power-seeking in scenario (1) vs improving Surviving/Flourishing in scenario (3). **Attack type: Hidden parameter (intervention sign under scenario uncertainty).** The calculation assumes your non-powerseeking actions have positive expected effect specifically in scenario (3), but many plausible interventions (AI governance centralization, safety standards, value coordination institutions) can *increase the probability of scenario (1)* by homogenizing power and reducing pluralistic value discovery, or by creating single points of failure that make “no AM-convergence” catastrophic. In that world, optimizing for scenario (3) conditional impact can worsen the ex ante mixture by shifting probability mass toward the worst case, flipping your recommendation back toward robustness/power-diffusion rather than “assume (3).” If this holds, you need to reframe §5 around policy robustness across scenarios (how actions change scenario probabilities), not just conditional EV within a chosen scenario, and you’d have to revise the headline practical upshot away from “focus on (3)” unless you show your favored actions don’t increase scenario-(1) likelihood.