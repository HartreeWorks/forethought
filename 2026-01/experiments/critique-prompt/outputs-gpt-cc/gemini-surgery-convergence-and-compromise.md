1. The paper’s opening “honing in” move (flight/natural selection analogy) is load-bearing for the claim that a narrow target (“mostly-great futures”) need not imply near-zero expected flourishing, because optimization can find narrow sets. The hidden lemma is that whatever does the optimizing in a post‑AGI world is actually optimizing for “mostly-greatness” rather than for a proxy (power, stability, legibility, prestige) that is correlated with flourishing only in the low-optimization regime. A counter-model consistent with the paper’s own “no easy eutopia” framing is: post‑AGI institutions optimize extremely hard for regime stability and conflict-minimization (a narrow target too), which reliably yields vast “peaceful” resource usage (e.g., sedation/experience-suppression, extreme surveillance, value-homogenization) that is robustly stable yet far from mostly-great on almost any view that values diversity, autonomy, or rich experiences. The argument’s metal/wing analogies then backfire: optimization finds *a* narrow target, just not the one you want. If this critique holds, the paper must replace the “optimization can hit narrow targets” reassurance with a theorem-like condition tying the optimizing objective (institutional selection pressures, AI delegates’ reward functions, governance metrics) to flourishing rather than to stability/advantage proxies.

2. Section 2 repeatedly treats “WAM-convergence ⇒ mostly-great future” as near-axiomatic (“We’ll here assume that, given WAM-convergence, we will reach a mostly-great future”), which is a keystone dependency because it lets the paper focus on moral convergence rather than execution. The missing inference is the jump from *endorsing the good de dicto* to *implementing world-trajectory changes that actually realize it* in a setting full of strategic actors, complex indirect effects, and AI-mediated action. A counterexample that satisfies WAM-convergence: most powerful actors sincerely converge on a correct theory that (say) prioritizes digital welfare and non-coercion, but the only available action pathways are via large-scale automated systems whose behavior is brittle to specification and adversarial manipulation; the result is “values are right, control is wrong,” producing lock-in to a mediocre equilibrium or even a catastrophe by the convergers’ own lights. This breaks the paper’s main decomposition, because WAM-convergence would not be sufficient even under “reasonably good conditions” unless “good conditions” smuggles in near-complete control/verification of implementation. The revision needed is to explicitly add (and then analyze) a second load-bearing premise: that there exist reliable institutions/technical controls that translate de dicto motivation into correct large-scale outcomes under adversarial pressure, rather than assuming it away.

3. The anti-realism argument in 2.4.2 leans on a “free parameters / astronomical haystack” lemma: without objective moral facts, idealization is underpowered, so reflective endpoints will diverge massively, hence WAM-convergence is unlikely. The unstated assumption is that the space of reflective equilibria is *not* shaped by convergent instrumental/strategic constraints (coordination, bargaining stability, acausal considerations the paper itself flags, etc.) that can create strong attractors even absent moral facts. A counter-model: suppose agents’ reflective procedures are chosen under self-modification pressures to avoid being Dutch-booked, exploited, or extorted; then many diverse starting points converge on a small set of stable bargaining norms (e.g., something close to impartial welfarism or broadly cooperative aggregation) because those norms are equilibrium-selection devices, not “moral truths.” In that world, anti-realism holds, individuals still choose their own idealization procedures, yet convergence is high precisely because non-convergent procedures are strategically dominated and get selected out. If this critique holds, the paper must either (i) argue that strategic/equilibrium attractors are too weak to concentrate reflective endpoints, or (ii) incorporate them as a separate “convergence mechanism” that can overpower the free-parameter divergence story.

4. The realism argument in 2.4.1 depends on a “distance/alienness” premise: if moral realism is true, the correct morality is likely far from human preferences, hence even if learned it won’t motivate (or people will avoid learning it under internalism). This is doing heavy work because it turns realism from a convergence-helps story into a motivation-hurts story, supporting pessimism about WAM-convergence even conditional on realism. A counterexample is a fully realist but naturalist picture where moral facts strongly track human flourishing/relationship goods/anti-suffering constraints—so the correct view is *less* alien than today’s parochial norms and is motivationally resonant (e.g., “compassion generalizes,” “coercion corrupts”), making internalism an engine of motivation rather than avoidance. In that world, realism yields both convergence and motivation; your realism-based pessimism flips sign. To survive this critique, the paper needs to defend (not assume) that realism makes the correct view “weird” in the relevant motivational sense—e.g., by showing that the probability mass of plausible realist theories is concentrated on ones that demand extreme impartiality or radical experience-optimization that humans would reject.

5. Section 2.3.2’s abundance argument turns on a key causal step: extreme wealth pushes marginal spending toward preferences with slow diminishing returns, and many such preferences can be non-altruistic (linear collector/status/shrine preferences), so abundance does not strongly imply altruistic dominance. The counterexample is a post‑AGI economy where *status competition and positional goods collapse* because replication, anonymity, and preference-editing make relative status unstable, while auditability and norm enforcement make reputational/contractual standing depend on meeting impartial welfare constraints; then the surviving slow-diminishing preferences are precisely pro-social/constraint-respecting ones, and abundance drives a huge share of resources into what looks altruistic. This respects your premise (abundance changes marginal allocation) but reverses your conclusion (abundance becomes a strong driver of altruistic use of resources). If this critique holds, the paper must revise the “linear non-altruism is natural” inference by conditioning on equilibrium properties of post‑AGI social status, enforcement, and preference markets, rather than extrapolating from billionaire philanthropy under today’s incentives.

6. The “long views win” mechanism in 2.3.3 is treated as a possible selector for altruistic/non-discounting values, but the paper then largely reasons as if the relevant bargaining happens in a regime where diverse values coexist with negotiable shares. The hidden incompatibility is that the same dynamics that make “long views win” plausible (self-modification, compounding, strategic patience) also make *first-mover capture and path-dependent lock-in* plausible, which can preempt the slow selection effects entirely. Counter-model: patient agents reinvest into capability, reach decisive strategic advantage early, and lock in a stable resource claim before demographic/wealth selection has time to shift population-level values; then “long views win” predicts dominance of whichever view got early compounding, not dominance of altruism or of anything reflective. This fits your own “major decisions might be made soon and persist” caveat, but if that caveat is taken seriously it undercuts the paper’s broader optimism about eventual trade/compromise emerging from a pluralistic landscape. The needed revision is to explicitly model the race between (i) compounding-driven power capture and (ii) value-selection-over-time, and show which regime is expected under your assumed post‑AGI timelines.

7. The trade-and-compromise optimism in 3.1–3.2 relies on a separability premise: that resources can be partitioned (galaxies, star systems, compute) so that different moral views can each get “near-best by their lights,” making trade massively positive-sum. A counterexample consistent with your own “weirder issues” list is that the dominant determinants of value are *global* rather than local—e.g., acausal trade commitments, simulation policies, universal security constraints, or irreversible cosmological engineering choices that impose externalities across all regions. Then “split the galaxies” fails: one group’s policy (say, creating vast numbers of simulated minds, or adopting a risky acausal bargaining stance, or implementing a universal ban on certain computations) changes the payoff landscape everywhere, so no clean partition exists and moral trade collapses into coercive constitutional design. If this critique holds, the paper must add and analyze explicit conditions for moral trades to exist (low externalities, enforceable boundaries, no global policy coupling), rather than treating resource divisibility as the default in cosmic settings.

8. The threats discussion in 3.3 is load-bearing for downgrading the trade pathway (“even small executed threats can eat into expected value”), but it quietly assumes that credible commitment to value-destroying actions is easy and not systematically deterred by equilibrium enforcement. A counter-model that preserves your “iron-clad contracts” premise is: superintelligence-enabled verification makes threat-making *publicly attributable and precommittable to punishment*, so coalitions adopt a meta-constitution (“threateners are expropriated / sandboxed”) that is mutually beneficial even to many self-interested actors because it eliminates extortion tax and increases expected surplus. In that world, the mere availability of credible commitments does not increase threats; it enables anti-threat enforcement, and the paper’s pessimistic weighting of threats flips. If this critique holds, you must either (i) show why anti-threat constitutions are unstable (e.g., because punishment itself creates new threat surfaces), or (ii) condition the threat risk on specific informational/attribution failures rather than on “credible commitment exists.”

9. Section 5’s recommendation to “act much more on the assumption that scenario (3) holds” relies on a pivotality premise: marginal effort in scenario (3) produces larger expected value gains than power-seeking in scenario (1), because scenario (1) is low-flourishing and your power share is tiny. A counterexample consistent with your scenario taxonomy is a threshold world where scenario (1) outcomes are winner-take-all among many non-convergent value systems; then increasing your power from 1e‑9 to 1e‑7 can move you from “never pivotal” to “occasionally pivotal” in a small set of decisive contests (e.g., controlling a key compute bottleneck or constitutional vote), producing discontinuously large expected value gains. Meanwhile in scenario (3), institutional robustness and convergence dilute individual marginal impact, making your non-powerseeking interventions less pivotal than assumed. If this critique holds, the paper must revise the EV comparison to include discontinuities and heavy-tailed pivotal events (not just proportional improvements), otherwise the “scenario (3) dominates for action” conclusion is not licensed.

10. The “concentration of power is a blocker” claim in 3.5 depends on an implicit independence assumption: the moral correctness (or AM-convergent fraction) among the powerful is roughly the same as among the general population, so fewer power-holders means lower probability the “correct view” is represented. A counter-model is one where the process that concentrates power is *positively correlated* with the paper’s own prerequisites for AM-convergence: long time horizons, willingness to use reflective AI advisors, comfort with abstraction, and norm internalization—traits that could plausibly be overrepresented among those who end up controlling key institutions/compute. Then power concentration can *increase* the representation of reflective altruists (even if it decreases diversity), raising rather than lowering the chance that partial AM-convergence exists at the bargaining table. If this critique holds, the paper must replace the monotonic “more concentration ⇒ worse” inference with a model that conditions on selection effects into power and their correlation (positive or negative) with the kinds of moral reflection/motivation you care about.