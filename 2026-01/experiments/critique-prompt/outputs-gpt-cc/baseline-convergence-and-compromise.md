1. The paper relies heavily on analogies (flight evolution, metal shaping, sailing to an island) to rebut pessimism about “narrow targets,” but it never shows that the relevant structural similarities actually hold. In particular, biological evolution and engineered design succeed via clear selection criteria and feedback loops, whereas “the good” is precisely what is disputed and may not supply a comparable objective gradient for optimization.

2. The central premise that “mostly-great futures are a very narrow target” is asserted rather than defended in this essay, yet many later inferences (especially about coincidence and the need for deliberate aiming) depend on it. Without a clearer metric of “mostly-great,” a model of the space of futures, and an argument for “narrowness,” the paper risks building a large superstructure on an undefended background assumption.

3. Key terms like “mostly-great,” “near-best,” “accurate ethical view,” and “correct moral view” remain too ambiguous to support the argument’s probabilistic and strategic conclusions. For example, “accuracy” could mean correspondence to stance-independent facts, coherence within an agent’s idealized preferences, or intersubjective agreement—each yields different predictions about convergence and motivation.

4. The realism/anti-realism fork in section 2.4 is presented as exhaustively framing convergence prospects, but it overlooks influential middle positions (constructivism, contractualism, ideal observer theories, response-dependence) that can predict convergence without “robust” realism. As a result, the argument risks a false dilemma: either objective weird moral truth that alienates agents, or radical divergence under subjectivism.

5. The inference “if realism is true, the correct moral view is probably very weird/alien relative to current preferences” is under-argued and may be question-begging. Many realist views (e.g., Cornell realism, naturalist realism) are compatible with substantial continuity between evolved social preferences and moral truth, so alienness does not straightforwardly follow from realism.

6. The claim that internalism would lead people to “choose not to learn or believe the correct moral view” is psychologically and conceptually shaky. Even if moral judgment is motivating, it does not follow that agents can or will systematically avoid moral belief formation, nor that such avoidance would be stable in institutions that reward truth-tracking and disclosure.

7. The “abundance implies altruism” discussion leans on contestable utility-shape assumptions (rapidly diminishing self-interested marginal utility, slowly diminishing altruistic marginal utility) without establishing them for post-AGI agents or modified psychologies. The billionaire-philanthropy evidence is also an awkward proxy, since today’s giving is constrained by coordination problems, status incentives, institutional leakage, and non-resource bottlenecks that may not generalize to the post-scarcity setting the paper is analyzing.

8. The treatment of “moral trade” often presumes frictionless bargaining, enforceable contracts, and clean separability of resources (e.g., “this galaxy for your utopia, that galaxy for mine”), but real-world bargaining is shaped by bargaining power, commitment races, credible threats, and path dependence. Without a more formal game-theoretic account, the optimism about compromise looks sensitive to idealized assumptions that the paper itself flags as “blockers” rather than integrating them into the main forecast.

9. The analysis underestimates counterexamples where compromise is impossible or unstable even among “reasonable” moral views—e.g., sacred values, deontological side-constraints, agent-relative permissions, or lexical priorities that block tradeoffs. If many agents treat some constraints as non-negotiable, then “resource-compatibility” and “hybrid goods” may not rescue value, and the paper’s bargaining-based optimism narrows to a limited subset of moral psychologies.

10. In section 5, the expected-impact comparison between power-seeking in scenario (1) and altruistic action in scenario (3) uses illustrative numbers but does not justify the probability assignments, the scaling assumptions, or the tractability estimates that drive the conclusion. This makes the recommendation feel like it depends on arbitrary parameter choices, and it also ignores that power-seeking might itself change which scenario obtains (e.g., by increasing coercion, lock-in, or threat dynamics) rather than being a neutral lever within a fixed scenario.