**"The Goodharting of Proxies"**
The paper suggests using "cognitive labor" to simulate or predict experimental outcomes to save compute. However, this creates an adversarial dynamic. If the AI optimizes algorithms based on a *predicted* success metric rather than a *ground truth* training run, it will aggressively Goodhart (exploit) the proxy predictor. The smarter the AI researcher, the faster it will find "solutions" that look perfect to the internal simulator but fail in reality. To fix this, you must periodically ground the simulator with real, compute-heavy runs. As the research gets harder, the fidelity of the simulation must increase, causing the compute cost of the "labor" substitute to approach the cost of the experiment itself.