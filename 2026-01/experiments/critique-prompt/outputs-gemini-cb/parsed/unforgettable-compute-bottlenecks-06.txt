**"Hardware-Software Overfitting"**
The paper assumes that software intelligence can continue to optimize algorithms independently of the hardware substrate. However, the "Transformer" paradigm is successful largely because it is perfectly overfit to the matrix-multiplication strengths of GPUs. If the next OOM of algorithmic progress requires a paradigm shift (e.g., sparse asynchronous updates, neuromorphic spiking) that fights against the current dense-matrix hardware, no amount of software intelligence can optimize away the mismatch. The $\rho$ value isn't intrinsic to "AI R&D"; it is path-dependent on the current hardware architecture. We may be locally optimal on GPUs, meaning further software gains require *new* hardware, re-locking the bottleneck.