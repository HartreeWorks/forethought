**"The Verification Asymmetry"**
The paper implies that "cognitive labor" (generating ideas/code) is a substitute for "physical capital" (running experiments). This misses that in empirical ML, verification is orders of magnitude more expensive than generation. A superintelligence might generate 1,000 brilliant new architecture ideas in a second, but if the only way to distinguish the one working architecture from the 999 subtle failures is a training run, the bottleneck remains entirely on compute. The "substitutability parameter" ($\rho$) is likely asymmetrical: high for idea generation, but near zero for idea verification. If valid theories are sparse and require empirical validation, infinite intelligence just generates a longer queue for the same GPUs.