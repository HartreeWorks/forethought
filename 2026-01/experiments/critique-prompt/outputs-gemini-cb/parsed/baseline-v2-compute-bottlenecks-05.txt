"The Recursive Efficiency Paradox" — The paper suggests that algorithmic efficiency gains will allow for more experiments, thereby loosening the compute bottleneck. However, achieving those specific efficiency gains likely requires running compute-intensive experiments to discover and verify them first. This creates a circular dependency: to save compute, one must spend compute. Unless the "low-hanging fruit" of efficiency can be discovered via pure *a priori* reasoning without empirical testing—a claim the paper asserts but does not substantiate with evidence from current ML engineering—the feedback loop may be dampened significantly.