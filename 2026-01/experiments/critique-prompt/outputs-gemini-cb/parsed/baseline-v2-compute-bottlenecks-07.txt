"The Finite Nature of Algorithmic Optimization" — The author’s optimism relies on the assumption that software efficiency can be improved by several orders of magnitude (OOMs). While distinct from hardware limits, algorithmic efficiency is still bounded by information-theoretic limits (e.g., Landauer’s principle applied to logical operations, or irreducible complexity of learning tasks). If the current inefficiencies in AI training are not vast—or if we are closer to the theoretical Pareto frontier of learning efficiency than the author assumes—the "room at the top" for software improvements may be quickly exhausted, causing the SIE to stall much earlier than predicted.