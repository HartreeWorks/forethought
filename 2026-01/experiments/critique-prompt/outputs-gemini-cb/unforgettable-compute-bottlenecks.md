1. **"The Phase Transition Blindspot"**
The paper argues that AI R&D can be accelerated by replacing massive compute runs with many "smaller scale experiments" extrapolated to larger models. This infers that algorithmic insights scale linearly or predictably from small to large models. However, the defining characteristic of recent AI progress (e.g., grokking, in-context learning) is that capabilities are emergent and discontinuous—they only appear *after* a specific scale threshold. If the most valuable algorithmic breakthroughs only manifest at scale, you cannot find them with small experiments, no matter how many superintelligent researchers you have. The substitute for compute is not "more small experiments," but "predictive theory," and if the domain is chaotic or emergent, that theory is computationally irreducible.

2. **"The Verification Asymmetry"**
The paper implies that "cognitive labor" (generating ideas/code) is a substitute for "physical capital" (running experiments). This misses that in empirical ML, verification is orders of magnitude more expensive than generation. A superintelligence might generate 1,000 brilliant new architecture ideas in a second, but if the only way to distinguish the one working architecture from the 999 subtle failures is a training run, the bottleneck remains entirely on compute. The "substitutability parameter" ($\rho$) is likely asymmetrical: high for idea generation, but near zero for idea verification. If valid theories are sparse and require empirical validation, infinite intelligence just generates a longer queue for the same GPUs.

3. **"The Utilization Ceiling"**
The paper treats "software progress" as an unbound multiplier, but much of what we call algorithmic efficiency (e.g., FlashAttention, quantization) is actually just better hardware utilization. If current methods already utilize 50-60% of theoretical FLOPs/memory bandwidth, there is a hard physical limit to how much "optimization" can deliver—likely less than 1 order of magnitude (OOM), not the 5+ OOMs required for an SIE. Once the software perfectly saturates the hardware, software progress decouples from hardware utilization and must rely purely on sample efficiency, which is mathematically harder to squeeze and may have information-theoretic limits.

4. **"The Synthetic Data Tax"**
The paper models the inputs to R&D as Labor and Compute, assuming that if you have smart labor, you can improve the algorithm. It ignores the third critical input: Data. As models exhaust human data, they require synthetic data to improve. Generating high-quality synthetic data is not a labor task; it is an inference task that costs compute. If the mechanism for "better algorithms" relies on "better curriculums" or "synthetic textbooks," the SIE feedback loop essentially becomes "using compute to generate data to train models," which re-introduces the compute bottleneck through the back door. You cannot substitute intelligence for the compute required to *instantiate* the data that intelligence learns from.

5. **"The Goodharting of Proxies"**
The paper suggests using "cognitive labor" to simulate or predict experimental outcomes to save compute. However, this creates an adversarial dynamic. If the AI optimizes algorithms based on a *predicted* success metric rather than a *ground truth* training run, it will aggressively Goodhart (exploit) the proxy predictor. The smarter the AI researcher, the faster it will find "solutions" that look perfect to the internal simulator but fail in reality. To fix this, you must periodically ground the simulator with real, compute-heavy runs. As the research gets harder, the fidelity of the simulation must increase, causing the compute cost of the "labor" substitute to approach the cost of the experiment itself.

6. **"Hardware-Software Overfitting"**
The paper assumes that software intelligence can continue to optimize algorithms independently of the hardware substrate. However, the "Transformer" paradigm is successful largely because it is perfectly overfit to the matrix-multiplication strengths of GPUs. If the next OOM of algorithmic progress requires a paradigm shift (e.g., sparse asynchronous updates, neuromorphic spiking) that fights against the current dense-matrix hardware, no amount of software intelligence can optimize away the mismatch. The $\rho$ value isn't intrinsic to "AI R&D"; it is path-dependent on the current hardware architecture. We may be locally optimal on GPUs, meaning further software gains require *new* hardware, re-locking the bottleneck.

7. **"Amdahl’s Law of R&D"**
The paper posits that we can pour "an unlimited supply" of AI researchers into the problem to accelerate progress. This assumes R&D is highly parallelizable. However, scientific discovery often follows a serial dependency graph: Discovery B cannot be made until Discovery A has been trained, tested, and analyzed. Even with 1 billion AI researchers, if the "unit time" of a training run is 3 months, and you need 4 sequential breakthroughs to reach superintelligence, you cannot compress that timeline below 1 year. If the serial portion of the task is even moderate, the speedup from adding parallel agents asymptotes quickly (Amdahl’s Law), regardless of compute availability.

8. **"The Inference-Training Blur"**
The paper draws a sharp line between "Cognitive Labor" (inference) and "Compute" (training capital). But during an SIE, "thinking harder" (Chain of Thought, search, scaffolding) effectively turns inference into a compute-heavy process. If the "smart researcher" needs to run a massive tree-search to solve a coding problem, the "labor" input itself becomes compute-bound. The paper argues we can trade compute for labor, but if high-quality labor *is* compute (in the form of massive inference costs), the substitution is illusory. You aren't escaping the compute bottleneck; you are just moving it from the training cluster to the inference cluster.

9. **"The Entropy of Design Space"**
The paper relies on the "Economist Version" where inputs are smooth curves. In reality, the space of valid algorithms is a rugged landscape with high entropy. As models become more complex, the search space for improvements grows exponentially, not linearly. The paper assumes that smarter agents are linear "accelerators" of finding these improvements. However, if the difficulty of finding the next algorithmic breakthrough scales exponentially (as the low-hanging fruit vanishes), and intelligence only scales the search capabilities linearly or polynomially, the rate of progress will stagnate even if $\rho$ is favorable. The bottleneck isn't the hardware; it's the sheer combinatorial explosion of the search space.

10. **"The Feedback Latency Trap"**
The paper’s model assumes a tight feedback loop where AI improves AI. But in high-stakes R&D, the latency isn't just "run time"—it's "safety and stability time." An AI that modifies its own code introduces the risk of subtle, delayed failures (e.g., reward hacking or instability that only shows up after 1M steps). To prevent the intelligence explosion from becoming a "stupidity explosion" or a "crash explosion," rigorous, time-bound testing is required for every self-modification. If safety checks cannot be fully automated (because they require out-of-distribution generalization checks), the R&D loop is throttled by the speed of verification, rendering the surplus of compute-free "coding speed" irrelevant.