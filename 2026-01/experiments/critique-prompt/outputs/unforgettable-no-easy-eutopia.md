1. "The Multiplicative Mirage" attacks the paper's inference that value being a product of independent factors implies most futures are far from eutopia. The paper assumes these factors are independently distributed, but in any realistic future, the factors are deeply correlated—societies that get digital welfare right are more likely to get population ethics right because both stem from similar moral reasoning capacities and institutional structures. If factors are positively correlated, the product distribution shifts dramatically rightward, making mostly-great futures far more common than the toy model suggests. The paper would need to defend the independence assumption with empirical evidence about how moral progress actually works across domains, which would require a substantially different argumentative strategy.

2. "The Extinction Anchor Problem" targets the paper's definition of value where extinction is stipulated at 0. The entire argument about how far common-sense utopia falls short depends on this anchoring—but why should extinction serve as the natural zero point rather than, say, the expected value of a random accessible future? If we anchor differently, the ratios change dramatically. The paper claims this follows from von Neumann-Morgenstern representation, but the representation theorem only gives uniqueness up to positive affine transformation, meaning the choice of zero is arbitrary and doing substantial argumentative work the authors don't acknowledge. The paper would need to justify this specific anchoring or show its conclusions are robust to alternative anchors.

3. "The Fat Tail Inversion" challenges the inference that fat-tailed distributions of value-efficiency make eutopia harder to achieve. The paper argues that because the best uses of resources are much better than typical uses, hitting the narrow peak is necessary. But fat-tailed distributions also mean that the difference between the 90th and 99th percentile is small compared to the difference between the 50th and 90th percentile—so once you're doing reasonably well, you're already capturing most of the value. The same evidence about fat tails that supposedly makes eutopia hard could equally suggest that "pretty good" captures most of what matters. The paper would need to show specifically where on the distribution the 50% threshold falls, which requires empirical claims it doesn't make.

4. "The Hedonic Treadmill Cuts Both Ways" undermines the paper's use of psychological evidence. The paper invokes treadmill effects to explain why eutopia seems closer than it is—but the same psychological mechanisms should make our current judgments that the future is far from eutopia equally suspect. If we systematically underestimate how far we are from ideal states, we might also systematically overestimate how specific our requirements are for satisfaction. The paper's own psychological argument suggests we cannot trust our intuitions about how fussy to be, yet the paper relies heavily on intuitions about what would constitute moral catastrophe. The argument would need some way to escape this self-undermining pattern.

5. "The Convergence Smuggle" identifies a hidden assumption in the paper's treatment of moral views. The paper lists many moral perspectives (religious, conservative, cosmopolitan, etc.) as each seeing current society as catastrophic, implying that satisfying any one view leaves others unsatisfied. But this treats these views as independent when many share deep structural features—most care about flourishing, suffering, autonomy, and meaning. A future optimized for these shared features might satisfy the core concerns of most views simultaneously, even if it fails on each view's distinctive requirements. The paper would need to defend the claim that the distinctive requirements, not the shared ones, are what matter for achieving most value.

6. "The Bounds Bootstrap" attacks the paper's treatment of bounded versus unbounded views. The paper argues bounded views are fussy because they become approximately linear over humanity's small contribution to universal value. But this assumes we should evaluate humanity's contribution relative to universal value rather than relative to accessible value. If bounded views care about diminishing returns within what we can affect, then the curvature kicks in precisely where it matters. The paper's argument requires bounded theorists to adopt a particular cosmological reference frame that they have no reason to accept. A bounded theorist could simply define their bounds over the accessible universe, escaping the linearization argument entirely.

7. "The Digital Being Dilemma" exposes how the paper's argument proves too much. The paper treats uncertainty about digital welfare as a risk of moral catastrophe. But this uncertainty exists now—we don't know if current AI systems have morally relevant experiences. If mere uncertainty about welfare creates catastrophic risk, then current society is already in moral catastrophe for different reasons than the paper acknowledges, and the "survival" baseline the paper uses is already deeply negative. This would collapse the paper's framework where survival is the denominator against which flourishing is measured. The paper cannot simultaneously treat digital welfare uncertainty as a future catastrophe and treat current survival as a meaningful positive baseline.

8. "The Common-Sense Utopia Trap" reveals that the paper's key example undermines its own argument. The paper stipulates that common-sense utopia involves "minimal suffering among nonhuman animals and non-biological beings"—but this is already loading in the correct answer to several of the supposedly difficult questions. If achieving common-sense utopia requires solving animal welfare and digital welfare, then it's not a baseline against which fussiness is measured but rather a demanding achievement. The paper needs common-sense utopia to seem easily achievable to make fussiness surprising, but describes it in terms that already require solving hard problems. The paper would need to define a genuinely minimal baseline that doesn't presuppose solutions to its own catastrophe examples.

9. "The Intertheoretic Escape Hatch" challenges the paper's claim that moral uncertainty makes things worse. The paper argues that aggregating across views remains fussy, but this assumes we should aggregate by taking expectations. An alternative approach—satisficing across views, aiming for outcomes that clear some threshold on most views—could be far more achievable. The paper dismisses joint-aggregation bounded views as implausible, but a satisficing approach could inherit joint aggregation's easygoingness without its "scale-tipping" problems. The paper treats expected value maximization across views as the only serious approach to moral uncertainty, which is itself a highly contested assumption. The paper's fussiness conclusion depends on this methodological choice.

10. "The Resources Red Herring" attacks the paper's claim that linear views require using "most accessible resources." The paper argues that achieving 50% of best feasible value requires controlling most of the accessible universe. But this assumes value scales linearly with resources at all scales, when the paper's own examples (Russell's ecstasy, Dostoevsky's epileptic experiences) suggest that some small-scale arrangements might achieve extraordinary value density. If the best use of resources is intensely concentrated rather than evenly spread, then a civilization controlling a tiny fraction of resources but using them optimally might achieve most feasible value. The paper's linearization argument undermines its scale argument.

11. "The Moral Progress Paradox" identifies a self-undermining element in the paper's historical argument. The paper uses past moral catastrophes (slavery, animal farming) to argue future catastrophes are likely. But recognizing past catastrophes requires moral progress—we now see what was wrong. If moral progress is real and continuing, the argument for ongoing catastrophes weakens over time. The paper needs both that we've made enough progress to identify past errors and that we won't make enough progress to avoid future errors. This requires a specific claim about where moral progress saturates that the paper doesn't provide. The paper would need to explain why moral progress was sufficient to reveal historical catastrophes but insufficient to prevent future ones.

12. "The Narrow View Nihilism" reveals that the paper's argument, if successful, undermines its own practical implications. If eutopia requires getting essentially everything right, and we have systematic uncertainty about what's right, then our expected impact on achieving eutopia is negligible regardless of what we do—we're almost certainly wrong about something crucial. But then the paper's implicit recommendation to work on "flourishing" rather than just "survival" loses force, since we have no reliable way to move toward the narrow target. The paper motivates concern about flourishing by showing the stakes are high, but simultaneously argues the target is so narrow that hitting it deliberately seems nearly impossible. The paper needs some account of how working on flourishing helps given radical uncertainty about the target.

13. "The Normalization Nullification" undermines the paper's intertheoretic comparison discussion. The paper surveys various normalization methods and notes they disagree significantly. But this disagreement is presented as neutral when it actually favors the "easy eutopia" position—if experts cannot agree on how to aggregate views, we have little reason to act on any particular aggregation. The paper uses intertheoretic uncertainty to suggest we should be fussy, but such deep uncertainty about how to even compare views suggests we should be epistemically humble about any strong claims regarding how difficult eutopia is. The paper's own demonstration of normalization disagreement undercuts confidence in its fussiness conclusion.

14. "The Experience Machine Reversal" turns the paper's wellbeing argument against itself. The paper argues that uncertainty about correct theories of wellbeing (hedonism vs. objective list vs. preference satisfaction) creates catastrophe risk. But the existence of this genuine uncertainty suggests that the "correct" theory might be one that doesn't distinguish sharply between these alternatives—perhaps what matters is some combination or none are fully correct. If moral reality doesn't carve at the joints the paper assumes, then futures that do reasonably well by all three theories might capture most of what matters. The paper's uncertainty argument assumes the true theory is one of the candidates when it might be none or a blend, which would make the target less narrow.

15. "The Selection Effect Blindspot" identifies how the paper's examples are systematically biased. The paper lists potential future catastrophes (wrong population ethics, wrong digital welfare attitudes, etc.) that each seem capable of destroying most value. But these examples are selected precisely because they're the kind of thing moral philosophers worry about—they represent the tail of our uncertainty distribution. The paper never considers that most features of a future civilization might be morally neutral or convergently good, such that the catastrophe-prone dimensions are rare. The paper would need to show that catastrophe-prone dimensions are actually common among the dimensions that determine value, not just that such dimensions exist and we've noticed some.