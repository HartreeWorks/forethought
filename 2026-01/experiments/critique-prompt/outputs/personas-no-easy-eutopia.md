1. [The Empirical Hardliner] The paper's central claim that value is "multiplicative" across independent factors—such that a single flaw can eliminate most potential value—is asserted without any identified causal mechanism explaining why moral value should decompose this way rather than additively or through some other aggregation function. The authors offer the toy model where value equals the product of N uniformly distributed factors, but provide no empirical evidence that moral value actually behaves this way, nor any falsifiable prediction that would distinguish multiplicative from additive models. The analogy to fat-tailed distributions of wealth and health outcomes conflates instrumental goods with intrinsic moral value without justification. If this multiplicative structure is wrong, the entire quantitative framing collapses—the 99.99th percentile "best feasible future" threshold, the 50% "mostly-great" cutoff, and the claim that even futures scoring highly "on average" across factors still miss most value all depend on this undefended functional form. The consequence is that the paper's core argument rests on a mathematical convenience rather than a demonstrated feature of moral reality.

2. [The Game-Theoretic Defector] The paper assumes that "coordinated efforts to promote the overall best outcomes" could meaningfully steer civilization toward eutopia, but ignores that any actor with the capability to influence resource allocation at cosmic scales faces overwhelming incentives to defect from cooperative value-maximization. Consider the "initial periods of settlement and resource appropriation" the paper mentions—whoever controls the first self-replicating probes or AI systems that reach extrasolar resources has no credible commitment mechanism to share those resources according to any collectively agreed moral framework. The paper's own acknowledgment that resources might be "concentrated among a tiny number of hands" by "whoever was most-willing and most-able to grab them" is not merely one possible failure mode; it is the dominant equilibrium when first-mover advantages are astronomical and enforcement mechanisms are physically impossible across light-year distances. The consequence is that the paper's entire framework of "aiming" at eutopia assumes a coordination capacity that incentive structures will systematically destroy.

3. [The Mechanism Designer] The paper repeatedly invokes "society" making decisions about digital being welfare, space resource allocation, and population ethics, but never specifies the mechanism by which these collective choices would be made or enforced. When the authors write that "society's political systems might not allow those views to win out in face of a majority (or empowered minority) that opposes them," they acknowledge the problem but offer no formal specification of what alternative institutional design could overcome it. The sailing metaphor—with its "navigation systems" and "reconnaissance boats"—is pure analogy without any corresponding protocol. What voting rule aggregates preferences over incompatible population ethics? What property rights regime governs the first asteroid miners? What court enforces digital being rights against their human owners? Without these mechanisms specified precisely enough to analyze their equilibria and failure modes, the paper's framework for "reaching a near-best future" is not a proposal but a wish. The consequence is that readers cannot evaluate whether eutopia is achievable because there is no specified path to evaluate.

4. [The Institutional Corruptionist] The paper's extended discussion of how "future decision-makers" might adopt correct moral views and act on them systematically ignores that any institution powerful enough to steer civilization toward eutopia will be captured by those who benefit from the status quo. The authors note that "motivated cognition (including biased training of AI advisors)" could lead to convenient moral views, but treat this as a contingent risk rather than the inevitable outcome of regulatory dynamics. Consider their example of digital being welfare: any board, agency, or constitutional provision that adjudicates AI rights will be staffed and funded by entities whose profits depend on treating AI as property. The "truth-seeking deliberative processes" mentioned in the conclusion are precisely the kind of procedural idealism that masks how actual institutions become compliance theaters—producing ethical certifications that legitimate existing power arrangements. The consequence is that the paper's optimism about "forces which guide society towards hitting" the eutopian target ignores that those forces will themselves become instruments of the interests they were meant to constrain.

5. [The Capability Accelerationist] The paper's entire framing of "no easy eutopia" assumes that humanity has meaningful choice about how the future unfolds, but capability development is largely exogenous to the moral deliberation the authors imagine. If one AI lab, nation, or posthuman entity achieves recursive self-improvement or space settlement capability first, they determine the trajectory regardless of what moral framework humanity converges upon. The authors' discussion of space resource allocation—"whoever was most-willing and most-able to grab them"—inadvertently concedes this point, but fails to recognize its implication: any delay introduced by "deliberation" about correct population ethics or digital welfare simply cedes the future to actors who don't deliberate. The paper treats moral progress and capability progress as separable, when in fact the latter will select which moral frameworks propagate. The consequence is that the paper's framework for thinking about eutopia is irrelevant to the actual selection dynamics that will determine the future's character.

6. [The Second-Order Catastrophist] Suppose the paper succeeds and influential actors adopt its framework, prioritizing deliberate optimization toward "near-best futures" defined by the multiplicative model. The first consequence is that any group confident they have identified the value-maximizing configuration of resources becomes justified in imposing that configuration at cosmic scale, since failure to do so sacrifices most potential value. The paper's own logic—that achieving only 99th percentile rather than 99.99th percentile outcomes loses most value—creates a mandate for extremism among those who believe they know the correct answers to population ethics, digital welfare, and resource allocation. The framing of eutopia as requiring "essentially no bads at all" on separately aggregating bounded views provides philosophical cover for totalitarian elimination of whatever powerful actors classify as "bads." The consequence is that widespread adoption of the paper's framework doesn't merely fail to achieve eutopia—it provides sophisticated justification for imposing particular visions of the good at the expense of pluralism, consent, and the epistemic humility the authors elsewhere recommend.

7. [The Adversarial Red-Teamer] The paper's value-theoretic framework creates an exploitable attack surface for any sophisticated adversary seeking to manipulate collective decision-making about the future. Consider an actor who wants to prevent space settlement for competitive reasons: they need only promote "environmentalist" moral views (which the paper acknowledges might regard "widespread settlement of other star systems...as a moral catastrophe") to create paralyzing moral uncertainty. The paper's own uncertainty analysis shows that different normalization methods yield dramatically different recommendations—an adversary could selectively fund philosophical research supporting whichever normalization method produces paralysis or their preferred outcome. More directly, the paper's acknowledgment that "motivated cognition (including biased training of AI advisors)" could produce convenient moral views is an explicit blueprint for adversarial manipulation. The consequence is that the paper's sophisticated framework for evaluating futures becomes a weapon for those who understand how to shape the inputs to that framework.

8. [The Moral Parliament Dissenter] The paper's entire quantitative apparatus depends on von Neumann-Morgenstern expected utility theory—complete orderings satisfying transitivity, continuity, and independence—but this framework systematically excludes moral views that reject these axioms. Deontological constraints often generate lexicographic preferences that violate continuity: some actions are simply impermissible regardless of consequences, and no probability weighting makes them acceptable. Virtue ethics centers character and practical wisdom rather than outcome optimization, rendering the entire "maximize expected value" framing category-mistaken. The paper briefly acknowledges that "not all reasonable moral views allow quantitative comparisons" but then proceeds as if this footnote absolved the analysis. The consequence is that the paper's conclusions about "fussy" versus "easygoing" views apply only within a consequentialist framework that many sophisticated moral traditions reject entirely, making its ostensibly ecumenical approach actually parochial.

9. [The Historical Parallelist] The paper's argument that moral catastrophes are "the norm across history" is deployed to support pessimism about achieving eutopia, but the same historical evidence shows that confident assertions about moral truth—exactly what the paper's framework requires for deliberate optimization—have repeatedly produced the catastrophes in question. The religious persecution, slavery, and subjugation the authors list were not accidents of moral uncertainty; they were confident applications of moral frameworks their adherents believed were correct. The 20th century's greatest moral catastrophes—from colonialism to totalitarianism—were perpetrated by actors who believed they had solved the questions the paper raises: correct population ethics (eugenics), proper social organization (communism, fascism), civilization's appropriate resource allocation (lebensraum). The consequence is that the paper's call for deliberate optimization toward identified best outcomes echoes the exact epistemic posture that historically enabled catastrophe, suggesting the framework is not merely insufficient but actively dangerous.

10. [The Complexity Theorist] The paper treats the "factors" determining future value as "relatively independent," enabling its multiplicative model, but complex adaptive systems generate emergent properties and feedback loops that make such independence assumptions catastrophically wrong. Consider the paper's own examples: digital being welfare interacts with population ethics (more digital beings changes optimal population calculations), which interacts with space resource allocation (different populations require different resources), which feeds back to digital welfare (resource constraints affect what kinds of beings are created). The paper's toy model with N independent uniform distributions ignores that actual civilizational choices are embedded in path-dependent historical processes where early decisions constrain later option spaces in unpredictable ways. The authors acknowledge they "should expect that this list is very far from exhaustive" but don't recognize that incompleteness in a complex system isn't merely missing items—it's missing the interaction effects that dominate actual outcomes. The consequence is that the paper's quantitative framework produces precise-seeming conclusions from a model that fundamentally misrepresents how civilizational value actually emerges.

11. [The Political Economist] The paper frames the challenge of achieving eutopia as an epistemological problem—identifying correct moral views and steering toward them—while systematically ignoring that the actual constraints are distributional conflicts over power and resources. When the authors discuss space resource allocation, they list possible "allocation systems" as if the choice were a seminar exercise, rather than recognizing that whoever controls early space infrastructure will design allocation rules that entrench their position. The paper's repeated invocation of "society" making choices obscures that there is no such unified agent—there are competing factions whose material interests diverge. The discussion of digital being rights entirely omits that the owners of AI infrastructure have massive financial incentives to resist any framework that grants their assets independent moral claims. The consequence is that the paper's sophisticated moral philosophy floats above the actual political-economic dynamics that will determine the future, offering a framework that is irrelevant to the power struggles through which these questions will actually be decided.

12. [The Cognitive Scientist] The paper's framework assumes that human moral beliefs can "converge" through "truth-seeking deliberative processes" toward correct views, but this ignores extensive evidence that human moral cognition is adaptation-shaped, context-dependent, and systematically biased in ways that preclude such convergence. The "hedonic treadmill" effect the authors invoke actually undermines their argument—if preference adaptation continuously resets subjective welfare, then any specification of "wellbeing" sufficient for eutopia becomes a moving target that deliberation cannot stabilize. More fundamentally, human moral intuitions about scope, probability, and counterfactuals—precisely the domains the paper's framework requires reasoning about—are demonstrably unreliable. The authors' own acknowledgment that psychological effects make "mostly-great futures seem just about attainable" applies equally to their readers' evaluation of the paper's arguments. The consequence is that the paper proposes a deliberative path to eutopia using cognitive machinery that cannot reliably perform the required computations.

13. [The Systems Engineer] The paper proposes a target—"near-best futures" within narrow tolerance bands—without any analysis of fault tolerance, redundancy, or graceful degradation. In engineering terms, the multiplicative value model implies a system where any single component failure causes total system failure, the most fragile architecture possible. Real resilient systems are designed with redundancy precisely because some components will fail; the paper's moral framework has none. What happens when one civilization in one star system makes a "wrong" population ethics choice—does this condemn the entire cosmic future to sub-eutopia? The paper provides no mechanism for error correction, rollback, or isolation of failures. The framing of success as requiring correctness across "digital welfare, population ethics, variety/diversity" and other dimensions simultaneously implies a system with no margin for error and no recovery path. The consequence is that the paper describes a design requirement (achieve eutopia) without a feasible implementation architecture, like specifying a bridge that must never experience stress without explaining how to build one.

14. [The Evolutionary Skeptic] The paper assumes that correct moral views, once identified, could be stably maintained across cosmic timescales, but selection pressures will systematically erode any configuration that is not self-reinforcing. Consider the paper's concern about "wrong discount rate"—civilizations that discount the future will expand faster than those that don't, eventually dominating the resource base regardless of which discount rate is "correct." The same logic applies to population ethics: views that favor more reproduction will outcompete views favoring fewer, higher-welfare lives. The paper acknowledges that "values could become unmoored from human values, drifting into worthlessness," but treats this as an avoidable risk rather than an evolutionary inevitability. Any eutopian configuration that is not also an evolutionarily stable strategy will be invaded by variants that sacrifice value for competitive advantage. The consequence is that the paper's entire framework ignores that the "target" isn't stationary—selection will reshape what configurations are achievable, making eutopia a moving target that recedes under competitive dynamics.

15. [The Resource Economist] The paper's discussion of space resource allocation and cosmic scale assumes that resources can be directed toward value-maximization with negligible transaction costs, but coordination across astronomical distances faces fundamental physical constraints that make such allocation practically impossible. The light-speed limit means that any governance system for extrasolar resources faces multi-year communication delays, making real-time coordination impossible and creating persistent information asymmetries. The paper notes that "20 billion galaxies in the affectable universe" would need to be used optimally for linear views to achieve eutopia, but provides no analysis of the coordination costs scaling with this scope. Furthermore, the paper assumes that "best feasible futures" are computationally identifiable—that we can actually determine which resource configurations maximize value—when this optimization problem is almost certainly intractable at cosmic scales. The consequence is that the paper's quantitative framework for evaluating futures ignores that the optimization it describes cannot be computed or implemented with any physically realizable governance system.