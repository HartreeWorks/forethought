[The Second-Order Catastrophist] Suppose the paper succeeds and influential actors adopt its framework, prioritizing deliberate optimization toward "near-best futures" defined by the multiplicative model. The first consequence is that any group confident they have identified the value-maximizing configuration of resources becomes justified in imposing that configuration at cosmic scale, since failure to do so sacrifices most potential value. The paper's own logic—that achieving only 99th percentile rather than 99.99th percentile outcomes loses most value—creates a mandate for extremism among those who believe they know the correct answers to population ethics, digital welfare, and resource allocation. The framing of eutopia as requiring "essentially no bads at all" on separately aggregating bounded views provides philosophical cover for totalitarian elimination of whatever powerful actors classify as "bads." The consequence is that widespread adoption of the paper's framework doesn't merely fail to achieve eutopia—it provides sophisticated justification for imposing particular visions of the good at the expense of pluralism, consent, and the epistemic humility the authors elsewhere recommend.