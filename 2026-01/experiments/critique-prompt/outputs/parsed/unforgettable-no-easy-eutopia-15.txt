"The Selection Effect Blindspot" identifies how the paper's examples are systematically biased. The paper lists potential future catastrophes (wrong population ethics, wrong digital welfare attitudes, etc.) that each seem capable of destroying most value. But these examples are selected precisely because they're the kind of thing moral philosophers worry aboutâ€”they represent the tail of our uncertainty distribution. The paper never considers that most features of a future civilization might be morally neutral or convergently good, such that the catastrophe-prone dimensions are rare. The paper would need to show that catastrophe-prone dimensions are actually common among the dimensions that determine value, not just that such dimensions exist and we've noticed some.