The paper proposes a target—"near-best futures" within narrow tolerance bands—without any analysis of fault tolerance, redundancy, or graceful degradation. In engineering terms, the multiplicative value model implies a system where any single component failure causes total system failure, the most fragile architecture possible. Real resilient systems are designed with redundancy precisely because some components will fail; the paper's moral framework has none. What happens when one civilization in one star system makes a "wrong" population ethics choice—does this condemn the entire cosmic future to sub-eutopia? The paper provides no mechanism for error correction, rollback, or isolation of failures. The framing of success as requiring correctness across "digital welfare, population ethics, variety/diversity" and other dimensions simultaneously implies a system with no margin for error and no recovery path. The consequence is that the paper describes a design requirement (achieve eutopia) without a feasible implementation architecture, like specifying a bridge that must never experience stress without explaining how to build one.