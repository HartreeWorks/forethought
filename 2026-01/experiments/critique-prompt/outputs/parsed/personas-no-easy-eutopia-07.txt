[The Adversarial Red-Teamer] The paper's value-theoretic framework creates an exploitable attack surface for any sophisticated adversary seeking to manipulate collective decision-making about the future. Consider an actor who wants to prevent space settlement for competitive reasons: they need only promote "environmentalist" moral views (which the paper acknowledges might regard "widespread settlement of other star systems...as a moral catastrophe") to create paralyzing moral uncertainty. The paper's own uncertainty analysis shows that different normalization methods yield dramatically different recommendationsâ€”an adversary could selectively fund philosophical research supporting whichever normalization method produces paralysis or their preferred outcome. More directly, the paper's acknowledgment that "motivated cognition (including biased training of AI advisors)" could produce convenient moral views is an explicit blueprint for adversarial manipulation. The consequence is that the paper's sophisticated framework for evaluating futures becomes a weapon for those who understand how to shape the inputs to that framework.