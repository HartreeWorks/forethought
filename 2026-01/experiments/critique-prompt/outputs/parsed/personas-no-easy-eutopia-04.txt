The paper's extended discussion of how "future decision-makers" might adopt correct moral views and act on them systematically ignores that any institution powerful enough to steer civilization toward eutopia will be captured by those who benefit from the status quo. The authors note that "motivated cognition (including biased training of AI advisors)" could lead to convenient moral views, but treat this as a contingent risk rather than the inevitable outcome of regulatory dynamics. Consider their example of digital being welfare: any board, agency, or constitutional provision that adjudicates AI rights will be staffed and funded by entities whose profits depend on treating AI as property. The "truth-seeking deliberative processes" mentioned in the conclusion are precisely the kind of procedural idealism that masks how actual institutions become compliance theatersâ€”producing ethical certifications that legitimate existing power arrangements. The consequence is that the paper's optimism about "forces which guide society towards hitting" the eutopian target ignores that those forces will themselves become instruments of the interests they were meant to constrain.