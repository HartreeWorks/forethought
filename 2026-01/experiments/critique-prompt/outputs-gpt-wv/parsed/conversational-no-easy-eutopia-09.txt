> The paper's "no coordinated optimization" baseline quietly assumes civilisation won't use its powerful new AI tools for moral progress, making "mostly-great is rare" a claim about an oddly self-handicapped world.

**"The paper under-specifies what counts as 'no serious coordinated optimization'—and that premise may be unrealistic post-AGI"**  
A central definitional move is evaluating futures conditional on survival *and* on “no serious coordinated efforts to promote the overall best outcomes de dicto.” But if transformative AI arrives, even modestly-aligned AI assistants could constitute precisely such coordinated optimization pressure by enabling planning, moral reflection, institutional design, and preference aggregation at scale. If you grant that AGI changes cognitive and coordination capacity (a Tier 1-ish background premise), “default” futures may not resemble today’s muddling-through at all. That means the distribution the paper is sampling from may be the wrong reference class, making “mostly-great is rare” a statement about a world where civilization oddly fails to use its new cognitive machinery. If the authors intend a “values are optimized, but for parochial/selfish aims” default, they should model that explicitly because it changes the target geometry (e.g., fewer random errors but stronger lock-in to a particular objective). As written, the baseline is doing a lot of work while staying ambiguous.