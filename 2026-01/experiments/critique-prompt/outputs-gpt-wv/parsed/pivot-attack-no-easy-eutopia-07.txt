> Even without anyone explicitly "aiming for the good," competitive pressures in an AI world naturally push toward stable welfare norms, so the paper's "default" may already be far more optimised than assumed.

**“‘No De Dicto Optimization’ is Not the Default Once AI Exists”** — This attacks **Pivot P4: the operational definition of fussiness as “mostly-great futures are very unlikely on a ‘default’ distribution conditional on survival and no serious de dicto optimization pressure,”** which is load-bearing because the headline “no easy eutopia” ultimately targets what happens *without deliberate aiming*. In an AI-driven future, powerful optimization pressure arises endogenously from mundane incentives (profit, security, status, ideological competition), and that pressure will heavily shape institutions, moral discourse, and enforcement mechanisms even if nobody says “we optimize for the good de dicto.” Many of the paper’s failure modes (digital slavery, massive suffering externalities, lock-in of unreflective values) are precisely the kinds of outcomes that competitive optimization can *also* counteract if they cause instability, rebellion, inefficiency, or reputational/international conflict—so “default” may not be a low-optimization regime at all. If the true default is “strong optimization toward locally stable, widely-legible welfare/rights norms” (even if imperfect), then the probability mass on mostly-great futures rises substantially, undermining the claim that the target is narrow in practice. If this objection holds, the paper must either justify that endogenous optimization systematically pushes toward the wrong attractors (and why), or else restrict its conclusion to a contrived low-optimization counterfactual rather than the world we are actually heading toward.