> The very optimization pressure needed to hit a narrow eutopian target also amplifies lock-in risk—the paper's own worst failure mode—so "narrow target, push harder" can backfire.

**“Optimization Lock‑In Paradox”** — The paper argues that *because eutopia is a narrow target, hitting it likely requires deliberate de dicto optimization/steering* **(B)** from the claim that *many fussy views punish single mistakes and fat tails reward extreme optimization* **(A)**, because *more optimization pressure increases the chance of landing in the tiny near-best region* **(M)**. But in the Forethought framework this is self-undermining: the same steering instruments (centralized planning, powerful AI advisers, institutional hardening, coordinated moral projects) are exactly the mechanisms that raise lock-in and power concentration risk—failure modes that the paper itself treats as capable of erasing most value. If “fussy” means “one wrong lock-in sinks you,” then increasing optimization pressure can *increase* expected value loss by increasing the probability and irreversibility of a wrong irreversible commitment, even if it also increases the conditional quality given success. This isn’t a generic “be careful with power” worry; it directly targets the paper’s implied policy moral: “narrow target → push harder,” by showing the sign can flip once you include endogenous lock-in as part of the same fussy landscape. If this objection holds, the paper can’t simply conclude that flourishing work deserves more priority because the ceiling is high; it must model the variance/irreversibility introduced by optimization and show that the net effect is positive under the very fragility assumptions it advances. To change the argument, the paper would need to distinguish *reversible, pluralistic, error-correcting* optimization (which might widen the basin) from *centralized, lock-in-prone* optimization (which might shrink it), and then defend that the former is the realistic path.