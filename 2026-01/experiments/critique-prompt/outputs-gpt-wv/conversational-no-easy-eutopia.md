1. **"The 'product of factors' model smuggles in fragility"**  
The paper’s core intuitive engine is that future value is multiplicative across many “independent” dimensions, making near-best outcomes rare. But multiplicativity is not a neutral default; it effectively builds in the conclusion that one severe flaw can dominate everything else. Many plausible moral aggregators are closer to additive-with-thresholds, leaky minima, or “diminishing penalties,” where big wins in one domain partially compensate for shortfalls elsewhere. Even within pluralistic ethics, people often endorse lexical priorities only for a small set of constraints (e.g., extreme suffering), not dozens of roughly-equal multipliers. Without a principled argument for *why* the right moral view (or robust moral uncertainty policy) should behave multiplicatively across many axes, the toy model risks being more rhetoric than inference. A related issue is that “mostly-great = 50% of best feasible” becomes hostage to the chosen functional form rather than tracking a stable intuition.

2. **"Independence assumptions are doing hidden work"**  
The paper repeatedly appeals to “relatively independent” factors (welfare theory, population ethics, digital rights, space allocation, etc.) to justify multiplicative compounding. Yet in realistic governance trajectories, many of these variables are strongly correlated via common causes like institutional quality, deliberation norms, and alignment/epistemics. Better institutions that handle digital minds well are also more likely to handle space property rights well; societies that reflect carefully about welfare are also likelier to adopt better population policies. Positive correlation weakens the “one miss ruins everything” distributional claim and can make “good futures cluster” more than the product model implies. Conversely, if correlation is negative (tradeoffs), the paper should model *explicit Pareto frontiers* rather than independent draws. Either way, the independence framing seems under-argued given how much it drives the “narrow target” conclusion.

3. **"The 'best feasible future' benchmark is epistemically unstable"**  
Defining “best feasible” as the 99.99th percentile of a “well-informed distribution” over futures makes the central thresholds (0.5, 0.9) extremely sensitive to how that distribution is constructed. In heavy-tailed domains (which the paper itself emphasizes), percentile-based benchmarks can move dramatically when you expand the outcome space, add speculative technologies, or revise cosmic assumptions. That makes “mostly-great is rare” partly an artifact of how wide you cast the hypothesis class, not purely a claim about steering difficulty. It also risks circularity: the more you believe in vast, weird upside, the higher the benchmark rises, and the harder “mostly-great” becomes by definition. A more decision-relevant metric might compare futures to *robust aspiration levels* (e.g., avoiding certain classes of moral catastrophe) rather than percentile-of-a-model benchmarks.

4. **"From 'many moral views condemn the present' to 'eutopia is hard' is a weak inference"**  
The historical observation—most societies were morally blind to major wrongs—supports humility, but it doesn’t directly imply that high-tech abundance futures are unlikely to resolve many wrongs by default. Some moral failures were sustained by coordination problems, scarcity, and lack of empathy/knowledge; those drivers plausibly weaken with prosperity, education, and transparency tools. The fact that moral perspectives disagree today also doesn’t show that *future* reflection won’t converge on a set of reforms that score well across many perspectives. The argument risks equivocating between “we might be wrong about something big” and “therefore near-best is a tiny target.” You’d need a more concrete model of moral learning dynamics, not just a catalog of contemporary disagreements. Otherwise the paper may be selecting evidence (moral dissensus) that is equally consistent with “progress is lumpy but strong once capability and information rise.”

5. **"Linear unbounded views are treated as more default than justified"**  
A major plank is that for unbounded linear views, missing galaxy-scale expansion forfeits almost all value, making “common-sense utopia” look tiny. But whether value is effectively linear in cosmic resources depends on contested population ethics, welfare comparability across radically different minds, and whether the marginal value of additional lives/compute saturates. Many people (including many consequentialists) accept some form of diminishing returns, person-affecting constraints, variable critical levels, or reasons to prioritize quality/rights over sheer quantity. The paper acknowledges bounded/concave possibilities, but often argues they are “approximately linear in practice” by invoking the huge universe and other civilizations—claims that import additional empirical and normative assumptions. If linearity is doing the work, the paper needs a tighter case that linear-ish, resource-separable views deserve dominant credence under moral uncertainty rather than being one family among many. Otherwise “no easy eutopia” partially reduces to “if total utilitarianism-like views are right, scale matters a lot,” which is less novel and less decisive.

6. **"The bounded-view critique leans too hard on controversial cosmic premises"**  
When arguing that many bounded views become “in-practice linear,” the paper relies on the universe being enormous (possibly infinite), with many alien civilizations, making humanity’s marginal contribution tiny so concave functions localize to linear. But if (i) the Great Filter is ahead, (ii) interstellar life is rare, (iii) expansion is hard, or (iv) moral concern is domain-relative (e.g., special obligations, relational views, or “our light cone” ethics), then the “tiny marginal difference” premise can fail. Even staying longtermist, you can accept astronomical stakes without assuming we are a rounding error relative to an already-value-saturated cosmos. The paper’s move here risks turning a normative conclusion (“easygoing bounded views are unlikely”) into a hostage of highly uncertain cosmology/astrobiology. A critic could argue that under plausible uncertainty, bounded concavity could remain practically relevant and make “mostly-great” easier than claimed.

7. **"The 'separate aggregation makes bads dominate' result is model-dependent and potentially alarmist"**  
The dramatic claim that as little as one part in \(10^{22}\) of resources going to “bads” prevents a mostly-great future hinges on symmetric bounding choices and on treating “a star system’s worth of bads” as automatically half-maximal disvalue. But moral bads are not naturally measured in “resource share,” and disvalue may not scale linearly with physical extent if the bad is localized, low-intensity, or avoidable by design. Many ethical views treat some harms as lexically severe (torture) but not all “bads” as fungible mass terms that aggregate like energy. Also, governance mechanisms could quarantine, compensate, or end bad states quickly, changing the relevant metric from “fraction of cosmos that is bad” to “total integrated suffering,” which may be tractably minimized. The upshot is that the “tiny fraction of bads ruins everything” conclusion looks like an artifact of a particular aggregation + scaling story, not a robust property of bounded ethics.

8. **"The fat-tailed 'value-efficiency' argument lacks a bridge from tails to target narrowness"**  
Even if value-per-resource is fat-tailed across possible uses of matter/compute, it doesn’t follow that *realistic* futures without deliberate optimization land far from the tail. Human and institutional selection pressures already push toward efficiency on many dimensions (health, technology, preference satisfaction), and advanced AI systems might intensify that by default. Moreover, fat tails often imply that a few opportunities dominate, but also that there are *many near-winners* (thick upper tail), which can make “good enough” easier than “the unique optimum.” The paper sometimes slides from “the best is vastly better than the median” to “most futures miss most value,” but the latter requires assumptions about how likely societies are to stumble into the upper tail. Without an explicit model of search/optimization dynamics (who searches, with what objective, under what constraints), fat-tailedness alone doesn’t establish a narrow eutopian target. In fact, the same fat-tailed structure can motivate “there will be lots of big wins to pick up” rather than “almost everything is tragically mediocre.”

9. **"The paper under-specifies what counts as 'no serious coordinated optimization'—and that premise may be unrealistic post-AGI"**  
A central definitional move is evaluating futures conditional on survival *and* on “no serious coordinated efforts to promote the overall best outcomes de dicto.” But if transformative AI arrives, even modestly-aligned AI assistants could constitute precisely such coordinated optimization pressure by enabling planning, moral reflection, institutional design, and preference aggregation at scale. If you grant that AGI changes cognitive and coordination capacity (a Tier 1-ish background premise), “default” futures may not resemble today’s muddling-through at all. That means the distribution the paper is sampling from may be the wrong reference class, making “mostly-great is rare” a statement about a world where civilization oddly fails to use its new cognitive machinery. If the authors intend a “values are optimized, but for parochial/selfish aims” default, they should model that explicitly because it changes the target geometry (e.g., fewer random errors but stronger lock-in to a particular objective). As written, the baseline is doing a lot of work while staying ambiguous.

10. **"Intertheoretic comparison skepticism cuts against the paper’s practical confidence"**  
The final sections concede that intertheoretic value comparisons are “thorny” and that normalization choices can flip which strategies look best (safety-focused vs upside-focused). But earlier conclusions (e.g., easygoingness is unlikely; fussy views dominate) are presented with more confidence than the uncertainty machinery seems to warrant. If moral uncertainty is severe enough that different plausible normalization schemes reverse recommendations, then claims like “we should expect fussy views to loom larger” need stronger justification than informal plausibility. In particular, the argument that “fair” comparisons often make unbounded views higher-stakes is itself contested in the moral uncertainty literature, and alternative approaches (e.g., bargaining/Parfitian meta-ethics, parliamentary models, maximin constraints, or imprecise credences) can mute the dominance of fussy/unbounded theories. This matters because the headline conclusion (“no easy eutopia”) is practically meaningful only if it survives reasonable meta-ethical variance. Otherwise the paper may be best read as “conditional on certain moral-uncertainty frameworks, eutopia is hard,” which is a narrower and less action-guiding claim.