**Load-bearing claim attacked**: The paper claims algorithmic efficiency gains mean "labs can increase the quantity of both key inputs: cognitive labor and # experiments." **Attack type: Parameter sensitivity.** This treats "algorithmic efficiency" as a single parameter that uniformly reduces compute requirements. But efficiency gains are heterogeneous: some reduce inference costs (helping cognitive labor), some reduce training costs (helping experiments), some reduce only specific architectures, some trade off with capabilities. The paper assumes efficiency gains during an SIE would disproportionately help experiments, but the optimization pressure would favor gains that improve the AGI researchers themselves (inference efficiency). If 80% of efficiency gains flow to inference and 20% to training, the "both inputs grow" claim is quantitatively much weaker than presented, and the experiment bottleneck remains binding.