> Compute might bottleneck AI progress not because it's a scarce CES input, but because there's a physical limit on how many bits of useful information each FLOP can extract.

**Load-bearing claim:**
The paper's core framework assumes AI R&D can be decomposed into "cognitive labor" and "compute" as separable CES inputs

**Attack type:**
Countermodel

Consider a world where AI progress is bottlenecked by *information* rather than either input—specifically, information that can only be extracted through compute-intensive experiments and cannot be predicted, compressed, or inferred. In this world, cognitive labor helps design experiments and interpret results, but the *rate of information extraction* is physically bounded by compute. Adding more cognitive labor lets you extract information more efficiently but cannot exceed the bits-per-FLOP limit of learning. This model has the same premises (experiments need compute, cognitive labor is abundant) but the conclusion differs: max speed is determined by information-theoretic limits, not CES complementarity, and could be much lower than 30x regardless of ρ.
