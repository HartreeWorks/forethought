> Algorithmic efficiency gains may not symmetrically benefit both the AI researchers and their experiments, since architectural improvements require retraining the researchers themselves before they can be applied.

**Load-bearing claim:**
"If your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed)," which the paper claims undermines the fixed-input assumption

**Attack type:**
Equilibrium shift

This argument assumes efficiency gains are symmetric across training and inference. But during an SIE, the algorithms being improved are the same ones running the AGI researchers. If the efficiency gains come from architectural changes that require retraining, you cannot apply them to your own researchers without a training run. If they only apply to inference, then the experiments (training runs) don't become more efficientâ€”only the cognitive labor becomes cheaper. The paper implicitly assumes efficiency gains flow primarily to experiments, but strategic optimization would likely prioritize gains that improve the researchers themselves, creating a dynamic where the "both inputs grow" claim doesn't hold for the bottleneck-relevant input.
