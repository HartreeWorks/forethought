**[The Mechanism Designer]** The paper hand-waves that "experiments can become more compute-efficient" and that labs can "extrapolate from experiments that use increasingly small fractions of compute," but provides no formal specification of what this extrapolation function looks like or when it breaks down. What is the mathematical relationship between experiment scale and information gained about frontier performance? At what point does the extrapolation error exceed the signal? The claim that "the number of near-frontier experiments has significantly decreased while algorithmic progress continued" is offered as evidence, but without a formal model of how information from sub-frontier experiments transfers to frontier predictions, this proves nothingâ€”maybe we've just been lucky, or maybe we're already accumulating extrapolation debt that will come due. Until someone specifies the actual extrapolation mechanism and its failure conditions, "you might not need near-frontier experiments" is wishful thinking dressed as analysis.