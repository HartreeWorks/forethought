> The historical trend of unslowed algorithmic progress doesn't actually test the bottleneck hypothesis, which only kicks in when cognitive labor massively outstrips available near-frontier experiments—precisely the SIE scenario.

**Load-bearing claim:**
The paper dismisses the "near-frontier experiments" reply by noting that "training run size has grown much faster than the world's total supply of AI compute" without slowing algorithmic progress

**Attack type:**
Quantitative cliff

This historical observation covers a period where the *absolute* number of near-frontier experiments remained sufficient (labs could still run dozens of large experiments per year). The relevant question is whether algorithmic progress scales with the *ratio* of near-frontier experiments to ideas-to-test. During an SIE with 10^5 AGI researchers generating ideas, the number of near-frontier experiments becomes the binding constraint regardless of historical trends. The paper treats "algorithmic progress didn't slow down" as evidence against the bottleneck, but the bottleneck hypothesis specifically predicts it won't bind until cognitive labor dramatically exceeds current levels—exactly the SIE scenario being analyzed.
