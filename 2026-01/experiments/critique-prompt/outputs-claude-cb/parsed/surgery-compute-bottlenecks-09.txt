**Load-bearing claim:**
The paper dismisses the "near-frontier experiments" reply by noting that "training run size has grown much faster than the world's total supply of AI compute" without slowing algorithmic progress

**Attack type:**
Quantitative cliff

This historical observation covers a period where the *absolute* number of near-frontier experiments remained sufficient (labs could still run dozens of large experiments per year). The relevant question is whether algorithmic progress scales with the *ratio* of near-frontier experiments to ideas-to-test. During an SIE with 10^5 AGI researchers generating ideas, the number of near-frontier experiments becomes the binding constraint regardless of historical trends. The paper treats "algorithmic progress didn't slow down" as evidence against the bottleneck, but the bottleneck hypothesis specifically predicts it won't bind until cognitive labor dramatically exceeds current levelsâ€”exactly the SIE scenario being analyzed.
