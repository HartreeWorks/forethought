1. "The Reconfiguration Paradox": The paper argues that short-run estimates of ρ showing strong complementarity don't apply because AGIs could quickly "reconfigure AI R&D to make good use of abundant cognitive labor," citing Jones (2003) on how production processes adapt over time. But this creates a paradox: if reconfiguration is itself a cognitive task that AGIs can accelerate, then the paper is assuming the conclusion—that cognitive labor can substitute for compute—in order to prove that cognitive labor can substitute for compute. The reconfiguration hypothesis requires that the meta-task of finding new production processes is not compute-bottlenecked, but there's no reason to grant this special exemption. If finding better algorithms requires experiments, then finding better ways to organize research also plausibly requires experiments. If this objection holds, the paper cannot appeal to rapid reconfiguration without separately establishing that organizational innovation in AI R&D has different complementarity properties than algorithmic innovation.

2. "The Extrapolation Sword Cuts Both Ways": The paper argues we should distrust economic estimates of ρ because they're measured over ~2× variation in L/K and extrapolating to multiple OOMs is "heroic." But this same reasoning undermines the paper's own positive claims about what happens at high cognitive labor inputs. The author's intuitions about "max speed" being implausibly low below 30× are also extrapolations from current experience—we've never observed what happens when you add even 10× more researchers to AI R&D, let alone trillions of superintelligent ones. The paper selectively applies extrapolation skepticism: dismissing empirical estimates as unreliable for extrapolation while treating armchair intuitions about the "specific things you could do with abundant cognitive labor" as reliable guides to behavior multiple OOMs away. If extrapolation is unreliable, the paper's confidence that ρ lies between -0.2 and 0 is equally unfounded.

3. "The Algorithmic Efficiency Treadmill": The paper's fifth counterargument claims that as algorithms become more efficient, labs can run more experiments per unit compute, so both inputs grow together. But this assumes algorithmic efficiency gains are "free" cognitive outputs that don't themselves require the compute-intensive experiments being studied. In reality, discovering that your algorithms are 2× more efficient likely required running experiments at near-frontier scale. The paper treats efficiency gains as a way to escape the compute constraint, but these gains are themselves the output variable (Y) in the CES model, not an independent input. This is accounting sleight-of-hand: you can't use the thing you're trying to produce as an input to producing more of it without circularity. If efficiency gains require compute to discover, then "more experiments at fixed capability" is purchased with the same scarce resource the bottleneck objection identifies.

4. "The Implausibility Bootstrap": The paper rejects economic estimates because they imply "max speeds" the author finds "implausibly low" (below 10-30×), and uses this intuition to constrain ρ to between -0.2 and 0. But this methodology is circular: the paper determines plausible max speeds by "thinking through, and talking to AI researchers about, the specific things you could do with abundant cognitive labor." These researchers have never operated with abundant cognitive labor—they're extrapolating from current conditions. The paper is effectively saying "economic estimates must be wrong because my intuitions, formed under compute-scarce conditions, suggest higher substitutability." There's no independent ground truth here; the author's intuitions about what abundant cognitive labor could achieve are exactly what's in question. If this objection holds, the paper needs an independent method for establishing plausible max speeds that doesn't rely on intuitions shaped by the current regime.

5. "The Strongest Link Fallacy": Counterargument 7 reframes AI R&D as a "strongest link" problem where you only need one non-bottlenecked route to succeed, against the CES model's "weakest link" structure. But this framing is inconsistent with how the paper treats the SIE mechanism. The SIE requires a feedback loop where improvements compound—AI improves algorithms, better algorithms improve AI, etc. For this loop to accelerate, the marginal route to improvement must itself not be bottlenecked. Finding one non-bottlenecked route only helps if that route remains non-bottlenecked as you iterate through the loop multiple times. The paper gives no reason to think that whichever route happens to have favorable ρ at step N will still have favorable ρ at step N+100. The "strongest link" framing might justify one-time gains but cannot justify sustained acceleration without additional assumptions about the durability of that link.

6. "The Survey Mismatch": The paper cites a survey finding that moving from median to top researchers would increase progress 6×, using this as evidence that "smarter researchers" dramatically increase output. But this finding, if anything, supports complementarity rather than substitutability. The survey measures quality variation among humans who all operate under the same compute constraints—it says nothing about what happens when you hold compute fixed and add more smart researchers. A 6× gain from quality at fixed quantity is perfectly consistent with strong complementarity between cognitive labor (however skilled) and compute. The paper needs evidence about what happens when you vary quantity/quality of researchers while holding compute fixed, not evidence about quality differences at current compute levels. This survey actually cannot distinguish between the paper's position and Epoch's.

7. "The Definitional Escape Hatch": The paper concludes with a 10-40% probability of an SIE, defined as ">=5 OOMs of increase in effective training compute in <1 year without needing more hardware." But "effective training compute" is defined by algorithmic efficiency gains, which the paper earlier argued scale with cognitive labor inputs (counterargument 5). This definition allows the paper to count any algorithmic progress as evidence for the SIE, regardless of whether it required compute-heavy experiments. The paper has essentially defined SIE in terms of the output metric (effective compute) rather than the mechanism (feedback loop acceleration), making the claim nearly unfalsifiable. A skeptic who believes algorithmic progress will continue but plateau at some multiple of current speed would still see "effective training compute" increase—just not accelerate. The paper's framing obscures the actual crux about acceleration dynamics.

8. "The Serial Dependency Blindspot": The paper lists ways cognitive labor could substitute for compute: "running smaller scale experiments, optimising every part of the stack, generating way better ideas, designing way better experiments, stopping experiments early." But these activities have serial dependencies that cognitive labor cannot parallelize away. You cannot "design way better experiments" until you've seen results from previous experiments; you cannot "stop experiments early" without running the early parts; "generating way better ideas" requires feedback on which ideas worked. Even with infinite cognitive labor, the wall-clock time for sequential experiment-feedback loops remains constrained by experiment runtime, which scales with compute. The paper treats cognitive labor as if all research activities are parallelizable, but the information dependencies in empirical research create serial bottlenecks that no amount of parallel cognition can eliminate.

9. "The Manufacturing Disanalogy That Proves Too Much": The paper argues that economic estimates from manufacturing overstate complementarity because manufacturing lacks analogues to "running smaller scale experiments" and "optimizing every part of the stack." But manufacturing has its own substitution mechanisms—process innovation, lean production, automation of inspection—that economists have presumably captured in their ρ estimates. If manufacturing ρ is still -0.4 despite these substitution mechanisms, this suggests that domains generally have lower substitutability than naive lists of "things you could do with more labor" would suggest. The paper assumes AI R&D's substitution mechanisms are qualitatively more powerful than manufacturing's, but provides no evidence for this beyond assertion. The existence of domain-specific substitution mechanisms in manufacturing that still yield low ρ estimates is evidence against, not for, the paper's optimism about AI R&D.

10. "The Late-Stage Irrelevance Problem": The paper concludes that compute bottlenecks "are unlikely to block an SIE in its early stages, but could well do so after a few OOMs of progress." But the entire motivation for worrying about SIE—stated in the intro—is that "you could shoot from human-level to superintelligent AI in a few months." If compute bottlenecks kick in "after a few OOMs," they would prevent exactly the scenario the paper frames as catastrophic. A world where AI improves 2-3 OOMs then plateaus is radically different from unbounded acceleration, yet the paper's conclusion treats this as a minor concession. The paper's own analysis, even granting all its assumptions, suggests that the "massive consequences" scenario requires ρ very close to zero—the most optimistic end of the paper's already-optimistic range. The paper has argued itself into a position where its headline claim (SIE is plausible) depends on parameters being at the extreme favorable end of its stated distribution.