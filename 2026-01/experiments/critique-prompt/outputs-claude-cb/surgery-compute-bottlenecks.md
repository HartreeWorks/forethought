## Argument Map

**Thesis**: Compute bottlenecks are unlikely to prevent a software intelligence explosion in its early stages, and there's a 10-40% chance an SIE occurs despite compute bottlenecks.

**Load-bearing claims**:
1. The CES model's ρ parameter for AI R&D is likely higher (closer to 0) than economic estimates from manufacturing/other sectors suggest
2. Economic estimates of ρ from other domains shouldn't transfer to AI R&D because AI R&D has unique properties (smarter workers matter more, experiments can become more efficient, multiple routes exist)
3. Long-run estimates showing ρ≈0 are more applicable than short-run estimates showing ρ<0
4. The "max speed" implications of economic ρ estimates (2-100x) are implausibly low given what superintelligent researchers could achieve
5. Algorithmic efficiency gains allow increasing both inputs (cognitive labor AND effective experiments), undermining the fixed-input assumption

**Dependencies**:
- The thesis depends on (1), which depends on (2), (3), and (4) as supporting arguments
- (5) is presented as an independent defeater of the bottleneck objection
- The "early stages" qualifier depends on the claim that even if -0.2<ρ<0, bottlenecks only bite after ~5 OOMs of progress

**Hidden load**:
- Assumes AI R&D progress can be meaningfully decomposed into separable inputs of "cognitive labor" and "compute"
- Assumes the SIE dynamic involves cognitive labor scaling while compute remains relatively fixed
- Assumes insights from talking to current AI researchers about what superintelligent researchers could do are reliable
- Assumes algorithmic efficiency gains during an SIE would be applicable to the experiments themselves (not just deployed models)

---

1. **Load-bearing claim attacked**: The paper argues that long-run economic estimates showing ρ≈0 should be weighted heavily because "Cobb Douglas is a good model for long-run growth." **Attack type: Reference class failure.** The Jones (2003) hypothesis explains long-run ρ≈0 via decades-long industrial reorganization—building new factories, retraining workforces, developing new supply chains. The paper assumes AGIs could accomplish equivalent "reconfiguration" of AI R&D in "days or weeks," but this analogy fails because the long-run economic estimates bundle together not just process optimization but also the *physical capital accumulation* that occurs over decades. When the economy adjusts long-run, K isn't held fixed—it grows alongside the new production processes. The paper is citing evidence for "L and K both grow and reorganize together" to support "L can grow alone and reorganize around fixed K." If this critique holds, the appeal to long-run estimates as supporting ρ≈0 under fixed compute collapses.

2. **Load-bearing claim attacked**: "If your AI algorithms become twice as efficient, you can run twice as many experiments (holding the capability level of AI in those experiments fixed)," which the paper claims undermines the fixed-input assumption. **Attack type: Equilibrium shift.** This argument assumes efficiency gains are symmetric across training and inference. But during an SIE, the algorithms being improved are the same ones running the AGI researchers. If the efficiency gains come from architectural changes that require retraining, you cannot apply them to your own researchers without a training run. If they only apply to inference, then the experiments (training runs) don't become more efficient—only the cognitive labor becomes cheaper. The paper implicitly assumes efficiency gains flow primarily to experiments, but strategic optimization would likely prioritize gains that improve the researchers themselves, creating a dynamic where the "both inputs grow" claim doesn't hold for the bottleneck-relevant input.

3. **Load-bearing claim attacked**: The paper claims max speeds of <30x are "implausible" based on reasoning about what superintelligent researchers could do (running smaller experiments, optimizing every part of the stack, etc.). **Attack type: Causal reversal.** The same evidence—that current AI progress has been rapid despite limited cognitive labor—could support the conclusion that we're *already* extracting most of the cognitive-labor-substitutable gains. If "optimizing every part of the stack" and "designing better experiments" were high-ρ activities, we'd expect current variations in researcher quality to show larger effects. The paper cites a 6x improvement from median-to-top researchers, but this is within the range the economic estimates would predict as achievable. The absence of observed 100x+ productivity differences among human researchers, despite enormous IQ variation, suggests the bottleneck is indeed binding, not that we haven't tried hard enough.

4. **Load-bearing claim attacked**: The paper dismisses the "near-frontier experiments" reply by noting that "training run size has grown much faster than the world's total supply of AI compute" without slowing algorithmic progress. **Attack type: Quantitative cliff.** This historical observation covers a period where the *absolute* number of near-frontier experiments remained sufficient (labs could still run dozens of large experiments per year). The relevant question is whether algorithmic progress scales with the *ratio* of near-frontier experiments to ideas-to-test. During an SIE with 10^5 AGI researchers generating ideas, the number of near-frontier experiments becomes the binding constraint regardless of historical trends. The paper treats "algorithmic progress didn't slow down" as evidence against the bottleneck, but the bottleneck hypothesis specifically predicts it won't bind until cognitive labor dramatically exceeds current levels—exactly the SIE scenario being analyzed.

5. **Load-bearing claim attacked**: The paper argues economic estimates don't account for "smarter workers" and cites a survey suggesting 6x gains from median-to-top researcher quality. **Attack type: Parameter sensitivity.** The 6x estimate spans only the human range. Extrapolating from "median human to top human = 6x" to "top human to superintelligence = huge gains" assumes intelligence returns are roughly linear or better across this range. But if the 6x gain from median-to-top reflects the *entire* cognitive contribution (with the rest being experiment-bound), then superintelligence offers no additional gains—the curve has already flattened. The paper treats this 6x as evidence for high ρ, but it's equally consistent with a model where cognitive contributions max out quickly and the remaining variance is experiment-limited.

6. **Load-bearing claim attacked**: The paper's core framework assumes AI R&D can be decomposed into "cognitive labor" and "compute" as separable CES inputs. **Attack type: Countermodel.** Consider a world where AI progress is bottlenecked by *information* rather than either input—specifically, information that can only be extracted through compute-intensive experiments and cannot be predicted, compressed, or inferred. In this world, cognitive labor helps design experiments and interpret results, but the *rate of information extraction* is physically bounded by compute. Adding more cognitive labor lets you extract information more efficiently but cannot exceed the bits-per-FLOP limit of learning. This model has the same premises (experiments need compute, cognitive labor is abundant) but the conclusion differs: max speed is determined by information-theoretic limits, not CES complementarity, and could be much lower than 30x regardless of ρ.

7. **Load-bearing claim attacked**: The paper argues there are "multiple routes to superintelligence" (extrapolation, scaffolding, data flywheels) and we only need one non-bottlenecked route. **Attack type: Equilibrium shift.** If multiple routes exist with different ρ values, and labs optimize for whichever route has highest ρ, then *current* observed progress already reflects this optimization. The revealed preference of frontier labs—which heavily invest in larger training runs rather than purely scaling researcher headcount—suggests the high-ρ routes have already been exploited or don't scale far. During an SIE, the same optimization would occur, but the fact that compute-intensive approaches dominate today implies the compute-light alternatives hit walls that abundant cognition can't overcome. The "strongest link" framing assumes we haven't already found and exhausted the best links.

8. **Load-bearing claim attacked**: The paper dismisses economic estimates because "extrapolating very very far" from observed L/K variation of ~2x to OOM changes is "truly heroic." **Attack type: Causal reversal.** This critique applies symmetrically to the paper's own reasoning. The paper's confidence that ρ is between -0.2 and 0 is based on informal reasoning about what superintelligent researchers "could do"—an extrapolation far beyond any observed researcher capability. If CES extrapolation is invalid for the skeptic, intuition-based extrapolation about superintelligent capabilities is equally invalid for the advocate. The paper cannot reject the skeptic's methodology while relying on even more speculative extrapolation for its own estimates.

9. **Load-bearing claim attacked**: The paper claims algorithmic efficiency gains mean "labs can increase the quantity of both key inputs: cognitive labor and # experiments." **Attack type: Parameter sensitivity.** This treats "algorithmic efficiency" as a single parameter that uniformly reduces compute requirements. But efficiency gains are heterogeneous: some reduce inference costs (helping cognitive labor), some reduce training costs (helping experiments), some reduce only specific architectures, some trade off with capabilities. The paper assumes efficiency gains during an SIE would disproportionately help experiments, but the optimization pressure would favor gains that improve the AGI researchers themselves (inference efficiency). If 80% of efficiency gains flow to inference and 20% to training, the "both inputs grow" claim is quantitatively much weaker than presented, and the experiment bottleneck remains binding.

10. **Load-bearing claim attacked**: The paper's final probability estimate (10-40% chance of SIE despite compute bottlenecks) is anchored on ρ being "likely between -0.2 and 0." **Attack type: Countermodel.** Construct a scenario where ρ is indeed -0.1 (within the paper's favored range), yet SIE fails for reasons orthogonal to CES dynamics. Suppose software improvements hit *capability walls* rather than compute walls—certain capabilities (robust reasoning, novel scientific insight, self-improvement) require architectural innovations that cannot be found through any amount of cognitive labor applied to current paradigms. The CES model describes input substitutability, not the existence of output ceilings. A high ρ means cognitive labor can substitute for compute, but says nothing about whether the resulting "progress" includes the specific capabilities needed for recursive self-improvement. If SIE requires capabilities that are paradigm-gated rather than effort-gated, the entire CES framing is orthogonal to the actual bottleneck.