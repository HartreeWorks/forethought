The paper’s model of "reflection" (Section 2.3.1) relies on a naive Cartesian view of cognition where reasoning serves to uncover truth rather than justify prior instincts. Empirical evidence suggests that human (and likely intelligent agent) reasoning is largely a tool for social signaling and post-hoc rationalization. Increasing cognitive abundance via AI won’t necessarily lead to "better" moral views; it is just as likely to produce hyper-sophisticated rationalizations for base drives or initially random priors. By assuming that "reflection" cleans away bias rather than entrenching it with higher-fidelity arguments, the authors mistakenly predict convergence when the cognitive reality suggests a divergence into increasingly complex, mutually unintelligible, and self-justifying dogmas.