1. **Node attacked:** “*We can apply the CES formula to AI R&D… Y represents the pace of AI software progress*” (Economist version). **Attack type:** Hidden parameter. **Break mechanism:** The CES mapping treats “pace of progress” as a smooth production output from two fungible inputs (cognitive labor and compute), but AI R&D speed has a serial critical path (train → evaluate → iterate) whose wall-clock is often dominated by the longest runs and queueing, not by total “labor”. In a world where each iteration requires a fixed-duration near-frontier training cycle (weeks) and only a small fraction of work is parallelizable, L→∞ doesn’t raise iteration rate much even if “insight” is abundant—so your inference from “ρ near 0 ⇒ no early compute bottleneck” can fail even if compute isn’t “fixed” in the CES sense. The bottleneck is not substitutability but *latency*, which CES doesn’t represent. **If this holds, you’d need to replace or augment CES with a pipeline/latency model (critical-path + parallelizable fraction) and show that the serial fraction is small enough for your “early stages” conclusion to go through.**

2. **Node attacked:** “*I’ll assume α=0.5 throughout*” (Economist version, multiple places). **Attack type:** Hidden parameter. **Break mechanism:** Your “max speed” ceilings (e.g., 6× at ρ=-0.4; 32× at ρ=-0.2) are extremely sensitive to α, because α effectively encodes how compute-intensive the marginal progress function is; but you treat it as a neutral simplification. A plausible counterworld is that near-frontier algorithmic progress is overwhelmingly compute-driven (α close to 1 in the “compute” term if K corresponds to experiments), in which case even slightly negative ρ yields a much tighter ceiling than your plotted intuition suggests, especially in the “early stages” you claim are safe. Conversely, if α is small early and large later, the “compute bottleneck bites late” claim can reverse. **If this holds, you’d need to justify α with a task decomposition of AI R&D (what fraction of marginal progress requires training/eval cycles vs pure thought/coding) and run sensitivity analyses over α(L,K) rather than fixing it.**

3. **Node attacked:** “*in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute… Cognitive labour can in principle fully substitute for compute!*” (Counterargument 2.3). **Attack type:** Countermodel. **Break mechanism:** “Doing the math in their heads” is still computation performed on a physical substrate; unless you assume “cognitive labor” is magically compute-free, this just relocates compute from “experiment machines” to “AGI brains.” In a physically realistic world, the AGIs’ thinking consumes the same scarce FLOPs/energy/memory bandwidth that you counted as K, so the apparent substitution is double-counting: L rises only by spending more K. Under that accounting, ρ can remain <0 even “in principle,” because the relevant constraint is total available physical computation, not whether it is labeled “researcher cognition” or “training run.” **If this holds, you’d need to explicitly define K as *total physical compute budget including inference/thinking* and re-argue substitutability under that unified constraint (or else show why “AGI cognition” draws from a different, non-competing resource).**

4. **Node attacked:** “*when your AI algorithms become twice as efficient, you can run twice as many experiments… This completely pulls the rug out… which assumed an essential input was held fixed*” (Counterargument 5). **Attack type:** Reversal. **Break mechanism:** The same mechanism can imply the opposite dynamic: efficiency gains frequently get spent on pushing the frontier to larger models (capability-seeking labs scale up), keeping the *effective* “near-frontier experiment count” roughly constant or even *reducing* it because each run becomes longer and more complex to evaluate. In that world, algorithmic efficiency doesn’t relax the bottleneck; it intensifies complementarity by making “insight” more valuable only when paired with massive, scarce runs (ρ more negative). Your “rug pull” requires assuming labs bank efficiency as “more shots on goal” rather than reinvesting it into bigger bets. **If this holds, you’d need to model the equilibrium allocation rule for compute under competitive scaling (how efficiency translates into run size vs run count) and show conditions where run-count truly increases at the relevant margin.**

5. **Node attacked:** “*you might not need near-frontier experiments… you might be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute*” (Counterargument 5.2). **Attack type:** Countermodel (quantitative cliff). **Break mechanism:** There are plausible phase-transition regimes where small-scale experiments are systematically misleading about large-scale behavior (emergent capabilities, optimization instabilities, data-curation interactions, tool-use failures), so extrapolation fails exactly where the high-stakes algorithmic decisions lie. In such a world, the *marginal* value of cognition without near-frontier validation collapses, creating a hard ceiling even if lots of useful work happens at small scale. Then “compute bottlenecks don’t bite early” becomes false once you reach the regime where qualitative behaviors appear only at scale. **If this holds, you’d need to argue (with concrete research workflows) why the key algorithmic improvements during an SIE are predictable from sub-frontier tests, or else incorporate a “scale-dependent validity” term that reinstates a compute bottleneck.**

6. **Node attacked:** “*Over the past ten years, the number of near-frontier experiments… decreased… If these… were truly a bottleneck… progress would have slowed down*” (Counterargument 5.2). **Attack type:** Dominant alternative. **Break mechanism:** The observation is also consistent with a world where near-frontier experiments *are* bottlenecked, but overall progress was carried by other changing margins you don’t control for (larger single runs, better data pipelines, better hardware utilization, transfer learning, open-source spillovers, and—crucially—evaluation standards shifting). Fewer near-frontier experiments can coexist with rapid progress if each one produces more learning because model families are more standardized and scaling laws make single large runs unusually informative. Your inference implicitly assumes a stable “learning-per-frontier-run” and stable evaluation difficulty across time; if those moved, the historical argument doesn’t identify the bottleneck. **If this holds, you’d need to decompose historical progress into (a) number of frontier-equivalent experiments, (b) information gained per experiment, and (c) shifting goalposts, and then show that (a) is not the limiting factor even after controlling for (b) and (c).**

7. **Node attacked:** “*Economic estimates don’t include labourers becoming smarter or thinking faster… Takeaway: raise our estimate of ρ*” (Counterargument 4). **Attack type:** Reversal. **Break mechanism:** Smarter/faster agents can increase *complementarity* with compute rather than reduce it: they generate more candidate hypotheses, architectures, and training recipes that require expensive validation, thereby increasing the demand for scarce compute per unit time. In a world where idea generation scales superlinearly with cognition but verification remains compute-bound, the “smarter workers” effect makes the compute bottleneck bite *earlier*, not later (ρ effectively becomes more negative at higher L). Your direction-of-update (“raise ρ”) assumes cognition substitutes for experiments, but it can instead amplify the verification burden. **If this holds, you’d need to distinguish “cognition that replaces experiments” from “cognition that increases the hypothesis throughput,” and argue that the former dominates during an SIE—ideally with a model where verification cost scales with idea volume.**

8. **Node attacked:** “*The implied ‘max speed’ … between 2 and 100… I think a max speed below 10 is implausible… informed by… optimising every part of the stack, generating way better ideas…*” (Counterargument 6). **Attack type:** Reference class sabotage (and hidden parameter). **Break mechanism:** Your “implausibly low” judgment leans on a reference class of software optimization where iteration is cheap, but frontier AI R&D includes non-software frictions that don’t scale with more cognition: cluster scheduling, hardware faults, distributed training brittleness, eval-suite construction, red-teaming, interpretability work, and integration testing. If those dominate the cycle time, then even “trillions of superintelligent researchers” don’t buy 10× speed because the binding constraints are organizational/operational and physical. That makes the low max-speed implications of negative ρ *plausible* rather than absurd, undermining your key move of rejecting most economic ρ estimates by intuition. **If this holds, you’d need to anchor “max speed” with an explicit breakdown of the end-to-end iteration loop (what fraction is parallelizable vs latency-bound vs compute-bound) and show why those non-idea frictions don’t cap speed below your thresholds.**

9. **Node attacked:** “*There are multiple possible routes… a ‘strongest link’ framing… We’ll ultimately use whichever has the most favourable ρ*” (Counterargument 7). **Attack type:** Strategic response (Goodhart/selection effects). **Break mechanism:** Selecting the “route with the most favorable ρ” can systematically select for methods that *appear* to work without compute-heavy validation but are actually under-validated, leading to brittle or deceptive systems that later require massive compute to debug, align, or verify—reintroducing the compute bottleneck at the worst time. In a world with strong incentives to accelerate, labs may prematurely adopt low-validation approaches (synthetic data flywheels, speculative architecture changes, aggressive distillation) that shift costs from “experiment compute” to “post-hoc failure recovery compute,” so the true bottleneck is displaced, not removed. Your argument treats route choice as a free optimization without adversarial incentives or hidden failure costs. **If this holds, you’d need to incorporate an endogenous “validation/verification” constraint and show that the fastest route under competitive pressure is still robustly non-compute-bottlenecked when accounting for downstream correction costs.**

10. **Node attacked:** “*I’ll define an ‘SIE’ as ‘>=5 OOMs of increase in effective training compute in <1 years without needing more hardware’*” (Taking stock). **Attack type:** Quantitative cliff (accounting inconsistency). **Break mechanism:** “Effective training compute” folds multiple margins (algorithmic efficiency, data efficiency, architectural changes, tooling) into a single scalar, but your own discussion repeatedly treats compute as both the bottleneck input K and something that can be “effectively increased” via software—creating a moving-target denominator. A counterworld: you achieve large “effective compute” gains only by shifting costs to inference-time search, larger context, heavier tool use, or more extensive evaluation; total physical compute remains binding, so the “no more hardware” clause fails under full-system accounting even if training FLOPs drop. Also, if you instantiate “millions of AGIs,” their inference/thinking compute can dominate the budget, making the constraint tighter exactly during the putative explosion. **If this holds, you’d need to specify a conservation-style compute budget over (training + inference for researchers + evaluation + deployment externalities), define “effective compute” in a way that can’t be inflated by shifting work across buckets, and then re-check whether 5 OOM in <1 year is achievable under that unified constraint.**