1. **“The CES Proxy Breaks the Object of Study”** (attacks **Pivot P1: “Apply a CES production function where L=cognitive labour, K=compute, Y=pace of AI software progress; then for −0.2<ρ<0 compute bottlenecks won’t bite until late stages.”** This pivot is load-bearing because the paper’s “late-stage only” claim is derived almost entirely from CES-shaped diminishing returns.) The CES mapping assumes “pace of progress” behaves like an aggregate production output with smooth marginal rates of substitution, but AI R&D progress is often *thresholdy* (e.g., a single training run enabling a new regime) and mediated by discrete bottlenecks (cluster scheduling, evaluation suites, data pipelines) that don’t average out. If the true relationship is closer to a “minimum of (ideas, validated experiments)” or a queueing system where experiments are the service bottleneck, then even small complementarity can create *hard caps* well before 5 OOM of “effective training compute.” Under that alternative, the sensitivity plot and “early stages unaffected” inference no longer follows, and compute scarcity can slow the loop as soon as automated researchers scale faster than experiment throughput. What would need to change is a model grounded in the mechanics of experiment throughput/information gain (e.g., queueing + value-of-information) that reproduces the “late-stage only” conclusion without assuming CES smoothness.

2. **“Constant ρ Is Doing Hidden Heavy Lifting”** (attacks **Pivot P1: “Within −0.2<ρ<0 the predictions don’t differ much until cognitive labour grows by ~5 OOMs, so compute bottlenecks probably don’t block early SIE.”** This is load-bearing because the conclusion hinges on *stage-invariance* of the substitutability.) The paper treats ρ as roughly stable across massive regime change, but in AI R&D the complementarity can *increase* precisely when automation makes idea-generation cheap: the marginal idea becomes more dependent on costly validation, integration, and large-scale failure-mode discovery. If ρ endogenously drifts more negative as L explodes (because “cheap ideas” saturate and the remaining ones are increasingly empirical), then the “won’t bite until late” result can flip: compute becomes binding *earlier*, not later, even if −0.2<ρ<0 held in today’s regime. Concretely, an SIE defined as ≥5 OOM “effective training compute” in <1 year could stall after 1–2 OOM if the marginal returns become validation-dominated once automated labor scales. What would need to change is an explicit mechanism for ρ(L/K) (or a non-CES model) plus evidence from historical ML where marginal algorithmic gains became increasingly tied to large runs as low-hanging fruit was exhausted.

3. **“Reconfiguration Isn’t a Purely Cognitive Move”** (attacks **Pivot P2: “Jones-style reconfiguration could be very quick with fast-thinking AGIs… days or weeks,” raising effective ρ toward 0.** This pivot is load-bearing because it’s the key escape hatch from short-run complementarity.) The claim assumes the main barrier to reorganizing AI R&D around abundant cognition is *planning*, but many frictions are compute- and infrastructure-coupled: implementing new training regimes requires repeated full-stack trials, kernel/hardware co-tuning, data reprocessing, long-horizon evals, and reliability testing that consume the very compute purportedly fixed. If that reconfiguration itself requires substantial near-frontier or long-duration runs (e.g., to validate scaling stability, emergent behaviors, or safety-critical generalization), then abundant AGI labor can’t compress the timeline much—compute remains the gating factor. In that world, the paper’s “late-stage only” bottleneck claim fails because the system hits “reconfiguration validation” bottlenecks early, before any rapid compounding. What would need to change is a decomposition showing which reconfiguration steps are truly compute-light versus compute-heavy, and empirical analogues from recent ML infra shifts (e.g., MoE, long-context, RLHF pipelines) with measured compute/time requirements.

4. **“Mental Simulation Smuggles in Compute”** (attacks **Pivot P2: “In the limit of infinite AGIs you could… fully simulate computational experiments using cognitive labour in place of compute… meaning ρ<0 is flawed in the absolute limit.”** This pivot is load-bearing because it underwrites the intuition that compute can be substituted away, pushing ρ toward 0.) Simulating neural network training “in their heads” doesn’t bypass computation; it just relocates it to whatever physical substrate runs those AGIs, which is still constrained by FLOPs, memory bandwidth, and energy—i.e., *compute* in the relevant sense. If the substrate is the same hardware pool, then “more AGIs” doesn’t create extra physical operations; it partitions the same compute budget, worsening contention rather than substituting for it. Once you remove this substitution intuition, it becomes entirely coherent that even superintelligent researchers face a hard cap from fixed experimental compute, making negative ρ plausible in the regime the paper cares about. What would need to change is replacing the “cognition can substitute for compute” argument with a physically grounded account of what parts of R&D can be advanced without additional executed operations (e.g., theorem-proving style progress) and showing those parts dominate the pathway to superintelligence.

5. **“Efficiency Gains Don’t Automatically Buy More Experiments”** (attacks **Pivot P3: “When algorithms become twice as efficient, you can run twice as many experiments… labs can increase both cognitive labour and # experiments, pulling the rug out from the compute-bottleneck argument.”** This is load-bearing because it’s the paper’s main rebuttal to ‘K fixed’.) The rebuttal assumes efficiency improvements translate into *experiment throughput* at a fixed capability level, but during an SIE the objective is not “more experiments at yesterday’s scale”—it’s discovering changes that work at (or near) frontier regimes where behavior shifts discontinuously. Historically, major efficiency gains (better optimizers, architectures, sparsity) are frequently “spent” on pushing scale/capability, not on multiplying independent experimental shots; the organization still ends up running a small number of massive runs whose wall-clock and risk dominate iteration speed. If that pattern persists, then fixed compute still binds the rate at which frontier hypotheses can be validated, and the paper’s “bottlenecks unlikely to slow early stages” conclusion collapses. What would need to change is an accounting identity (with data) that partitions compute into (a) hypothesis-testing runs, (b) capability-scaling runs, and (c) integration/debugging, showing that efficiency gains plausibly expand (a) fast enough to sustain ≥5 OOM “effective training compute” progress within a year.

6. **“The Near-Frontier Decline Argument Misdiagnoses the Past”** (attacks **Pivot P3: “Near-frontier experiments can’t be the bottleneck, because over 10 years the number of near-frontier experiments decreased yet algorithmic progress didn’t slow.”** This pivot is load-bearing because it dismisses the skeptic’s ‘frontier validation is fixed’ counter.) The historical observation is compatible with an alternative story: progress may have been driven by a *small number* of near-frontier runs (each producing many downstream papers/products) plus a large ecosystem of *non-frontier* work that was only useful because those few frontier anchors existed. That doesn’t imply near-frontier runs won’t bottleneck an SIE—automation changes the ratio by making hypothesis generation explode, so the constraint can newly bind even if it didn’t in the human-labor regime. If, in an SIE, the limiting factor becomes “how many frontier anchors can we afford per unit time,” then fixed compute can choke the feedback loop quickly, undermining the “few OOMs before slowing” claim. What would need to change is a retrospective attribution study estimating “marginal algorithmic progress per additional near-frontier run” over time, and projecting how that marginal value scales when cognitive labor increases by orders of magnitude.

7. **“Small-Scale Extrapolation Fails Exactly Where SIE Needs It”** (attacks **Pivot P4: “You might not need near-frontier experiments… you might extrapolate from experiments that use increasingly small fractions of the lab’s compute.”** This is load-bearing because it’s the key move that lets progress accelerate without increasing K.) Many of the failure modes that matter for frontier capability—training instabilities, emergent tool use, deception risks, long-context pathologies, data contamination effects, distributed training quirks—are weak or absent at small scale and appear nonlinearly. If extrapolation is unreliable, then each serious algorithmic candidate requires near-frontier validation, reinstating compute as a hard gate and making ρ effectively more negative than the paper’s preferred range. Concretely, the “≥5 OOM effective training compute in <1 year” definition becomes implausible under fixed hardware because the number of required frontier validations rises with the number of generated ideas. What would need to change is evidence from recent frontier-era ML showing that a large fraction of major improvements were predictable from ≤1% scale tests, with quantified transfer reliability (not just anecdotal plausibility).

8. **“The ‘Strongest-Link’ Escape Routes Are Compute-Entangled”** (attacks **Pivot P4: “There are multiple possible routes… scaffolding, data flywheel, better experiments… compute bottleneck only works if all routes are compute-bottlenecked.”** This pivot is load-bearing because it claims there will exist at least one high-ρ route.) The proposed routes are not independent of compute in the way the argument needs: scaffolding quality depends on base-model capability (trained with large compute), data flywheels require massive inference and filtering (still bounded by fixed hardware-time), and “designing better experiments” ultimately needs executed runs to validate generalization. If all candidate routes converge on a shared need for high-trust, high-scale validation, then “pick the most favorable ρ” doesn’t help—the frontier-validation complementarity reappears across routes. In that case, the paper’s “compute bottlenecks don’t slow early stages” conclusion fails because the strongest-link still runs through the same compute gate. What would need to change is a concrete pathway to large capability gains that is demonstrably dominated by compute-light work (e.g., formal methods/theory or guaranteed-transfer modular improvements) and an argument that it can compound to superintelligence without frequent frontier-scale checks.

9. **“‘Max Speed Is Implausibly Low’ Is a Mirror of the Conclusion”** (attacks **Pivot P5: “Economic ρ estimates imply max speed 2–100×; max speed below ~10–30× is implausible; therefore economic estimates likely overstate bottlenecks and −0.2<ρ<0 is most likely.”** This pivot is load-bearing because it justifies discounting the main quantitative objection from economics and supports the 10–40% probability.) The “implausibly low” judgment is driven by an inventory of things abundant cognitive labor *could* do (optimize stack, early stopping, better ideas), but it doesn’t show those actions translate into proportional *validated progress* once you account for integration costs, regressions, and the need to confirm gains at relevant scales. If, in reality, the binding constraint is “number of high-confidence, high-scale validations per unit time,” then a 6× or even 10× ceiling is not implausible—it’s exactly what you’d expect under fixed compute and sequential validation. If that’s true, the paper’s posterior (“good chance bottlenecks don’t slow until late; 10–40% SIE despite bottlenecks”) is too high because the economics-based low max-speed world remains live. What would need to change is a quantitative upper-bound argument (or simulation) translating proposed labor-driven improvements into expected progress per unit of fixed compute, with uncertainty bands wide enough to honestly compare against ρ≈−0.4 worlds.

10. **“The SIE Target Metric Quietly Reintroduces Hardware Limits”** (attacks **Pivot P5: “Define SIE as ≥5 OOM increase in effective training compute in <1 year without more hardware; compute bottlenecks probably don’t block this until late stages.”** This pivot is load-bearing because the headline probability is stated in this metric.) With fixed hardware, the total available FLOPs in a year is capped; achieving 5 OOM “effective training compute” implies ~100,000× aggregate efficiency on the tasks that matter, but many efficiency gains require tradeoffs that are not free (more memory, more communication, more wall-clock, or degraded robustness) and may not stack multiplicatively. If the realistic ceiling on software-only efficiency within a year is, say, 10–1,000× (already aggressive), then the paper’s SIE definition becomes unattainable regardless of how favorable ρ is, and the conclusion that compute bottlenecks wait until “late stages” becomes moot—physical throughput binds almost immediately. In that scenario, the 10–40% probability estimate is inflated because it assigns substantial mass to worlds where software produces five orders of magnitude of “effective compute” without corresponding physical throughput. What would need to change is either (a) a capability-based SIE definition decoupled from “effective compute,” or (b) a stepwise efficiency roadmap showing how specific, known bottlenecks (optimizer, architecture, data efficiency, eval, systems) plausibly compound to 100k× within the time/throughput budget of fixed hardware.