1. **"Your ‘labor’ is made of compute, so holding compute fixed makes labor fictional."** The paper treats cognitive labor \(L\) and compute \(K\) as separable inputs, then asks what happens as \(L \to \infty\) with \(K\) fixed. But in the setting at issue—automated AI R&D—every marginal unit of “cognitive labor” is literally additional inference/training cycles run on the same compute substrate; an extra AGI researcher is not a worker you can hire without buying the CPU that runs its brain. If you truly hold \(K\) fixed while adding “more AGIs,” you are implicitly dividing the same finite compute among more agents, so per-agent thinking speed collapses and \(L\) doesn’t actually rise in any meaningful sense. This is not a parameter-tweak; it removes the entire conceptual lever the argument pulls on (substitution) because the variables aren’t independent degrees of freedom. Any defense (“assume ultra-cheap inference” or “run them slower”) concedes the point: you’ve quietly smuggled in either more \(K\) or less \(L\), making the anti-bottleneck conclusion an artifact of a mis-specified input space.

2. **"CES is a production function; you’re modeling a search process as if it were a factory."** The core move maps \(Y\) (output) to “pace of AI software progress,” and then imports the CES notion of a ceiling on \(Y\) when inputs are complementary. But R&D progress isn’t produced by contemporaneous factor mixing; it is a stochastic search over hypotheses with feedback loops, path dependence, and winner-take-all discoveries where marginal returns are governed by exploration dynamics, not static substitution elasticities. In a search process, you can have long flat periods followed by discontinuous jumps from one conceptual breakthrough, meaning there may be no well-defined “max speed” that depends smoothly on \(L\) and \(K\) the way CES assumes. This isn’t “CES is imperfect”; it’s that the object \(Y(L,K)\) the paper needs may not exist as a stable function at all. If the author defends by saying “it’s a rough prior,” then the later quantitative claims (e.g., max speeds of 6× vs 30× vs 100×) are revealed as numerology: the model’s outputs have no bearing on the phenomenon being argued about.

3. **"The ‘simulate NNs in their heads’ escape hatch detonates your own definition of compute bottleneck."** The paper’s key rebuttal to \( \rho<0 \) leans on the idea that with enough cognitive labor, AGIs could “do the math for NNs in their heads” and thus substitute for compute, so a hard compute bottleneck can’t exist “in principle.” But “doing the math in their heads” is just computation performed somewhere else; for AI agents those “heads” are still hardware executing operations, i.e., compute. This is a load-bearing metaphor treated as a factual counterexample: it confuses relocating compute with eliminating compute, and thereby dissolves the bottleneck by redefining compute out of existence. That’s not a rebuttal to compute constraints; it’s a tautology (“compute isn’t limiting because we can compute”). If the author tries to defend by insisting cognitive labor could be non-digital (humans, wetware), they’ve abandoned the paper’s scenario of software R&D automation driving the explosion—switching substrate is exactly “additional hardware” by another name.

4. **"Algorithmic efficiency doesn’t give ‘more experiments’; it makes each experiment more expensive in information terms."** The paper claims that because algorithms can become more compute-efficient, you can run more experiments at fixed compute, undermining the skeptic’s “compute is fixed” premise. But the quantity that matters isn’t “number of runs,” it’s the information gained per unit compute about near-frontier behavior—and as you approach the frontier, the signal-to-noise of cheap proxies typically collapses (distribution shift, emergent behaviors, scaling-law breaks). In other words, making models cheaper often forces you into smaller, less faithful regimes, and the marginal value of each added cheap experiment can fall faster than linearly, restoring a hard ceiling on *effective* experimentation even if raw run-count increases. This is a structural reversal: the very move proposed to escape compute bottlenecks (shift to smaller experiments) is exactly what makes you blind to the phenomena that determine frontier capability. If the author defends with “we can extrapolate,” they’ve exposed an unstated crux: that extrapolation remains reliable through regime changes—the one thing a compute bottleneck skeptic denies.

5. **"Your ‘near-frontier experiments are declining yet progress continued’ point supports the skeptic, not you."** The paper argues that since near-frontier experiment count has decreased over the past decade while progress continued, near-frontier experiments can’t be the bottleneck. But the skeptic’s story is precisely that researchers *respond to compute scarcity by reallocating effort toward algorithmic tricks, scaling-law exploitation, and better engineering*—i.e., progress becomes dominated by whatever is feasible under a compute constraint. Observing continued progress under tightening frontier-experiment budgets is evidence of adaptation under constraint, not evidence the constraint is irrelevant; it’s compatible with a world where compute is still the binding resource that shapes the research agenda. This is a reversal result: your observation is exactly what you’d expect if compute bottlenecks are real and researchers are forced into a narrower, compute-compatible path. If the author defends by saying “adaptation can be fast in an SIE,” that concedes the skeptic’s core point: compute scarcity dictates the feasible research frontier; it doesn’t disappear.

6. **"The ‘max speed is implausibly low’ argument is pure incredulity dressed up as calibration."** The paper dismisses \( \rho<-0.3 \) largely because it implies a max progress speed under fixed compute that “seems implausible,” based on conversations and a list of plausible optimizations. But the whole question is whether those optimizations are compute-bound or idea-bound at the margin; asserting they exist does not show they can be executed without consuming the scarce resource (compute for training, evals, ablations, data generation, safety testing, etc.). More importantly, “drop in trillions of God-like AIs” is not a neutral thought experiment: it invites you to imagine capabilities that secretly smuggle in the ability to predict empirical outcomes without running them—exactly the disputed bottleneck. This isn’t a fixable empirical gap; it’s a non-argument that replaces the key parameter with vibes and then uses the vibes to reject inconvenient parameter values. If the author tries to defend by making the thought experiment more concrete, they’ll be forced to quantify which steps require how much compute—reintroducing the very ceiling they tried to wave away.

7. **"You attack ‘heroic extrapolation’ and then immediately bet your conclusion on an even more heroic extrapolation."** The paper argues that using CES estimates from small \(L/K\) variation to predict many-orders-of-magnitude changes is “truly heroic,” suggesting we should be very wary of such inference. But the paper’s bottom line still depends on choosing a narrow favorable range \(-0.2<\rho<0\) and then extrapolating that range through 5+ OOMs of labor increase to claim compute bottlenecks “won’t bite until late stages.” That is the same extrapolation move, just with a hand-picked parameter justified by plausibility arguments rather than data. The critique isn’t “you need better estimates”; it’s that the paper invalidates its own inferential machinery and then uses it anyway to produce policy-relevant probabilities (10–40%). If the author defends by saying “it’s only a guess,” then the paper’s central purpose—defusing the compute bottleneck objection—collapses into an admission that the objection remains standing.

8. **"Your ‘smarter/faster researchers’ point double-counts compute and erases the bottleneck by definition."** The paper says economic estimates of \(\rho\) miss “smarter workers” and “faster thinking,” and therefore understate substitutability in AI R&D. But for AI systems, faster thinking and smarter inference are not free: they require either more compute per unit time (higher throughput) or more compute per thought (larger models / longer reasoning traces), both of which intensify the compute constraint rather than relax it. Treating “faster thinking” as an increase in \(L\) that doesn’t draw on \(K\) is exactly the mistake a compute bottleneck argument targets. This isn’t a minor modeling tweak; it flips the sign of the claimed effect—smarts and speed can *tighten* the compute bottleneck by increasing the compute required to reach a given level of research output quality. If the author defends by positing algorithmic improvements that make inference cheaper, they’re back in the circularity trap: those improvements themselves must be discovered and validated under the bottlenecked regime.

9. **"The strongest-link escape (‘many routes not bottlenecked by compute’) is vacuous unless you name one route that survives contact with reality."** The paper argues the compute bottleneck objection only works if *all* routes to superintelligence are compute-bottlenecked, implying that one non-compute-intensive route could carry an SIE. But this “strongest link” framing is an existence proof without an existence argument: it doesn’t specify any concrete path from current ML to “>=5 OOM effective training compute in <1 year” that doesn’t require extensive empirical validation at or near the frontier. In modern ML, even “non-training” routes (scaffolding, data flywheels, architecture changes) still bottleneck on evaluation, robustness testing, and distribution-shift probing—each demanding substantial compute to avoid self-deception. The move isn’t optimistic; it’s unfalsifiable: any observed compute constraint can be waved away by gesturing at an unspecified alternative route. If the author defends by proposing a candidate route, they inherit the burden they’ve been avoiding: to show it doesn’t secretly require the compute-heavy experiments the objection is about.

10. **"Your definition of SIE (‘effective training compute’) lets you declare explosion without intelligence, and dodge the actual bottleneck."** The paper operationalizes SIE as “>=5 OOM increase in effective training compute in <1 year without more hardware,” equating algorithmic efficiency gains with an “intelligence explosion.” But effective compute is not capability: you can get huge paper gains in efficiency metrics by narrowing task distribution, exploiting benchmark artifacts, or shifting to architectures that are cheaper but brittle—none of which implies the ability to automate AI R&D at superhuman levels. This definition makes the conclusion easier by redefining the target into something that can rise on paper while the actual system-level constraint—reliable, frontier-generalizing performance under real-world distribution shift—still demands expensive training and evaluation. That’s a vacuous-truth failure mode: the thesis becomes true in a world where “effective compute” explodes but the promised feedback loop (AI improving AI) stalls because the systems aren’t robust enough to replace researchers. If the author defends by saying “effective compute correlates with capability,” they must confront the exact crux compute-bottleneck skeptics press: correlation breaks at regime changes, and demonstrating it holds is itself compute-intensive.