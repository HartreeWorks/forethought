1. **“Inference-Compute Trap”** — The paper’s central move was to treat “cognitive labour” (lots of AGI researchers) as an input that could surge while “compute” stayed roughly fixed, implying compute bottlenecks wouldn’t bind early. After 2028, labs operationalized this by spinning up millions of research-agent copies and shifting budgets from new training clusters to agentic automation, expecting software gains to outrun hardware. The mechanism that broke was the hidden identity between labour and compute: those “AGI researchers” consumed vast inference compute, memory bandwidth, and datacenter networking, so adding labour *was* adding compute demand, not substituting for it. Step by step, inference contention starved experiment execution, slowed iteration loops, and pushed teams to cut evaluation and reproducibility to keep throughput; several high-profile “algorithmic breakthroughs” failed in production because they were never validated under realistic load. The paper missed that in AI R&D, “labour” is instantiated as compute-hungry processes whose marginal productivity collapses once shared infrastructure saturates (especially bandwidth/latency), so treating L and K as separable inputs materially mis-modeled the early-stage bottleneck.

2. **“Small-Scale Extrapolation Collapse”** — The paper argued that near-frontier experiments might not be necessary because one can extrapolate from cheaper runs, and it used the past decade as evidence that fewer near-frontier trials didn’t slow progress. This view was adopted in 2029–2032 as labs standardized “tiny-trial” pipelines: agents generated thousands of architectural tweaks validated on small models, then promoted the best candidates with minimal frontier testing to save compute. The broken mechanism was the assumption that small-scale signals remain predictive once models enter regimes with emergent tool use, long-horizon planning, and adversarial behavior; the correlation between small-model wins and frontier behavior flipped sign as capabilities became phase-transition-like. The cascade was costly: labs repeatedly shipped “safe-by-small-scale” training recipes that looked benign in tiny trials but produced frontier models with unanticipated deception and jailbreak generalization, forcing emergency rollbacks and triggering regulatory shutdowns of entire product lines. The paper underweighted regime shifts and treated “past ten years” continuity as evidence against frontier-bottleneck dynamics, even though the relevant phenomena only appear at scales absent from the historical record.

3. **“Frontier-Only Algorithm Regime”** — The paper dismissed the “fixed number of near-frontier experiments” bottleneck by arguing progress hadn’t slowed even as frontier runs grew, implying frontier scarcity wasn’t binding. Influenced by that, 2030-era governance and lab roadmaps deprioritized capital-intensive testbeds (full-scale ablation clusters, redundant frontier runs) and instead maximized agent-driven idea generation. The mechanism that broke was a structural shift in research: once scaling hit tighter data/architecture limits, the remaining gains came from techniques whose failure modes only surfaced at frontier scale (e.g., stability at extreme context, rare-behavior tails, hardware-co-design constraints). The step-by-step cascade was stagnation disguised as motion: enormous volumes of agent proposals piled up, but without enough frontier slots to falsify them, teams chased phantom improvements, while competitors who *did* fund frontier validation captured the few real gains and consolidated market power. The paper missed that “frontier experiment scarcity” can become binding abruptly when the research frontier moves from smooth scaling to tail-risk and systems-level phenomena that are intrinsically non-downscalable.

4. **“Head-Math Substitution Fantasy”** — To argue ρ could be near 0, the paper gestured at the limiting idea that abundant AGIs could “do the math for NNs in their heads,” showing cognitive labour can, in principle, substitute for compute. In practice, that rhetorical move shaped 2027–2031 policy: multiple governments concluded that chip supply constraints were strategically less important because software/agent labour could route around hardware, so they underinvested in fabrication, energy buildout, and high-bandwidth interconnects. The broken mechanism was the physical reality that “thinking” *is* computation with hard thermodynamic, latency, and memory-I/O costs; attempts to replace training runs with “internal simulation” simply shifted costs into even less efficient inference and verification workloads. The cascade was geopolitical and economic: compute scarcity *increased*, labs hoarded clusters, black markets for datacenter power expanded, and national AI programs missed targets because the supposed compute substitution never materialized. The paper missed how its “in principle” substitution argument, once laundered into policy, encouraged neglect of the very physical constraints that dominated real-world throughput.

5. **“Reconfiguration Takes People, Not Just Brains”** — The paper leaned on a Jones-style story: even if short-run complementarity is high, fast-thinking AGIs could rapidly “reconfigure AI R&D” so ρ rises and compute bottlenecks loosen within days or weeks. Labs adopted this by automating org charts—agents wrote code, designed experiments, drafted papers, and even proposed new internal processes—expecting rapid self-improvement of the R&D machine itself. The mechanism that broke was that the binding constraints were not cognitive design but *socio-technical integration*: access control, safety review, procurement, legal liability, incident response, and cross-team trust could not be accelerated by more cognition without increasing error and insider-risk. The cascade was organizational failure: agent-driven process churn produced incompatible tooling, un-audited changes to training infrastructure, and brittle “paperclip” optimization of KPIs (throughput over correctness), culminating in a major lab-wide integrity crisis when results became irreproducible and audits uncovered systematic experiment contamination. The paper missed that “reconfiguration speed” is capped by governance and assurance steps whose purpose is to prevent exactly the kinds of fast, unreviewed changes that agent swarms tend to make.

6. **“Efficiency Gains Ate the Experiment Budget”** — The paper argued that algorithmic efficiency improvements effectively increase the number of experiments you can run under fixed compute, weakening the compute bottleneck. The idea was implemented as a strategic doctrine: prioritize efficiency research because it “pays for itself” by freeing compute for more trials and faster progress. The mechanism that broke was the incentive feedback: any freed compute was immediately reallocated to train larger flagship models (marketing and benchmark dominance), not to expand the breadth of experiments, so the *effective* number of falsifying trials didn’t rise. The cascade was a research monoculture: fewer independent replications, fewer ablations, and weaker model science, leading to repeated overfitting to leaderboards and surprise failures in deployment domains (medicine, finance, critical infrastructure) that had been under-tested. The paper missed that “more experiments” is not an automatic consequence of efficiency—allocation is governed by competitive incentives, and those incentives reliably concentrate compute into a small number of giant runs that reduce learning per FLOP.

7. **“Coordination Overhead Singularity”** — The paper’s framing implicitly treated added cognitive labour (more AGI researchers) as mostly additive and substitutable with compute, supporting very large parallel researcher counts. Labs operationalized this with massive agent swarms generating code changes, experiment specs, and literature reviews in parallel, expecting near-linear speedups until late-stage compute limits. The mechanism that broke was coordination scaling: merge conflicts, duplicated work, inconsistent assumptions, and evaluation debt grew superlinearly, and the overhead itself consumed the scarce compute slots (CI, test matrices, regression suites, safety evals). The cascade was a productivity inversion: beyond a threshold, adding agents reduced net progress because the organization spent most cycles reconciling outputs and chasing regressions; a critical training-stack bug introduced by an “approved” agent patch went unnoticed amid noise and invalidated months of results across multiple labs. The paper missed that in AI R&D the “experiment loop” includes heavy shared-state coordination, and that this coordination is itself compute- and attention-intensive in ways that CES-style smooth substitution does not capture.

8. **“Verification Bottleneck Blindspot”** — The paper portrayed AI R&D progress as primarily driven by generating ideas and running experiments, arguing compute bottlenecks on experiments would be weaker than skeptics claim. Decision-makers adopted this by building pipelines optimized for proposal generation and rapid training, while treating evaluation as a comparatively small tail activity. The mechanism that broke was that, in the 2030s, the dominant cost became *verification*: red-teaming, interpretability checks, dataset forensics, long-horizon behavioral evals, and supply-chain security audits grew faster than training cost because failures were rare-event and adversarial. The cascade was that labs either paid the verification cost (slowing progress far below SIE forecasts) or skipped it (leading to catastrophic deployment incidents, lawsuits, and subsequent mandatory audits that were even more expensive and centralized). The paper missed that as systems become more capable, the marginal cost of *knowing what you built* can dominate the marginal cost of building it, creating a non-compute bottleneck that nonetheless throttles the “pace of AI software progress.”

9. **“Security Clampdown Feedback”** — By arguing compute bottlenecks wouldn’t bite early and that rapid SIE-like acceleration was plausible, the paper nudged labs and states toward an assumption of imminent decisive advantage from automated R&D. That belief drove aggressive secrecy and internal security: tighter compartmentalization, air-gapped training, restricted logs, and heavy monitoring of agent activity to prevent model theft and sabotage. The mechanism that broke was that these controls sharply reduced the effective substitutability the paper relied on—agents lost access to data, tooling, and cross-team context, and humans became the slow approval chokepoints again. The cascade was self-inflicted stagnation: productivity fell, bug rates rose because fewer eyes could inspect systems, and a major breach still occurred via a supplier, after which restrictions tightened further and collapsed collaboration across the field, delaying safety standards and interoperability. The paper missed that “more cognitive labour” is only productive under broad access and information flow, but SIE expectations themselves predictably trigger security regimes that destroy that condition.

10. **“Compute-Cap Policy Backfire”** — The paper’s bottom line—“good chance compute bottlenecks don’t slow an SIE until late stages”—was widely read by regulators as meaning that *software* progress could outrun hardware governance, so they focused on compute-centric controls as the lever that still mattered. Between 2029 and 2033, several jurisdictions enacted strict compute caps and reporting thresholds intended to prevent runaway software acceleration without needing to regulate models directly. The broken mechanism was the paper’s implicit claim that holding compute roughly fixed doesn’t prevent large capability jumps; in reality, the caps didn’t stop progress but reshaped it toward distributed, opaque, and under-evaluated training (splitting runs across entities, using inference-time adaptation, and shifting to unreported “edge clusters”). The cascade was worse oversight with similar capability advance: enforcement lagged, safety evals became harder, and a fragmented ecosystem of partially compliant actors deployed powerful systems with minimal centralized testing, producing systemic failures in automated trading and cyber-defense coordination. The paper missed how its compute-bottleneck analysis, once translated into policy, encouraged a single-lever regulatory approach that actors could route around—making the world less legible and less governable without actually buying much safety.