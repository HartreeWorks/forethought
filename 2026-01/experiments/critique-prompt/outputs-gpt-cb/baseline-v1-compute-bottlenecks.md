1. The paper assumes that AI R&D can be modeled by a two-input production function with “cognitive labor” and “compute” as separable factors, but it never justifies that these are the right primitives. Key drivers like data quality, organizational coordination, latency/throughput constraints, and evaluation/verification effort may not be reducible to either input. If the input decomposition is wrong, inferences about “bottlenecks” from ρ become largely ungrounded.

2. The mapping from CES “output” (Y) to “pace of AI software progress” is under-argued and potentially invalid. Economic production functions typically model levels of output, not rates of innovation, and translating a static production relationship into a dynamic claim about acceleration requires additional assumptions about knowledge accumulation and diminishing returns. Without those bridging assumptions, the ceiling-on-speed interpretation may be a category mistake.

3. The argument leans heavily on the “max speed” thought experiment (infinite AGIs, fixed compute) but treats its implications as intuitive rather than formally derived. “Max speed below 10 seems implausible” is presented as a rebuttal to economic estimates, yet it functions more like a personal incredulity claim than an argument. A skeptic could reasonably demand a model showing why the ceiling must be high rather than accepting an intuition pump.

4. The Jones-style move (“in the long run, processes reconfigure so ρ rises toward 0”) is asserted to apply to AI R&D but the paper doesn’t show that such reconfiguration is feasible without additional compute. Many proposed reconfigurations—better architectures, better training methods, better evaluations—still require empirical testing, and the time-to-reconfigure may itself be compute-limited. If reconfiguration is bottlenecked by the same scarce input, invoking it to dissolve the bottleneck risks circularity.

5. The claim that cognitive labor can “in principle fully substitute for compute” via AIs simulating neural nets “in their heads” conflates physical computation with a metaphorical description of cognition. Any “thinking” implemented on hardware still consumes compute; substituting “labor” for compute may just redescribe compute spent elsewhere. This undermines the paper’s suggestion that complementarity must break down in the absolute limit.

6. The “near-frontier experiments aren’t necessary” reply is insufficiently defended and ignores counterevidence from scaling-centric ML practice. Many capabilities and failure modes only appear at large scale (emergent behaviors, optimization instabilities, long-context effects), making extrapolation from small runs systematically unreliable. If frontier phenomena dominate progress, then compute could remain a hard bottleneck even with abundant cognitive labor.

7. The paper’s historical argument—“near-frontier experiments decreased over 10 years, yet progress didn’t slow”—may be empirically shaky and confounded. Progress could have been driven by increasing total compute, better data, improved hardware efficiency, and scaling laws rather than by a stable ability to do informative non-frontier experiments. Without a careful decomposition of what actually drove progress, the inference that frontier experiments aren’t bottlenecking is underdetermined.

8. The discussion of “smarter/faster researchers” risks double-counting and equivocation about what counts as cognitive labor versus compute. Faster “thinking” by AIs is typically realized by running more inference steps or more parallel copies, which consumes additional compute and memory bandwidth. If “more labor” implicitly requires “more compute,” then the paper overstates substitutability and may be inflating ρ by definitional fiat.

9. The “strongest-link” framing (many routes to progress, only one needs to work) is presented as undermining CES complementarity, but it doesn’t show that any route actually has high substitutability with compute. Several proposed routes—scaffolding, data flywheels, better experiments—still rely on training, evaluation, and deployment at scale, each of which can be compute-heavy. Without identifying at least one plausible high-ρ pathway with evidence, the strongest-link argument remains speculative.

10. The conclusion assigns a 10–40% probability to an SIE under a specific definition (≥5 OOM effective training compute in <1 year without more hardware), but the paper doesn’t provide a probabilistic model that connects the earlier qualitative points to those numbers. Key parameters (distribution over ρ, rate of algorithmic efficiency gains, experiment-to-insight elasticity, coordination overhead) are neither quantified nor sensitivity-tested in a way that yields that probability range. This makes the headline credences look impressionistic and potentially overconfident given the acknowledged uncertainty.