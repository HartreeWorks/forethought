1. "The CES Mapping Category Error" — The paper’s core move is to apply a CES production function (developed for GDP from labor and physical capital) to “pace of AI software progress” from “cognitive labor” and “compute.” That mapping is not defended beyond analogy, yet the conclusion depends on it: that a negative ρ implies a hard ceiling on progress speed. “Output” in CES is a flow of goods/services with market prices, while “AI software progress” is a poorly-defined research outcome with nonstationary metrics, shifting targets, and path dependence. R&D outputs are also often discontinuous (rare breakthroughs) and option-like, which violates the smooth marginal substitution logic that makes CES tractable. Without an argument that the CES form is even approximately appropriate for research production, the ceiling and “max speed” results look like artifacts of a convenient functional form rather than genuine constraints.

2. "Unjustified Parameter Fixing (α = 0.5) Drives the ‘Max Speed’" — The paper repeatedly assumes α = 0.5 “throughout,” treating compute and cognitive labor as equally weighted inputs, then translates ρ values into dramatic “max speed” ceilings. But α is not a cosmetic parameter here: for ρ < 0, the asymptotic ceiling depends heavily on α, and small changes can move the ceiling by large factors. In AI R&D, the “share” of tasks done by compute versus cognition is precisely what is contested, and plausibly varies by phase (architecture search vs debugging vs evaluation vs scaling). Holding α fixed while letting L vary by orders of magnitude bakes in a contentious division of labor rather than discovering one. This undermines the paper’s numerical claims (e.g., “ρ=-0.2 implies ~30×”) because those numbers may be mostly an artifact of α rather than informative about compute bottlenecks.

3. "Conflating ‘More Researchers’ with ‘More Effective Cognitive Labor’" — The paper treats L as “the amount of AI cognitive labour applied to R&D,” sliding between (i) more parallel copies, (ii) smarter researchers, and (iii) faster thinking as if they are commensurable and enter the same substitution relationship with compute. Yet these are different interventions with different bottlenecks: parallelism hits coordination costs and experiment throughput; speed hits serial dependencies; “smarts” may reduce sample complexity but may also require more compute for larger models. Economic estimates of substitution are indeed about “more workers,” but that doesn’t license replacing them with an undifferentiated “cognitive labor” variable that includes qualitative intelligence gains. The argument that “economic estimates don’t include smarter workers, so raise ρ” is not a valid inference unless you show that increased intelligence systematically increases substitutability with compute rather than increasing compute demand (e.g., to run more ambitious experiments). The paper’s conclusion relies on this conflation to make compute look less binding than it might be under a more realistic decomposition.

4. "The ‘AGIs Can Simulate Compute in Their Heads’ Escape Hatch" — The toy claim that with infinite AGIs “you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute” is doing heavy conceptual work: it is used to argue that ρ < 0 is “flawed in the absolute limit.” But this trades on an equivocation about what compute is: if the AGI is doing the math, it is still implementing computation on some physical substrate with energy, time, and hardware constraints, so the bottleneck hasn’t disappeared—its accounting has moved. Moreover, the relevant constraint is not abstract Turing-computability but physical throughput (memory bandwidth, interconnect, energy, latency), which “thinking” cannot bypass. Invoking an “in principle” substitution conflates logical possibility with economically and physically feasible substitution, which is precisely what bottleneck arguments are about. This weakens the paper because it suggests that extreme-limit intuitions are being used to undercut a constraint that is fundamentally physical.

5. "Selective Use of ‘Near-Frontier Experiments’ to Infer No Bottleneck" — The paper argues that if near-frontier experiments were a binding bottleneck, algorithmic progress would have slowed because “the number of near-frontier experiments… has significantly decreased” as training runs scaled faster than total compute. That is a shaky empirical inference: progress could be driven by a small number of near-frontier runs (still increasing in absolute compute), by better transfer from sub-frontier experiments, by data/engineering improvements, or by reallocating budgets toward fewer but higher-quality frontier runs. Also, “near-frontier” is not operationalized—whether 1% of a lab’s compute is the right threshold depends on the curvature of scaling laws and the kind of hypothesis being tested. The paper treats “not obviously slowing” as evidence against bottlenecks, but it provides no measurement of progress rate, no control for increased spending, and no counterfactual about what progress would have been with more near-frontier capacity. As a result, this rebuttal risks being an argument from anecdote, and it doesn’t decisively show that compute cannot become binding during an SIE.

6. "Assuming Algorithmic Efficiency Automatically Multiplies Experiment Throughput" — The paper claims that when algorithms become twice as efficient, “you can run twice as many experiments (holding capability fixed),” and that this “pulls the rug out” from the compute-bottleneck objection because the key input is not fixed after all. But the bottleneck objection is about *near-frontier capability gains*, where the relevant experiments often need to be run at or near the new frontier to validate behavior, safety, generalization, or emergent properties. Many algorithmic improvements also require costly evaluation, ablations, and robustness checks whose compute may scale with model size, dataset size, or deployment constraints. Additionally, efficiency gains can be “spent” on training larger models rather than increasing the count of experiments, so experiment throughput is not a guaranteed dividend. The paper’s inference from “algorithmic efficiency improves” to “compute bottlenecks loosen” is therefore incomplete without a model of how labs allocate compute and what fraction must be spent on frontier training and evaluation.

7. "From ‘Max Speed Seems Implausible’ to ‘ρ Must Be Higher’ Is Not an Argument" — A key argumentative pivot is: economic ρ estimates imply low “max speed,” low max speed feels implausible to the author, therefore “most likely, −0.2 < ρ < 0.” This is largely an intuition pump rather than an inference: it relies on informal imagination about what “trillions of maximally superintelligent AI researchers” could do, without specifying the actual constraints (experiment latency, validation, integration, safety, deployment, diminishing idea value). The paper acknowledges that this could be “its own post,” which highlights that the central premise—what max speed should be—is not established. Moreover, “implausible” depends on background assumptions about how much of progress is idea-limited versus experiment-limited, and those are precisely what the compute-bottleneck debate concerns. If the max-speed intuition is wrong (e.g., because most gains require frontier training and long-horizon validation), then the paper’s preferred ρ range collapses. As written, the argument risks being question-begging: it rejects ρ values mainly because they conflict with the author’s prior expectations about acceleration.

8. "The ‘Strongest-Link’ Framing Mischaracterizes the Objection It’s Rejecting" — The paper claims the CES framing is “weakest link” and proposes a “strongest link” alternative: many routes to progress exist and you only need one not bottlenecked by compute. But compute-bottleneck skeptics can accept multiple routes while still arguing that each plausible route has a compute-heavy component (training, evaluation, search, data processing, or environment simulation). The paper doesn’t identify a concrete route that demonstrably scales to superintelligence with substantially sub-frontier compute, nor does it show that the relevant route remains available once easy gains are exhausted. In practice, “routes” are not independent: many rely on the same compute-intensive substrate (large-scale training plus extensive evaluation), so they can be bottlenecked together. The strongest-link move therefore reads more like a hope that some path will be compute-light than a rebuttal to the claim that compute constraints are pervasive across candidate pathways. Without specifying which route dominates and why it evades compute limits, the argument undercuts the objection rhetorically rather than analytically.

9. "Neglecting Non-Compute Physical and Organizational Bottlenecks" — The paper frames the debate as “compute vs cognitive labor,” but in real AI R&D the bottlenecks often include data availability/quality, experiment management infrastructure, memory bandwidth, communication overhead, hardware utilization, energy, and human-in-the-loop evaluation or red-teaming. If those factors bind, then raising “cognitive labor” does not translate into proportionate increases in effective experimentation or reliable progress, even if compute in FLOPs is constant. This matters because the paper’s claim is that compute bottlenecks “don’t slow an SIE until late stages,” yet other bottlenecks can produce early-stage plateaus or slowdowns that look compute-like in effect. The CES simplification hides multi-input complementarity: even if labor substitutes for compute somewhat, it may be strongly complementary with other scarce inputs (high-quality data, secure deployment feedback, scarce expert oversight). By excluding these, the paper risks overstating how much acceleration is available “without any additional hardware,” because “hardware” is not just training accelerators but the whole experimental and deployment stack.

10. "Ambiguous and Potentially Stipulative Definition of ‘SIE’" — The paper defines an SIE as “>=5 OOMs of increase in effective training compute in <1 years without needing more hardware,” but earlier it describes SIE as “AI improving AI algorithms leads to accelerating progress without any additional hardware.” “Effective training compute” conflates algorithmic efficiency, better data, improved architectures, and possibly changed objectives into a single scalar, yet the compute-bottleneck objection is about *physical throughput and empirical validation* rather than a re-labeled efficiency metric. The definition also builds in a particular target (5 OOM in a year) that may not track “superintelligence” or socially relevant capability jumps, and it risks making the debate hinge on an arbitrary threshold. Moreover, the paper’s earlier “max speed” discussion concerns “pace of progress” while the later definition concerns “effective training compute,” which are not the same variable; improvements might increase effective compute without proportionate speedups (or vice versa). This ambiguity weakens the conclusion because it becomes unclear whether the argument shows “no compute bottleneck,” or merely that some efficiency metric can rise quickly even if real-world progress remains bottlenecked by training/evaluation time and other constraints.