1. [The Empirical Hardliner] The paper’s central move—“most likely −0.2 < ρ < 0, so compute bottlenecks won’t bite early”—is asserted via informal plausibility (“seems implausible,” “talking to researchers”) rather than a measurable mechanism that ties algorithmic progress to an experimentally testable scaling law. You never specify what observable quantity in current ML maps to “pace of AI software progress” in the CES analogy, so there’s no falsifiable prediction that could distinguish your “ρ near 0” world from Epoch’s “ρ < −0.2” world. The “max speed” argument is especially non-empirical: it converts a parameter into an intuition pump (“trillions of God-like AIs would surely get >10×”), then uses that intuition to reject the parameter range. Without a causal model of where algorithmic improvements come from (e.g., distribution of idea quality, experiment cost curves, validation overhead) you’re just handwaving at the sign and magnitude of ρ. If this objection holds, your 10–40% SIE probability is not an estimate but a vibe, and the reader can’t update on it with new evidence.

2. [The Mechanism Designer] The claim that “in the limit of infinite AGIs you could do the math for NNs in their heads and fully simulate computational experiments using cognitive labour in place of compute” tries to break the compute ceiling by redefining compute as “thinking.” But those AGIs are instantiated on the same physical compute substrate you are supposedly holding fixed, so “cognitive labour” is not an independent input—your L is literally carved out of K. The paper never formally specifies the resource accounting: what fraction of inference FLOPs per AGI-hour replaces what fraction of training FLOPs per experiment, and how memory bandwidth/communication are handled when “in-head simulation” scales. Without a formal model, this move is equivalent to claiming a perpetual motion machine inside the CES framework by double-counting the same hardware as both L and K. If this objection holds, your key escape hatch (“ρ can approach 0 because labour can substitute for compute”) collapses, and the ceiling reappears as a hard budget constraint on total FLOPs and memory traffic.

3. [The Resource Economist] The paper’s “bottleneck is # experiments, and algorithms can become more compute-efficient, so you can run more experiments holding compute fixed” conflates two different notions of efficiency: capability-per-FLOP at deployment versus research-search cost to discover the next improvement. In modern ML, many algorithmic gains are discovered by running *more* or *larger* sweeps (hyperparameters, data mixtures, RL rollouts, evals) and then paying additional compute to validate, ablate, and reproduce; the search process can scale superlinearly even if the final model is more efficient. Your rebuttal to “near-frontier experiments matter” leans on the past decade, but that decade also featured a giant expansion in aggregate compute and industrialization of the experiment pipeline—exactly the confound that breaks your inference. You never quantify the share of progress attributable to sub-frontier experiments versus frontier-scale training runs, so “you might not need near-frontier experiments” is an unsupported pivot. If this objection holds, algorithmic improvements do not relax the compute constraint the way you claim, and the compute bottleneck can bite *earlier* precisely because the research process demands expensive validation.

4. [The Complexity Theorist] You model AI R&D as two smooth, substitutable inputs (cognitive labour L and compute K) and argue that with enough L the system can rapidly “reconfigure” to raise effective ρ. Real AI R&D is a tightly coupled pipeline with emergent coordination costs: integrating thousands of parallel contributions requires regression testing, evaluation suites, reproducibility checks, and dependency management, all of which consume scarce compute and create serial bottlenecks. As L scales by orders of magnitude, the number of interactions (and therefore integration work) can grow faster than linearly, pushing the system into a regime where “more researchers” increases the fraction of work spent on merge conflicts, contradictory experimental results, and chasing non-reproducible wins. Your “fast-thinking AGIs can reconfigure in days” ignores that many bottlenecks are not “thinking” but systems-level coupling (data versioning, infra constraints, evaluation drift, and deployment feedback loops). If this objection holds, the effective ρ becomes *more negative* as the organization scales, making your early-stage “no compute bottleneck” conclusion directionally wrong.

5. [The Game-Theoretic Defector] The paper assumes that adding vast numbers of AGI researchers translates into proportionally better experimental design, early stopping, and “optimizing every part of the stack,” effectively converting L into progress. But inside a lab (or across labs), parallel agents face misaligned incentives: they can pad “progress” with compute-wasting sweeps, hoard promising ideas to bargain for resources, or optimize for metrics that win internal approval rather than for globally useful algorithmic advances. Your argument that “we’ll use whichever route has the most favourable ρ” ignores that routes are chosen by actors optimizing for prestige, funding, and strategic advantage, not for minimizing compute burn per unit of insight. In a high-speed SIE attempt, the equilibrium can be massive redundant experimentation and compute racing because no one trusts rivals’ results and everyone wants first-mover credit. If this objection holds, the *effective* compute available for genuine algorithmic progress shrinks due to strategic waste, and “lots of cognitive labour” produces less acceleration than your model assumes.

6. [The Institutional Corruptionist] You implicitly invite readers to focus on “compute bottlenecks” as a measurable limiting factor, then argue they may not matter until late stages, which nudges governance thinking toward tracking compute and relaxing early concern. But compute accounting is the perfect domain for compliance theatre: labs can underreport training runs, relabel experiments as “inference,” shard workloads across affiliates, or use third-party cloud and overseas clusters specifically to evade scrutiny. Because your “SIE” definition is “≥5 OOM increase in effective training compute in <1 year without more hardware,” the obvious incentive is to game the “effective” part—claim algorithmic efficiency gains while hiding the real compute spend and evaluation failures. You don’t address principal-agent failures inside firms (safety/compliance teams versus product orgs) that systematically bias reported efficiency upward. If this objection holds, any strategy that leans on “we’ll notice compute bottlenecks/thresholds as they approach” fails, and a putative SIE could proceed in a fog of manipulated metrics.

7. [The Political Economist] The paper treats compute as a quasi-fixed input in the relevant window and frames the question as substitutability between cognitive labour and compute inside R&D. In reality, the moment credible “software-only acceleration” appears, the distribution of power and capital changes: investors and states pour money into compute, firms vertically integrate chips, and geopolitics gates access to leading-edge fabs. That makes K endogenous to expected rents, and it also concentrates K in a few actors who can impose their own bottlenecks (export controls, exclusive supply contracts, internal allocation fights) irrespective of any technical ρ. Your model therefore misses the dominant mechanism by which “bottlenecks” actually appear: not physics, but control of supply chains and legal coercion. If this objection holds, your focus on whether compute bottlenecks are technically weak mispredicts the *actual* limiter on an SIE—political allocation of compute—and your timelines and probabilities are anchored to the wrong variable.

8. [The Adversarial Red-Teamer] You argue that near-frontier experiments may not be necessary because you can extrapolate from smaller runs and because progress didn’t slow when near-frontier experiments allegedly got rarer. That creates an exploitable vulnerability: if your R&D process increasingly relies on extrapolation and proxy experiments, a sophisticated adversary can poison the inference channel—tamper with datasets, contaminate benchmarks, or seed architectures that look good at small scale but fail catastrophically at frontier scale. The more you compress validation due to compute scarcity (early stopping, smaller-scale proxies), the easier it becomes to smuggle in “progress” that is actually brittle, unsafe, or non-generalizing. The paper never specifies an adversary model, yet its strongest-link framing (“we’ll use whichever route works”) is exactly what an attacker exploits by targeting the measurement of “works.” If this objection holds, the project can burn enormous effort on illusory gains and then hit sudden, late-stage failures that look like “compute bottlenecks” but are actually adversarially induced epistemic collapse.

9. [The Historical Parallelist] You dismiss manufacturing-based substitution estimates as poor analogues because AI has “optimizing every part of the stack” and “running smaller scale experiments,” implying much higher substitutability than sectors like factories. A closer historical analogue is not manufacturing but other empirically gated R&D domains—drug discovery, aerospace, semiconductors—where floods of researchers and better theory still hit hard experimental/validation chokepoints (wet-lab assays, wind tunnels/flight tests, fabrication cycles). In those fields, “reconfiguration” didn’t compress validation cycles from years to days just because more cognition was available; the bottleneck moved to the most reality-coupled tests, and progress became gated by the slowest trustworthy feedback. Frontier ML training runs are exactly that kind of reality-coupled test for many claims about generalization and robustness, and your paper offers no reason they won’t play the same role as fabrication runs in chip design. If this objection holds, using “software is different” to infer ρ near 0 is historically naïve, and compute-heavy validation remains the pacing item that prevents your months-scale explosion.

10. [The Second-Order Catastrophist] The paper claims compute bottlenecks may not slow an SIE until late stages, partly because there are “routes to improving AI that don’t use compute-intensive experiments” and because algorithmic efficiency can compound. If that’s true, it doesn’t merely speed up one lab—it collapses barriers to entry by making capability jumps less dependent on owning frontier hardware, enabling many mid-tier actors to ride the same software wave. That increases the number of simultaneous near-frontier programs, multiplies deployment of partially tested systems, and turns “SIE” from a single fast takeoff into a chaotic proliferation event with many independent failure points. Your analysis treats “more routes” as purely beneficial for escaping bottlenecks, but in a world of many actors it also means more leakage, less centralized control, and faster diffusion of dangerous capabilities. If this objection holds, the main consequence of your own thesis is not “compute bottlenecks won’t stop SIE,” but “software-driven acceleration makes containment and coordination dramatically harder,” worsening the very societal-risk scenario you use to motivate urgency.