1. The paper’s central move is to treat “pace of AI software progress” as a smooth CES production output \(Y=f(K,L)\), where \(K=\) compute and \(L=\) cognitive labour, and then read “max speed” off the \(L\to\infty\) limit. That mapping hides a load-bearing assumption: that marginal progress is continuously producible by mixing compute and cognition, rather than being gated by a small number of discrete, high-cost validation events. Counter-model: suppose most candidate algorithmic ideas are cheap to generate (lots of \(L\)) but only become knowably correct after one or two full-scale training runs that each consume ~30–50% of annual compute; then “progress speed” is dominated by a serial queue of frontier trainings, so \(L\to\infty\) barely changes wall-clock progress. Under this model, the CES ceiling is not “high but finite,” it is effectively the inverse of frontier-run cycle time, and it bites immediately. If this critique holds, the paper must replace CES-with-\(Y=\)progress with a model where progress is a function of (i) idea generation rate and (ii) frontier-validation throughput, and show the second term is not the binding constraint in early SIE.

2. The “economist version” is stress-tested almost entirely by varying \(\rho\), while the share parameter \(\alpha\) is held at 0.5 “throughout,” and then “max speed” claims (e.g., 6×, 30×, 100×) are treated as substantive. But those max-speed numbers are not robust to \(\alpha\); they rely on a hidden lemma that AI R&D is roughly half compute-limited and half labour-limited in a CES sense, which is exactly what is at issue. Counter-example: let \(\alpha=0.95\) (compute is 95% of the binding input for frontier-relevant progress because training/evals dominate), with \(\rho=-0.2\); then even astronomical increases in \(L\) produce only marginal speedup and the “compute bottlenecks bite late” conclusion flips to “compute bites essentially immediately.” The paper’s downstream argument (“economic \(\rho\) implies implausibly low max speed”) can be made to say almost anything by changing \(\alpha\), so the critique lands on a load-bearing beam: the quantitative intuition is unanchored. If this holds, the author must (a) endogenize \(\alpha\) as a function of capability level and research phase, or (b) derive \(\alpha\) from a concrete decomposition of AI R&D tasks (training, evals, interpretability, data, tooling) and then redo every “max speed” claim.

3. The Jones-style move—“in the longer run, we reconfigure production so \(\rho\) rises toward 0, and with fast AGIs this reconfiguration could happen in days or weeks”—is a keystone step for dismissing complementarity as a binding early bottleneck. The hidden assumption is that “reconfiguration” is primarily cognitive (new org/processes) rather than itself being compute-intensive, because the mechanisms proposed (new paradigms, new evals, new training recipes) still require running models to test and calibrate. Counter-model: suppose the only credible way to “reconfigure” to higher substitutability is to build new automated experimentation infrastructure that itself requires repeated large-scale ablation studies and training runs (to learn what to automate safely and correctly); with fixed compute, the adjustment period is compute-bounded and cannot be “days or weeks,” so the system remains in the low-\(\rho\) regime throughout the window that matters for an SIE. Then the paper’s escape hatch (“short-run \(\rho<0\), long-run \(\rho\approx 0\)”) becomes irrelevant because the “long run” arrives after the supposed explosion window. If this critique holds, the paper must explicitly model adjustment costs and show that the time-to-reconfigure is not itself bottlenecked by the same fixed compute that allegedly doesn’t bite until late.

4. The toy argument that “in the limit of infinite AGIs you could use AGIs to do the math for NNs in their heads and thereby fully simulate computational experiments using cognitive labour in place of compute” is doing illicit work: it is being used to undermine the absolute validity of \(\rho<0\) by claiming cognition can fully substitute for compute. The hidden lemma is that “AGI cognition” is not itself implemented on compute that draws from the same fixed hardware budget \(K\); but in the scenario under discussion, those AGIs are software running on GPUs/TPUs, so their “thinking” is literally compute consumption. Counter-model: fix hardware; you can allocate it to (a) running AGI “researcher” inference or (b) running training/eval experiments, but both draw from the same compute pool; “simulating experiments in heads” just reallocates compute from explicit training to implicit mental simulation and does not create extra experimental throughput. Under this counter-model, the claimed principled refutation of \(\rho<0\) collapses: compute remains the conserved quantity and cognition cannot escape it. If this holds, the author must drop “AGIs can replace compute” as a substitute-argument and instead argue for a specific, non-handwavy channel where algorithmic progress reduces required *total* compute for frontier-relevant learning (not merely shifts compute between thinking and training).

5. The paper’s most load-bearing “rug pull” is: “when algorithms become twice as efficient, you can run twice as many experiments (holding capability fixed), so during an SIE labs can increase both cognitive labour and # experiments; this undermines the objection that an essential input is fixed.” This relies on a hidden lemma that the experiments that matter for accelerating progress are not the near-frontier experiments whose cost scales with the frontier, but cheaper experiments whose count can grow as efficiency improves. Counter-model: algorithmic ideas are systematically misleading at small scale due to scaling pathologies (optimization instabilities, emergent behaviors, data mixture effects), so the only experiments that discriminate between competing approaches are near-frontier trainings/evals; efficiency improvements that look good at small scale do not buy you “more relevant experiments” unless you spend compute near the frontier anyway. Then “twice as efficient” does not yield twice the *decision-relevant* experiments; it yields more cheap noise while the bottlenecked frontier queue remains fixed, so acceleration stalls early. If this holds, the author must revise the argument to distinguish “experiment count” from “information gain about frontier performance,” and show that information gain per unit compute increases fast enough to overcome the fixed near-frontier budget.

6. The rebuttal to “near-frontier experiments are fixed” is: (i) maybe we can extrapolate from smaller experiments, and (ii) “over the past ten years, the number of near-frontier experiments … has decreased; if they were a bottleneck, progress would have slowed.” The inference step is brittle because it assumes that “near-frontier experiment count” is the binding statistic, rather than “total frontier-equivalent compute devoted to a small set of decisive runs,” which can rise even as the *count* falls. Counter-model: the world runs fewer near-frontier trainings, but each is vastly larger and more instrumented, and the downstream ecosystem (fine-tuning, distillation, scaffolding, data curation) is keyed to those few runs; progress can continue as long as those flagship runs keep growing, but if compute is held constant those runs cannot grow and the whole pipeline slows. This counter-model fits the historical observation (count down, progress up) while reversing the paper’s conclusion about bottlenecks. If this holds, the author must replace the “count decreased ⇒ not bottleneck” argument with an analysis keyed to (a) frontier-equivalent compute per year and (b) marginal capability gain per frontier-equivalent compute, not raw run counts.

7. The “max speed is implausibly low” argument (e.g., “below 10× seems implausible,” “below 30× seems kinda implausible”) is doing decisive work to push \(\rho\) upward into \([-0.2,0)\). But it smuggles in a hidden assumption that AI R&D is highly parallelizable in wall-clock time once you have enough cognitive labour, rather than being dominated by a critical path of sequential dependencies (design → train → analyze → redesign) whose steps each consume fixed minimum time on fixed compute. Counter-example: imagine a research regime where each iteration requires (1) one training run at scale, (2) one full eval battery, (3) post-hoc interpretability and red-teaming, each step taking a minimum of N days on fixed compute; even with infinite AGIs, you cannot compress those compute-time steps below their physical runtime, so speedup saturates at ~2–5× via better scheduling and early stopping, not 30×. This preserves the paper’s premise that lots of cognition helps, while snapping the inference that “max speed must be high, therefore \(\rho\) near 0.” If this holds, the author needs to replace intuition about “what abundant cognitive labour could do” with a critical-path model that upper-bounds wall-clock acceleration given fixed compute throughput and sequential experiment dependencies.

8. The “strongest link” framing—there are many routes to superintelligence, we only need one route not bottlenecked by compute—is used to argue that compute-bottleneck objections must hold across *all* routes, which is presented as unlikely. The hidden lemma is that there exists at least one route where the decisive capability gains can be achieved without consuming near-frontier training compute, i.e., that “route diversity” implies “compute non-binding.” Counter-model: every route that actually yields deployable capability increases must ultimately be instantiated in updated weights/policies that generalize in the real world, and the only reliable way to get that instantiation is through training/evaluating at or near the frontier; you can generate many theories and scaffolds with \(L\), but capability jumps bottleneck on the same compute-limited training step. Under this model, “strongest link” doesn’t help because all links share the same final common path: frontier training throughput. If this holds, the author must separate (i) routes to *ideas* from (ii) routes to *realized capability*, and demonstrate at least one realized-capability route whose compute demand does not scale with the frontier.

9. The paper operationalizes SIE as “\(\ge 5\) OOM increase in effective training compute in <1 year without more hardware,” and then argues compute bottlenecks likely don’t bite until late because algorithmic progress can raise “effective compute.” This relies on a hidden monotonicity: that algorithmic improvements predominantly *reduce* the compute needed for a given capability target, rather than also enabling/inducing shifts to more compute-hungry training objectives, modalities, or safety constraints that absorb the savings. Counter-model: as algorithms improve, labs expand the ambition set (larger context, richer world models, more agentic training, heavier eval/regression suites), so net compute per “frontier step” stays flat or rises; “effective training compute” doesn’t jump 5 OOM because the frontier definition moves with the method. Then the paper’s conclusion (“compute bottlenecks don’t slow early SIE”) fails even though algorithmic efficiency is improving—because the efficiency is endogenously spent. If this holds, the author must revise the metric to something invariant to shifting frontiers (e.g., capability per joule at fixed task distribution) and show that invariant improves by 5 OOM on a <1-year timeline under fixed hardware.

10. A core simplification is treating \(L\) (cognitive labour) and \(K\) (compute) as independently scalable inputs during an SIE—e.g., “in the early stages… total cognitive labour is 1–3 OOM bigger than the human contribution” while “holding compute constant.” But in AI, “more cognitive labour” typically means “more AGI instances running,” which consumes inference compute and memory bandwidth drawn from the same fixed hardware pool, making \(L\) a function of \(K\) rather than an independent axis. Counter-model: with fixed hardware, spinning up 100× more AGI researchers forces each to run 100× slower or at much smaller model size; the net researcher-seconds per wall-clock may not increase much, and may even decrease if smaller/slower models are less effective per FLOP for research tasks. Then the predicted early-stage acceleration from 1–3 OOM more \(L\) evaporates, and compute bottlenecks bite immediately through the coupling \(L(K)\). If this holds, the author must revise the production model so \(L\) is bounded by an inference-compute budget (and interacts with training-compute needs), and re-derive whether any early-stage “software-only” acceleration survives once researcher compute competes directly with experiment compute.