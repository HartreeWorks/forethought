1. The paper’s argument leans on the load‑bearing claim that “cognitive labour (L) and compute (K) are separable inputs in AI R&D,” so that you can hold K fixed while scaling L arbitrarily. **Countermodel:** imagine a lab where “adding researchers” means instantiating additional copies of a frontier model that each require substantial inference compute to read papers, write code, run verifiers, and even just stay interactive; in that world L is largely *implemented by* K, so scaling L necessarily consumes the same fixed compute pool. All the paper’s premises about automation and smarter researchers can hold, yet the conclusion (“compute bottlenecks won’t bite until late”) fails because the “extra labor” draws down the very K assumed fixed. If this critique holds, the CES framing in the post is mis-specified for AI: you can’t infer late-stage compute non-bottlenecking from a model that treats AI researchers as compute-free labor.

2. A key load-bearing step is the translation from ρ estimates to a “max speed of AI software progress,” holding α=0.5 throughout. **Parameter sensitivity:** in the CES function, α (the share/weight on compute) is exactly what controls how quickly output saturates when K is fixed, and there’s no paper-specific reason AI R&D should be near α=0.5 rather than, say, α≫0.5 for frontier ML where experiments dominate costs. If α is 0.8 instead of 0.5, the same ρ produces a much lower ceiling on speedups from extra “labour,” undercutting the “implausibly low max speed” argument used to push ρ toward 0. If this critique holds, the paper’s central numerical intuition (e.g., “ρ=-0.2 implies ~32×”) is not robust enough to support the conclusion about bottlenecks arriving only in late stages.

3. The post relies on the claim that “long-run estimates push ρ upward (toward Cobb–Douglas), and AGI could quickly reconfigure production so AI R&D behaves like the long run.” **Reference class failure:** the Jones-style “long run” in macro comes from *accumulating or redesigning capital* and reorganizing production, but in this paper’s SIE setting hardware compute is explicitly held fixed, and most “reconfiguration” options (bigger batch sizes, more parallelism, more elaborate evaluators, more synthetic data generation) themselves consume more compute rather than substituting it away. Treating “fast-thinking AGIs” as a substitute for the real-world time needed to build/retrofit capital conflates organizational redesign with physical capacity change. If this critique holds, the paper’s main pathway from “short-run complementarity” to “ρ≈0 quickly” is blocked, strengthening the original compute-bottleneck objection rather than weakening it.

4. The argument that “the real bottleneck is number of experiments, and algorithmic efficiency improvements let you run more experiments at fixed compute” is load-bearing for dismissing fixed-K ceilings. **Causal reversal:** many of the efficiency gains that matter for capability (not just cheaper replications) are exactly those that *increase* the appetite for near-frontier experiments—because each promising idea must be validated on the hardest regimes to avoid Goodharting on small-scale proxies (distribution shift, long-horizon agency, tool use, etc.). In that world, algorithmic progress increases the *marginal value* of scarce frontier experiments, tightening rather than loosening the compute constraint for the decisions that steer the trajectory. If this critique holds, “compute-efficient experiments” doesn’t pull the rug out from under the bottleneck objection; it can make compute more binding precisely where the paper needs it to become less binding.

5. The post’s rebuttal to “near-frontier experiments are fixed” depends on the claim that “you might not need near-frontier experiments; you can extrapolate from smaller ones,” and that the near-frontier count falling over the last decade didn’t slow progress. **Quantitative cliff:** extrapolation error can be mild within a regime but become discontinuous at capability thresholds (new emergent behaviors, new failure modes, qualitatively different scaling exponents), so the adequacy of small experiments can collapse once models start doing long-context planning, autonomous tool use, or adversarial adaptation. The last decade’s progress is not evidence against frontier-experiment necessity because the field also expanded frontier compute; scarcity of near-frontier experiments could have been masked by hardware scaling even if algorithmic *validation* was bottlenecked. If this critique holds, the paper loses a main empirical-style prop for dismissing the “frontier experiments are the binding constraint” version of the compute objection.

6. The paper argues that economic ρ values imply “implausibly low” max speeds (e.g., <10×), and uses that implausibility to push ρ into −0.2<ρ<0. **Countermodel:** suppose AI R&D has a heavy-tailed idea distribution where most candidate improvements are worthless, and the binding resource is not coding time but evaluating rare, brittle gains across many tasks and safety-critical edge cases—evaluation that is itself compute-hungry and hard to shortcut. Then even “trillions of superintelligent researchers” don’t yield big speedups because the pipeline is dominated by high-fidelity training/eval cycles, not human-style brainstorming. If this critique holds, “low max speed” stops being implausible, so the argument that “most economic estimates must be wrong for AI” no longer supports the conclusion.

7. A prominent move is the claim that “in the absolute limit, cognitive labour can fully substitute for compute (AGIs do the math in their heads), so ρ<0 is flawed in principle.” **Quantitative cliff:** any “do the math in your head” substitution requires that the cognitive system itself instantiate the computation, which—if it’s an AI—still cashes out as physical compute somewhere, and for state-of-the-art training dynamics the relevant computations (matrix multiplies over huge tensors, optimizer states, long sequences) scale so steeply that the substitution becomes astronomically inefficient. This isn’t just “impractical”; it collapses the inference that complementarity must break down near the levels relevant to an SIE, because the regime where substitution is “possible” may be many orders of magnitude beyond what a fixed-compute world can realize. If this critique holds, the “ρ can’t be <0 in the limit” point doesn’t meaningfully weaken the compute-bottleneck objection at the scales that matter to the paper’s 1-year / 5-OOM definition.

8. The conclusion “compute bottlenecks likely don’t slow an SIE until late stages” depends on treating ρ as roughly stable over the SIE trajectory (or at least not getting more negative early). **Equilibrium shift:** as AI R&D becomes faster and more automated, labs rationally shift toward more compute-intensive search methods (larger sweeps, more self-play, more automated red-teaming, more elaborate synthetic data pipelines) because the opportunity cost of compute falls relative to cheap cognitive labor, making the effective production process *more* complementary in compute. That is, strategic optimization can drive ρ downward endogenously, even if today’s ρ is mild, because actors choose methods that cash out as “burn compute to buy certainty.” If this critique holds, the paper’s “early stages look similar across −0.2<ρ<0” sensitivity story breaks: the process can move into a compute-bound regime precisely when automation arrives, not only “late.”

9. The paper’s “strongest link” framing (multiple routes to superintelligence; we’ll use whichever has favorable ρ) is load-bearing for arguing compute bottlenecks must bind across *all* routes to matter. **Reference class failure:** many “alternative routes” listed (scaffolding, data flywheel, better experiments) still depend on large-model training/evaluation as the selection mechanism that tells you which scaffold or dataset actually works, and that selection pressure is compute-intensive in a way that doesn’t disappear with more cognitive labor. In other words, route diversity does not imply input substitutability; it can just mean many paths all share the same compute-heavy gatekeeper (high-fidelity training and eval). If this critique holds, the “strongest link” move doesn’t undercut the compute bottleneck objection; it may even strengthen it by highlighting that selection/evaluation compute is a common bottleneck across routes.

10. The paper operationalizes an SIE as “≥5 OOM increase in effective training compute in <1 year without more hardware” and suggests compute bottlenecks probably won’t block that early. **Parameter sensitivity:** “effective training compute” is treated as if it compounds cleanly from algorithmic improvements, but many improvements don’t multiply—some trade off against each other (e.g., lowering training compute increases evaluation needs, or improves sample efficiency but requires more optimizer state/longer contexts), and some gains are one-time architectural step-changes that don’t sustain exponential feedback. If compounding is weak or strongly coupled to extra evaluation/training stages, then even with ρ near 0 the system may fail to reach 5 OOM in a year, making the stated probability range (10–40%) hinge on an implicit multiplicativity assumption. If this critique holds, the paper’s bottom-line forecast becomes materially overconfident even under its own “compute bottlenecks are mild early” premise, because the metric chosen silently presumes a kind of compounding the production model doesn’t establish.