1. **“The CES Category Error”** — The paper’s key inference is that a CES production function with inputs **cognitive labour (L)** and **compute (K)** can be used to bound the *pace of algorithmic progress* (Y) via a “max speed” ceiling when ρ<0. This breaks because CES is a model of *static output from factor inputs*, while “pace of progress” is a dynamic, path-dependent object: the mapping from (L,K) to marginal research progress changes endogenously as methods, tooling, and baselines improve. In AI R&D, today’s “output” is tomorrow’s production function (e.g., better optimizers, better evaluation harnesses), so assuming a time-invariant ρ is doing the load-bearing work without justification. If this objection holds, the paper can’t treat any particular ρ-range (e.g. −0.2<ρ<0) as informative about early-vs-late SIE stages without a separate model linking factors to *knowledge accumulation*.

2. **“L and K Aren’t Independent Here”** — The paper attacks the compute-bottleneck objection by increasing substitutability between cognitive labor and compute, but the central inference assumes we can vary L upward while holding K fixed. In reality, “cognitive labour” in an AI-R&D-automation scenario is itself instantiated as inference compute (agent runs, tool use, long-horizon deliberation), so raising L *necessarily consumes K* unless you posit some separate non-compute substrate. That means the relevant constraint is not CES with separable inputs, but a budget constraint where agents and experiments draw from the same compute pool, often turning “more researchers” into fewer experiments. If this holds, the paper must reformulate the argument around an explicit compute-allocation model (agents vs training vs eval), and the claim that compute bottlenecks “likely don’t bite until late stages” becomes much harder to sustain.

3. **“Small-Scale Extrapolation Mirage”** — A linchpin rebuttal is that we may not need near-frontier experiments because we can extrapolate from smaller ones, weakening compute bottlenecks. This breaks if many algorithmic changes have *scale-dependent effects* (optimizer stability, emergent capabilities, data/architecture interactions) such that performance at small scale is a poor predictor near the frontier—the exact regime that matters for pushing capability. In that world, you can do vast numbers of cheap experiments yet still be forced to validate and iterate at frontier scale, reintroducing a hard compute gate. If this objection holds, the paper must either (i) argue empirically that small-to-large transfer is reliable for the classes of innovations that drive big capability jumps, or (ii) concede that “#experiments” is not readily substitutable for frontier compute.

4. **“The Near-Frontier Trend Doesn’t Falsify Bottlenecks”** — The paper argues that because the number of near-frontier experiments has decreased over a decade while progress continued, near-frontier experiments cannot be the bottleneck. The failure mechanism is confounding: progress over the past decade was heavily driven by *increasing training compute, better data pipelines, and architectural paradigm shifts* that may require only occasional frontier runs but still be compute-gated at key validation points. Moreover, “number of near-frontier experiments” is not measured; depending on how you count (aborted runs, hyperparameter sweeps, distributed training variations, multiple labs), it may not have decreased in the relevant sense. If this objection holds, the paper must replace the rhetorical “proves too much” move with a quantitative accounting tying historical progress rates to an estimated frontier-experiment budget.

5. **“Effective-Compute Gains Don’t Equal Capability-Speed Gains”** — The concluding probability claim depends on a definition of SIE as “≥5 OOM increase in effective training compute in <1 year without more hardware,” implicitly treating algorithmic efficiency as fungible with raw compute for speeding capability gains. This breaks if efficiency improvements face diminishing returns or shift you onto different parts of scaling curves where *capability per effective compute* is not constant (e.g., data quality limits, optimization plateaus, or new bottlenecks like context length or eval reliability). Then “5 OOM effective compute” could correspond to far less than “months-to-superintelligence,” undermining the paper’s central implication that compute bottlenecks likely don’t prevent rapid takeoff early on. If this holds, the paper must ground “effective compute” in an explicit capability metric (task performance, economic value, autonomy) and show the mapping stays steep over the relevant range.

6. **“Reconfiguration Still Costs Compute”** — The paper leans on a Jones-style story: short-run complementarity may be high, but with time (and fast AGIs) the R&D process can be reconfigured so ρ rises toward 0 quickly. The failure mechanism is that “reconfiguration” in ML R&D—new training recipes, new eval suites, new architectures, automated interpretability, robustifying pipelines—typically requires substantial iterative experimentation and validation, i.e., the very compute the bottleneck restricts. Fast thinking alone doesn’t bypass the need to run trainings, ablations, stress tests, and deployment-like evals, especially as models become more capable and safety-critical. If this holds, the paper must model the *compute cost of escaping the bottleneck*; otherwise it assumes the conclusion (high substitutability) by positing an easy transition to methods that allegedly remove complementarity.

7. **“Mental Simulation Smuggles in Compute”** — The paper’s toy argument that infinite AGIs could “do the math for NNs in their heads” is used to suggest that, in principle, cognitive labour can substitute for compute, weakening ρ<0 in the limit. But the mechanism fails under the paper’s own premise of holding compute fixed: those AGIs’ “heads” are compute, so any faithful simulation of training dynamics is just compute consumption in disguise, not a free substitute. If “in-head simulation” is merely shifting FLOPs from “training runs” to “agent cognition,” then the bottleneck remains a compute budget constraint, not a substitutability win. If this objection holds, the paper should drop this as evidence for higher ρ and instead argue with concrete, compute-cheap research pathways whose validity can be established without frontier-scale execution.

8. **“The ‘Max Speed Is Implausibly Low’ Argument Is Unanchored”** — The paper attacks negative-ρ estimates by claiming they imply implausibly low “max speed” (e.g., <10×), relying on intuition about what abundant cognitive labor could do (optimize stack, generate ideas, stop runs early, etc.). The failure mechanism is that many of those gains either (i) are themselves compute-intensive to validate, (ii) saturate quickly, or (iii) run into verification/robustness constraints where speed is limited by reliable evaluation rather than ideation. Without empirical calibration—e.g., historical speedups from additional researchers at fixed compute, or measured returns to automation on experiment throughput—“implausible” just restates the desired conclusion. If this holds, the paper must either provide quantitative priors from historical lab productivity or weaken its central claim to “we don’t know,” rather than “most economic ρ imply absurd ceilings.”

9. **“Strongest-Link Optimism Ignores Correlated Compute Dependence”** — The “strongest link” framing claims there are multiple routes to progress (scaffolding, data flywheels, better experiments, extrapolation), and only one needs to be weakly compute-bottlenecked. This breaks if those routes share correlated dependencies on compute-intensive primitives: strong base models, large-scale eval, extensive fine-tuning, synthetic data generation at scale, and repeated red-teaming—each consuming substantial training and inference compute. If the routes are not independent but all bottleneck on the same compute budget, then “we’ll pick the least bottlenecked route” offers little relief. If this holds, the paper must map the candidate routes to a common compute ledger and show at least one plausible path where frontier capability gains can be reliably achieved with modest compute growth.

10. **“Hidden Bottleneck: Evaluation and Reliability, Not Just Training”** — The paper largely equates “compute bottleneck” with training-experiment throughput, but the central conclusion (“compute bottlenecks probably don’t slow SIE until late”) assumes progress is mainly gated by running more experiments. A concrete failure mechanism is that as systems become more capable and agentic, the binding constraint may shift to *evaluation*: measuring real improvements, catching regressions, ensuring robustness and alignment, and preventing deceptive or non-generalizable gains—often requiring expensive, broad, and adversarial testing (frequently at large scale). If eval is the true bottleneck, then algorithmic efficiency that lets you run more cheap trainings doesn’t buy proportional progress because the expensive part is proving the system is actually better and safe enough to deploy for further R&D. If this holds, the paper must extend its model from “experiments” to “validated progress,” and its early-stage SIE optimism may need major qualification.