The “economist version” is stress-tested almost entirely by varying \(\rho\), while the share parameter \(\alpha\) is held at 0.5 “throughout,” and then “max speed” claims (e.g., 6×, 30×, 100×) are treated as substantive. But those max-speed numbers are not robust to \(\alpha\); they rely on a hidden lemma that AI R&D is roughly half compute-limited and half labour-limited in a CES sense, which is exactly what is at issue. Counter-example: let \(\alpha=0.95\) (compute is 95% of the binding input for frontier-relevant progress because training/evals dominate), with \(\rho=-0.2\); then even astronomical increases in \(L\) produce only marginal speedup and the “compute bottlenecks bite late” conclusion flips to “compute bites essentially immediately.” The paper’s downstream argument (“economic \(\rho\) implies implausibly low max speed”) can be made to say almost anything by changing \(\alpha\), so the critique lands on a load-bearing beam: the quantitative intuition is unanchored. If this holds, the author must (a) endogenize \(\alpha\) as a function of capability level and research phase, or (b) derive \(\alpha\) from a concrete decomposition of AI R&D tasks (training, evals, interpretability, data, tooling) and then redo every “max speed” claim.