> Reorganising AI R&D around abundant cognition isn't just a planning problem—validating new training regimes, scaling behaviours, and safety properties demands the very compute that's supposedly the bottleneck.

**“Reconfiguration Isn’t a Purely Cognitive Move”** (attacks **Pivot P2: “Jones-style reconfiguration could be very quick with fast-thinking AGIs… days or weeks,” raising effective ρ toward 0.** This pivot is load-bearing because it’s the key escape hatch from short-run complementarity.) The claim assumes the main barrier to reorganizing AI R&D around abundant cognition is *planning*, but many frictions are compute- and infrastructure-coupled: implementing new training regimes requires repeated full-stack trials, kernel/hardware co-tuning, data reprocessing, long-horizon evals, and reliability testing that consume the very compute purportedly fixed. If that reconfiguration itself requires substantial near-frontier or long-duration runs (e.g., to validate scaling stability, emergent behaviors, or safety-critical generalization), then abundant AGI labor can’t compress the timeline much—compute remains the gating factor. In that world, the paper’s “late-stage only” bottleneck claim fails because the system hits “reconfiguration validation” bottlenecks early, before any rapid compounding. What would need to change is a decomposition showing which reconfiguration steps are truly compute-light versus compute-heavy, and empirical analogues from recent ML infra shifts (e.g., MoE, long-context, RLHF pipelines) with measured compute/time requirements.