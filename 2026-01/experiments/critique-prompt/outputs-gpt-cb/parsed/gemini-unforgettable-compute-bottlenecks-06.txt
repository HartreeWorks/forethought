**"The ‘max speed is implausibly low’ argument is pure incredulity dressed up as calibration."** The paper dismisses \( \rho<-0.3 \) largely because it implies a max progress speed under fixed compute that “seems implausible,” based on conversations and a list of plausible optimizations. But the whole question is whether those optimizations are compute-bound or idea-bound at the margin; asserting they exist does not show they can be executed without consuming the scarce resource (compute for training, evals, ablations, data generation, safety testing, etc.). More importantly, “drop in trillions of God-like AIs” is not a neutral thought experiment: it invites you to imagine capabilities that secretly smuggle in the ability to predict empirical outcomes without running them—exactly the disputed bottleneck. This isn’t a fixable empirical gap; it’s a non-argument that replaces the key parameter with vibes and then uses the vibes to reject inconvenient parameter values. If the author tries to defend by making the thought experiment more concrete, they’ll be forced to quantify which steps require how much compute—reintroducing the very ceiling they tried to wave away.