**“Small-Scale Extrapolation Mirage”** — A linchpin rebuttal is that we may not need near-frontier experiments because we can extrapolate from smaller ones, weakening compute bottlenecks. This breaks if many algorithmic changes have *scale-dependent effects* (optimizer stability, emergent capabilities, data/architecture interactions) such that performance at small scale is a poor predictor near the frontier—the exact regime that matters for pushing capability. In that world, you can do vast numbers of cheap experiments yet still be forced to validate and iterate at frontier scale, reintroducing a hard compute gate. If this objection holds, the paper must either (i) argue empirically that small-to-large transfer is reliable for the classes of innovations that drive big capability jumps, or (ii) concede that “#experiments” is not readily substitutable for frontier compute.