> The historical decline in near-frontier experiments doesn't prove they weren't bottlenecks—a few "anchor" runs may have driven most progress, and automation could make that constraint bite much harder.

**“The Near-Frontier Decline Argument Misdiagnoses the Past”** (attacks **Pivot P3: “Near-frontier experiments can’t be the bottleneck, because over 10 years the number of near-frontier experiments decreased yet algorithmic progress didn’t slow.”** This pivot is load-bearing because it dismisses the skeptic’s ‘frontier validation is fixed’ counter.) The historical observation is compatible with an alternative story: progress may have been driven by a *small number* of near-frontier runs (each producing many downstream papers/products) plus a large ecosystem of *non-frontier* work that was only useful because those few frontier anchors existed. That doesn’t imply near-frontier runs won’t bottleneck an SIE—automation changes the ratio by making hypothesis generation explode, so the constraint can newly bind even if it didn’t in the human-labor regime. If, in an SIE, the limiting factor becomes “how many frontier anchors can we afford per unit time,” then fixed compute can choke the feedback loop quickly, undermining the “few OOMs before slowing” claim. What would need to change is a retrospective attribution study estimating “marginal algorithmic progress per additional near-frontier run” over time, and projecting how that marginal value scales when cognitive labor increases by orders of magnitude.