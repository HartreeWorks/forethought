The Jones-style move—“in the longer run, we reconfigure production so \(\rho\) rises toward 0, and with fast AGIs this reconfiguration could happen in days or weeks”—is a keystone step for dismissing complementarity as a binding early bottleneck. The hidden assumption is that “reconfiguration” is primarily cognitive (new org/processes) rather than itself being compute-intensive, because the mechanisms proposed (new paradigms, new evals, new training recipes) still require running models to test and calibrate. Counter-model: suppose the only credible way to “reconfigure” to higher substitutability is to build new automated experimentation infrastructure that itself requires repeated large-scale ablation studies and training runs (to learn what to automate safely and correctly); with fixed compute, the adjustment period is compute-bounded and cannot be “days or weeks,” so the system remains in the low-\(\rho\) regime throughout the window that matters for an SIE. Then the paper’s escape hatch (“short-run \(\rho<0\), long-run \(\rho\approx 0\)”) becomes irrelevant because the “long run” arrives after the supposed explosion window. If this critique holds, the paper must explicitly model adjustment costs and show that the time-to-reconfigure is not itself bottlenecked by the same fixed compute that allegedly doesn’t bite until late.