**Load-bearing claim:**
“*We can apply the CES formula to AI R&D… Y represents the pace of AI software progress*” (Economist version)

**Attack type:**
Hidden parameter

**Break mechanism:** The CES mapping treats “pace of progress” as a smooth production output from two fungible inputs (cognitive labor and compute), but AI R&D speed has a serial critical path (train → evaluate → iterate) whose wall-clock is often dominated by the longest runs and queueing, not by total “labor”. In a world where each iteration requires a fixed-duration near-frontier training cycle (weeks) and only a small fraction of work is parallelizable, L→∞ doesn’t raise iteration rate much even if “insight” is abundant—so your inference from “ρ near 0 ⇒ no early compute bottleneck” can fail even if compute isn’t “fixed” in the CES sense. The bottleneck is not substitutability but *latency*, which CES doesn’t represent. **If this holds, you’d need to replace or augment CES with a pipeline/latency model (critical-path + parallelizable fraction) and show that the serial fraction is small enough for your “early stages” conclusion to go through.**
