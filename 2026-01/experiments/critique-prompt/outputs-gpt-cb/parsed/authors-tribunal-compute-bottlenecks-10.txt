> The real bottleneck for AI self-improvement may not be running more training experiments but *evaluating* whether each iteration is actually better and safe—a cost that algorithmic efficiency alone can't shrink.

**“Hidden Bottleneck: Evaluation and Reliability, Not Just Training”** — The paper largely equates “compute bottleneck” with training-experiment throughput, but the central conclusion (“compute bottlenecks probably don’t slow SIE until late”) assumes progress is mainly gated by running more experiments. A concrete failure mechanism is that as systems become more capable and agentic, the binding constraint may shift to *evaluation*: measuring real improvements, catching regressions, ensuring robustness and alignment, and preventing deceptive or non-generalizable gains—often requiring expensive, broad, and adversarial testing (frequently at large scale). If eval is the true bottleneck, then algorithmic efficiency that lets you run more cheap trainings doesn’t buy proportional progress because the expensive part is proving the system is actually better and safe enough to deploy for further R&D. If this holds, the paper must extend its model from “experiments” to “validated progress,” and its early-stage SIE optimism may need major qualification.