> Optimising for the "fastest route" under competitive pressure will systematically favour under-validated approaches, displacing compute costs into catastrophic post-hoc debugging rather than eliminating them.

**Load-bearing claim:**
“*There are multiple possible routes… a ‘strongest link’ framing… We’ll ultimately use whichever has the most favourable ρ*” (Counterargument 7)

**Attack type:**
Strategic response (Goodhart/selection effects)

**Break mechanism:** Selecting the “route with the most favorable ρ” can systematically select for methods that *appear* to work without compute-heavy validation but are actually under-validated, leading to brittle or deceptive systems that later require massive compute to debug, align, or verify—reintroducing the compute bottleneck at the worst time. In a world with strong incentives to accelerate, labs may prematurely adopt low-validation approaches (synthetic data flywheels, speculative architecture changes, aggressive distillation) that shift costs from “experiment compute” to “post-hoc failure recovery compute,” so the true bottleneck is displaced, not removed. Your argument treats route choice as a free optimization without adversarial incentives or hidden failure costs. **If this holds, you’d need to incorporate an endogenous “validation/verification” constraint and show that the fastest route under competitive pressure is still robustly non-compute-bottlenecked when accounting for downstream correction costs.**
