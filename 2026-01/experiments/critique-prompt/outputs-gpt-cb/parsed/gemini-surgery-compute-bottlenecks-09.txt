The paper operationalizes SIE as “\(\ge 5\) OOM increase in effective training compute in <1 year without more hardware,” and then argues compute bottlenecks likely don’t bite until late because algorithmic progress can raise “effective compute.” This relies on a hidden monotonicity: that algorithmic improvements predominantly *reduce* the compute needed for a given capability target, rather than also enabling/inducing shifts to more compute-hungry training objectives, modalities, or safety constraints that absorb the savings. Counter-model: as algorithms improve, labs expand the ambition set (larger context, richer world models, more agentic training, heavier eval/regression suites), so net compute per “frontier step” stays flat or rises; “effective training compute” doesn’t jump 5 OOM because the frontier definition moves with the method. Then the paper’s conclusion (“compute bottlenecks don’t slow early SIE”) fails even though algorithmic efficiency is improving—because the efficiency is endogenously spent. If this holds, the author must revise the metric to something invariant to shifting frontiers (e.g., capability per joule at fixed task distribution) and show that invariant improves by 5 OOM on a <1-year timeline under fixed hardware.