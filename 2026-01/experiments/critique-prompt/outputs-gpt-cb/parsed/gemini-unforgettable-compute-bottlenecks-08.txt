**"Your ‘smarter/faster researchers’ point double-counts compute and erases the bottleneck by definition."** The paper says economic estimates of \(\rho\) miss “smarter workers” and “faster thinking,” and therefore understate substitutability in AI R&D. But for AI systems, faster thinking and smarter inference are not free: they require either more compute per unit time (higher throughput) or more compute per thought (larger models / longer reasoning traces), both of which intensify the compute constraint rather than relax it. Treating “faster thinking” as an increase in \(L\) that doesn’t draw on \(K\) is exactly the mistake a compute bottleneck argument targets. This isn’t a minor modeling tweak; it flips the sign of the claimed effect—smarts and speed can *tighten* the compute bottleneck by increasing the compute required to reach a given level of research output quality. If the author defends by positing algorithmic improvements that make inference cheaper, they’re back in the circularity trap: those improvements themselves must be discovered and validated under the bottlenecked regime.