> Even if small-scale experiments abound, many algorithmic insights only emerge at near-frontier scale, so the real bottleneck may be how many full-size training runs you can afford.

**"Near-frontier experiments might be indispensable, not optional"**  
A key rebuttal is that progress might not require near-frontier experiments because you can extrapolate from smaller runs or optimize experimentation. But many algorithmic ideas only reveal their value (or failure modes) at scale: optimization stability, emergent capabilities, long-horizon credit assignment, tool-use reliability, and data/architecture interactions can be qualitatively different at frontier sizes. If that’s right, then the number of informative experiments is tightly coupled to the ability to run large trainings, which is compute-limited by design. The paper’s claim that “near-frontier experiments have declined yet progress continued” may conflate algorithmic progress with scale-driven gains and better engineering around a small number of huge runs. Even if you can do lots of small experiments, the binding constraint could remain “how many full-scale training shots can you afford.”