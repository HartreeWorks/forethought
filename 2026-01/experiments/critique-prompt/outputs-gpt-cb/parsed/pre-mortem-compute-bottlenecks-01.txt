**“Inference-Compute Trap”** — The paper’s central move was to treat “cognitive labour” (lots of AGI researchers) as an input that could surge while “compute” stayed roughly fixed, implying compute bottlenecks wouldn’t bind early. After 2028, labs operationalized this by spinning up millions of research-agent copies and shifting budgets from new training clusters to agentic automation, expecting software gains to outrun hardware. The mechanism that broke was the hidden identity between labour and compute: those “AGI researchers” consumed vast inference compute, memory bandwidth, and datacenter networking, so adding labour *was* adding compute demand, not substituting for it. Step by step, inference contention starved experiment execution, slowed iteration loops, and pushed teams to cut evaluation and reproducibility to keep throughput; several high-profile “algorithmic breakthroughs” failed in production because they were never validated under realistic load. The paper missed that in AI R&D, “labour” is instantiated as compute-hungry processes whose marginal productivity collapses once shared infrastructure saturates (especially bandwidth/latency), so treating L and K as separable inputs materially mis-modeled the early-stage bottleneck.