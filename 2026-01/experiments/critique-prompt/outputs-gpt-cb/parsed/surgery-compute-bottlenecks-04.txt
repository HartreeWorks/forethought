The argument that “the real bottleneck is number of experiments, and algorithmic efficiency improvements let you run more experiments at fixed compute” is load-bearing for dismissing fixed-K ceilings. **Causal reversal:** many of the efficiency gains that matter for capability (not just cheaper replications) are exactly those that *increase* the appetite for near-frontier experiments—because each promising idea must be validated on the hardest regimes to avoid Goodharting on small-scale proxies (distribution shift, long-horizon agency, tool use, etc.). In that world, algorithmic progress increases the *marginal value* of scarce frontier experiments, tightening rather than loosening the compute constraint for the decisions that steer the trajectory. If this critique holds, “compute-efficient experiments” doesn’t pull the rug out from under the bottleneck objection; it can make compute more binding precisely where the paper needs it to become less binding.