> Efficiency gains don't automatically buy more experiments—competitive incentives funnel freed compute into fewer, larger flagship runs, shrinking the diversity of trials and weakening the science.

**“Efficiency Gains Ate the Experiment Budget”** — The paper argued that algorithmic efficiency improvements effectively increase the number of experiments you can run under fixed compute, weakening the compute bottleneck. The idea was implemented as a strategic doctrine: prioritize efficiency research because it “pays for itself” by freeing compute for more trials and faster progress. The mechanism that broke was the incentive feedback: any freed compute was immediately reallocated to train larger flagship models (marketing and benchmark dominance), not to expand the breadth of experiments, so the *effective* number of falsifying trials didn’t rise. The cascade was a research monoculture: fewer independent replications, fewer ablations, and weaker model science, leading to repeated overfitting to leaderboards and surprise failures in deployment domains (medicine, finance, critical infrastructure) that had been under-tested. The paper missed that “more experiments” is not an automatic consequence of efficiency—allocation is governed by competitive incentives, and those incentives reliably concentrate compute into a small number of giant runs that reduce learning per FLOP.