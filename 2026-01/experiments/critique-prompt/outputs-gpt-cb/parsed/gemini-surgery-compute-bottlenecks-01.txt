The paper’s central move is to treat “pace of AI software progress” as a smooth CES production output \(Y=f(K,L)\), where \(K=\) compute and \(L=\) cognitive labour, and then read “max speed” off the \(L\to\infty\) limit. That mapping hides a load-bearing assumption: that marginal progress is continuously producible by mixing compute and cognition, rather than being gated by a small number of discrete, high-cost validation events. Counter-model: suppose most candidate algorithmic ideas are cheap to generate (lots of \(L\)) but only become knowably correct after one or two full-scale training runs that each consume ~30–50% of annual compute; then “progress speed” is dominated by a serial queue of frontier trainings, so \(L\to\infty\) barely changes wall-clock progress. Under this model, the CES ceiling is not “high but finite,” it is effectively the inverse of frontier-run cycle time, and it bites immediately. If this critique holds, the paper must replace CES-with-\(Y=\)progress with a model where progress is a function of (i) idea generation rate and (ii) frontier-validation throughput, and show the second term is not the binding constraint in early SIE.