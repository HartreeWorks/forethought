> Each frontier experiment may now yield far more insight than a decade ago—thanks to scaling laws, standardisation, and better tooling—so fewer experiments can still drive rapid progress without disproving the bottleneck.

**Load-bearing claim:**
“*Over the past ten years, the number of near-frontier experiments… decreased… If these… were truly a bottleneck… progress would have slowed down*” (Counterargument 5.2)

**Attack type:**
Dominant alternative

**Break mechanism:** The observation is also consistent with a world where near-frontier experiments *are* bottlenecked, but overall progress was carried by other changing margins you don’t control for (larger single runs, better data pipelines, better hardware utilization, transfer learning, open-source spillovers, and—crucially—evaluation standards shifting). Fewer near-frontier experiments can coexist with rapid progress if each one produces more learning because model families are more standardized and scaling laws make single large runs unusually informative. Your inference implicitly assumes a stable “learning-per-frontier-run” and stable evaluation difficulty across time; if those moved, the historical argument doesn’t identify the bottleneck. **If this holds, you’d need to decompose historical progress into (a) number of frontier-equivalent experiments, (b) information gained per experiment, and (c) shifting goalposts, and then show that (a) is not the limiting factor even after controlling for (b) and (c).**
