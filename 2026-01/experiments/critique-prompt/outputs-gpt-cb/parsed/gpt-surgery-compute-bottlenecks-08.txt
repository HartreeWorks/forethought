**Node attacked:** “*The implied ‘max speed’ … between 2 and 100… I think a max speed below 10 is implausible… informed by… optimising every part of the stack, generating way better ideas…*” (Counterargument 6). **Attack type:** Reference class sabotage (and hidden parameter). **Break mechanism:** Your “implausibly low” judgment leans on a reference class of software optimization where iteration is cheap, but frontier AI R&D includes non-software frictions that don’t scale with more cognition: cluster scheduling, hardware faults, distributed training brittleness, eval-suite construction, red-teaming, interpretability work, and integration testing. If those dominate the cycle time, then even “trillions of superintelligent researchers” don’t buy 10× speed because the binding constraints are organizational/operational and physical. That makes the low max-speed implications of negative ρ *plausible* rather than absurd, undermining your key move of rejecting most economic ρ estimates by intuition. **If this holds, you’d need to anchor “max speed” with an explicit breakdown of the end-to-end iteration loop (what fraction is parallelizable vs latency-bound vs compute-bound) and show why those non-idea frictions don’t cap speed below your thresholds.**