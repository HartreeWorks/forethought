"The Non-Stationary Benchmark Illusion" — You rebut the near-frontier bottleneck by noting that “over the past ten years, the number of near-frontier experiments the world can run has decreased” while algorithmic progress continued, implying near-frontier experiments aren’t limiting. That inference breaks because the meaning of “near-frontier” is non-stationary: as training runs balloon, the *information gained per near-frontier run* also changes (bigger models unlock qualitatively new empirical signals), so fewer runs can still dominate progress. Step-by-step: if each frontier run is more informative (or more capability-relevant) than prior-era runs, then a decline in count is not evidence of non-bottleneck; it can be exactly what a compute bottleneck looks like—progress concentrated into rare, expensive trials. If this objection holds, you need to analyze marginal information gain per unit compute across scales (how much algorithmic learning comes from one 2026-scale run vs many 2016-scale runs), rather than using raw “number of near-frontier experiments” trends as the decisive empirical intuition.