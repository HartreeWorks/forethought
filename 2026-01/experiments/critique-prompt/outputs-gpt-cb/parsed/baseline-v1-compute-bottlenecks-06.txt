The “near-frontier experiments aren’t necessary” reply is insufficiently defended and ignores counterevidence from scaling-centric ML practice. Many capabilities and failure modes only appear at large scale (emergent behaviors, optimization instabilities, long-context effects), making extrapolation from small runs systematically unreliable. If frontier phenomena dominate progress, then compute could remain a hard bottleneck even with abundant cognitive labor.