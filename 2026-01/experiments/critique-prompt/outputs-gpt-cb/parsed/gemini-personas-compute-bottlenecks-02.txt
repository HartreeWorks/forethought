[The Empirical Hardliner] Target claim: “Over the past ten years, the number of near-frontier experiments… decreased… If near-frontier experiments were truly a bottleneck, algorithmic progress would have slowed.” Failure mechanism: you never specify a causal model linking near-frontier experiment count to measured “algorithmic progress,” and you ignore confounding from architecture shifts, data scaling, infrastructure, and benchmarking drift. Agent A (benchmark designers / the community) introduces new tasks and scoring practices, causing System B (apparent progress rate) to rise even if near-frontier experimentation is constrained. Consequence: your key rebuttal to the “1% of compute” argument collapses because your observation (“progress didn’t slow”) doesn’t falsify the bottleneck hypothesis—it’s consistent with progress coming from non-frontier sources while frontier remains binding for the next paradigm.