"Ambiguous and Potentially Stipulative Definition of ‘SIE’" — The paper defines an SIE as “>=5 OOMs of increase in effective training compute in <1 years without needing more hardware,” but earlier it describes SIE as “AI improving AI algorithms leads to accelerating progress without any additional hardware.” “Effective training compute” conflates algorithmic efficiency, better data, improved architectures, and possibly changed objectives into a single scalar, yet the compute-bottleneck objection is about *physical throughput and empirical validation* rather than a re-labeled efficiency metric. The definition also builds in a particular target (5 OOM in a year) that may not track “superintelligence” or socially relevant capability jumps, and it risks making the debate hinge on an arbitrary threshold. Moreover, the paper’s earlier “max speed” discussion concerns “pace of progress” while the later definition concerns “effective training compute,” which are not the same variable; improvements might increase effective compute without proportionate speedups (or vice versa). This ambiguity weakens the conclusion because it becomes unclear whether the argument shows “no compute bottleneck,” or merely that some efficiency metric can rise quickly even if real-world progress remains bottlenecked by training/evaluation time and other constraints.