The rebuttal to “near-frontier experiments are fixed” is: (i) maybe we can extrapolate from smaller experiments, and (ii) “over the past ten years, the number of near-frontier experiments … has decreased; if they were a bottleneck, progress would have slowed.” The inference step is brittle because it assumes that “near-frontier experiment count” is the binding statistic, rather than “total frontier-equivalent compute devoted to a small set of decisive runs,” which can rise even as the *count* falls. Counter-model: the world runs fewer near-frontier trainings, but each is vastly larger and more instrumented, and the downstream ecosystem (fine-tuning, distillation, scaffolding, data curation) is keyed to those few runs; progress can continue as long as those flagship runs keep growing, but if compute is held constant those runs cannot grow and the whole pipeline slows. This counter-model fits the historical observation (count down, progress up) while reversing the paper’s conclusion about bottlenecks. If this holds, the author must replace the “count decreased ⇒ not bottleneck” argument with an analysis keyed to (a) frontier-equivalent compute per year and (b) marginal capability gain per frontier-equivalent compute, not raw run counts.