> The "strongest link" argument only works if the alternative routes to progress are independent, but they all depend on the same compute-hungry primitives like base models and large-scale eval.

**“Strongest-Link Optimism Ignores Correlated Compute Dependence”** — The “strongest link” framing claims there are multiple routes to progress (scaffolding, data flywheels, better experiments, extrapolation), and only one needs to be weakly compute-bottlenecked. This breaks if those routes share correlated dependencies on compute-intensive primitives: strong base models, large-scale eval, extensive fine-tuning, synthetic data generation at scale, and repeated red-teaming—each consuming substantial training and inference compute. If the routes are not independent but all bottleneck on the same compute budget, then “we’ll pick the least bottlenecked route” offers little relief. If this holds, the paper must map the candidate routes to a common compute ledger and show at least one plausible path where frontier capability gains can be reliably achieved with modest compute growth.