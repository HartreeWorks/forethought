[The Second-Order Catastrophist] Target claim: “Define an SIE as ≥5 OOM increase in effective training compute in <1 year without more hardware… 10–40% chance.” Failure mechanism: your “effective training compute” definition incentivizes a perverse optimization target where labs chase paper gains (synthetic data loops, eval gaming, distillation cascades) that spike “effective compute” metrics while eroding epistemic grounding and safety margins. Agent A (a lab exec) mandates hitting “5 OOM effective compute” via aggressive automation and shortcut methods, causing System B (model reliability and alignment validation) to fail because the organization can no longer tell whether capability gains are real or just metric inflation. Consequence: you don’t get a clean SIE—you get an un-auditable, self-referential model ecosystem where nobody can evaluate true capability or risk, which is worse than a compute-bottlenecked world because it destroys the possibility of informed intervention.