> Hardware bottlenecks aren't just "compute"—many big algorithmic wins trade FLOPs for memory bandwidth or interconnect, so on fixed clusters, smarter software can't always substitute for better hardware.

**"Hardware-Algorithm Mismatch Trap"** — The paper argues that **high substitutability between cognitive labor and compute** implies **ρ closer to 0 in AI R&D than in manufacturing**, because **AGIs can reconfigure workflows and find methods that use available compute more effectively** (Jones-style adaptation). But many of the largest algorithmic wins in modern ML are *not compute-fungible* on fixed hardware: they trade FLOPs for memory bandwidth, cache locality, communication patterns, or specialized kernels (e.g., sparse MoE routing, long-context attention variants, speculative decoding, low-precision formats, custom collectives). On a fixed cluster, these “software improvements” can be unusable or even net-negative because the bottleneck is interconnect/VRAM/latency rather than raw FLOPs, so cognitive labor cannot convert into progress without *hardware rebalancing*. This undermines the paper’s repeated move of treating “compute” as a single scalar K that software can always exploit better. If this holds, the argument would need to replace K with a vector of binding hardware constraints and show that software gains predominantly relax the currently-binding constraint rather than shifting pressure to another.