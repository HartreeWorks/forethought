**“Efficiency Gains Don’t Automatically Buy More Experiments”** (attacks **Pivot P3: “When algorithms become twice as efficient, you can run twice as many experiments… labs can increase both cognitive labour and # experiments, pulling the rug out from the compute-bottleneck argument.”** This is load-bearing because it’s the paper’s main rebuttal to ‘K fixed’.) The rebuttal assumes efficiency improvements translate into *experiment throughput* at a fixed capability level, but during an SIE the objective is not “more experiments at yesterday’s scale”—it’s discovering changes that work at (or near) frontier regimes where behavior shifts discontinuously. Historically, major efficiency gains (better optimizers, architectures, sparsity) are frequently “spent” on pushing scale/capability, not on multiplying independent experimental shots; the organization still ends up running a small number of massive runs whose wall-clock and risk dominate iteration speed. If that pattern persists, then fixed compute still binds the rate at which frontier hypotheses can be validated, and the paper’s “bottlenecks unlikely to slow early stages” conclusion collapses. What would need to change is an accounting identity (with data) that partitions compute into (a) hypothesis-testing runs, (b) capability-scaling runs, and (c) integration/debugging, showing that efficiency gains plausibly expand (a) fast enough to sustain ≥5 OOM “effective training compute” progress within a year.