**“The CES Proxy Breaks the Object of Study”** (attacks **Pivot P1: “Apply a CES production function where L=cognitive labour, K=compute, Y=pace of AI software progress; then for −0.2<ρ<0 compute bottlenecks won’t bite until late stages.”** This pivot is load-bearing because the paper’s “late-stage only” claim is derived almost entirely from CES-shaped diminishing returns.) The CES mapping assumes “pace of progress” behaves like an aggregate production output with smooth marginal rates of substitution, but AI R&D progress is often *thresholdy* (e.g., a single training run enabling a new regime) and mediated by discrete bottlenecks (cluster scheduling, evaluation suites, data pipelines) that don’t average out. If the true relationship is closer to a “minimum of (ideas, validated experiments)” or a queueing system where experiments are the service bottleneck, then even small complementarity can create *hard caps* well before 5 OOM of “effective training compute.” Under that alternative, the sensitivity plot and “early stages unaffected” inference no longer follows, and compute scarcity can slow the loop as soon as automated researchers scale faster than experiment throughput. What would need to change is a model grounded in the mechanics of experiment throughput/information gain (e.g., queueing + value-of-information) that reproduces the “late-stage only” conclusion without assuming CES smoothness.