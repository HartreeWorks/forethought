> When the things that break frontier models—instabilities, emergent deception, long-context failures—only show up at scale, you can't shortcut validation with small experiments, so compute stays a hard bottleneck.

**“Small-Scale Extrapolation Fails Exactly Where SIE Needs It”** (attacks **Pivot P4: “You might not need near-frontier experiments… you might extrapolate from experiments that use increasingly small fractions of the lab’s compute.”** This is load-bearing because it’s the key move that lets progress accelerate without increasing K.) Many of the failure modes that matter for frontier capability—training instabilities, emergent tool use, deception risks, long-context pathologies, data contamination effects, distributed training quirks—are weak or absent at small scale and appear nonlinearly. If extrapolation is unreliable, then each serious algorithmic candidate requires near-frontier validation, reinstating compute as a hard gate and making ρ effectively more negative than the paper’s preferred range. Concretely, the “≥5 OOM effective training compute in <1 year” definition becomes implausible under fixed hardware because the number of required frontier validations rises with the number of generated ideas. What would need to change is evidence from recent frontier-era ML showing that a large fraction of major improvements were predictable from ≤1% scale tests, with quantified transfer reliability (not just anecdotal plausibility).