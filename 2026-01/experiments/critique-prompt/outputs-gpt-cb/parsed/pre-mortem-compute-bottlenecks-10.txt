> Framing compute as the one lever that "still mattered" invited regulators to build single-point controls that actors routed around, fragmenting oversight while barely slowing capability gains.

**“Compute-Cap Policy Backfire”** — The paper’s bottom line—“good chance compute bottlenecks don’t slow an SIE until late stages”—was widely read by regulators as meaning that *software* progress could outrun hardware governance, so they focused on compute-centric controls as the lever that still mattered. Between 2029 and 2033, several jurisdictions enacted strict compute caps and reporting thresholds intended to prevent runaway software acceleration without needing to regulate models directly. The broken mechanism was the paper’s implicit claim that holding compute roughly fixed doesn’t prevent large capability jumps; in reality, the caps didn’t stop progress but reshaped it toward distributed, opaque, and under-evaluated training (splitting runs across entities, using inference-time adaptation, and shifting to unreported “edge clusters”). The cascade was worse oversight with similar capability advance: enforcement lagged, safety evals became harder, and a fragmented ecosystem of partially compliant actors deployed powerful systems with minimal centralized testing, producing systemic failures in automated trading and cyber-defense coordination. The paper missed how its compute-bottleneck analysis, once translated into policy, encouraged a single-lever regulatory approach that actors could route around—making the world less legible and less governable without actually buying much safety.