You argue that near-frontier experiments may not be necessary because you can extrapolate from smaller runs and because progress didn’t slow when near-frontier experiments allegedly got rarer. That creates an exploitable vulnerability: if your R&D process increasingly relies on extrapolation and proxy experiments, a sophisticated adversary can poison the inference channel—tamper with datasets, contaminate benchmarks, or seed architectures that look good at small scale but fail catastrophically at frontier scale. The more you compress validation due to compute scarcity (early stopping, smaller-scale proxies), the easier it becomes to smuggle in “progress” that is actually brittle, unsafe, or non-generalizing. The paper never specifies an adversary model, yet its strongest-link framing (“we’ll use whichever route works”) is exactly what an attacker exploits by targeting the measurement of “works.” If this objection holds, the project can burn enormous effort on illusory gains and then hit sudden, late-stage failures that look like “compute bottlenecks” but are actually adversarially induced epistemic collapse.