[The Adversarial Red-Teamer] Target claim: “Different routes to producing superintelligence… you just need one of them to work… The compute bottleneck objection only works if all routes are bottlenecked by compute.” Failure mechanism: you treat routes as independent, but in real labs they share failure modes: evaluation is compute-heavy, debugging is compute-heavy, and adversarial robustness requires compute-heavy red-teaming—so compute binds even when “idea generation” doesn’t. Agent A (a malicious insider or rival) poisons training data or model weights, causing System B (the supposed low-compute ‘route’ like scaffolding/extrapolation from small experiments) to silently fail at deployment because validation at scale is the bottleneck you hand-waved away. Consequence: you encourage a strategy of skipping compute-intensive verification, which is exactly how you get catastrophic model misbehavior at the moment the system is most powerful.