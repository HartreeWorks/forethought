The paper’s “bottleneck is # experiments, and algorithms can become more compute-efficient, so you can run more experiments holding compute fixed” conflates two different notions of efficiency: capability-per-FLOP at deployment versus research-search cost to discover the next improvement. In modern ML, many algorithmic gains are discovered by running *more* or *larger* sweeps (hyperparameters, data mixtures, RL rollouts, evals) and then paying additional compute to validate, ablate, and reproduce; the search process can scale superlinearly even if the final model is more efficient. Your rebuttal to “near-frontier experiments matter” leans on the past decade, but that decade also featured a giant expansion in aggregate compute and industrialization of the experiment pipeline—exactly the confound that breaks your inference. You never quantify the share of progress attributable to sub-frontier experiments versus frontier-scale training runs, so “you might not need near-frontier experiments” is an unsupported pivot. If this objection holds, algorithmic improvements do not relax the compute constraint the way you claim, and the compute bottleneck can bite *earlier* precisely because the research process demands expensive validation.