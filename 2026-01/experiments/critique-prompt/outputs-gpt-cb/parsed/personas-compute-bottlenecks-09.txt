You dismiss manufacturing-based substitution estimates as poor analogues because AI has “optimizing every part of the stack” and “running smaller scale experiments,” implying much higher substitutability than sectors like factories. A closer historical analogue is not manufacturing but other empirically gated R&D domains—drug discovery, aerospace, semiconductors—where floods of researchers and better theory still hit hard experimental/validation chokepoints (wet-lab assays, wind tunnels/flight tests, fabrication cycles). In those fields, “reconfiguration” didn’t compress validation cycles from years to days just because more cognition was available; the bottleneck moved to the most reality-coupled tests, and progress became gated by the slowest trustworthy feedback. Frontier ML training runs are exactly that kind of reality-coupled test for many claims about generalization and robustness, and your paper offers no reason they won’t play the same role as fabrication runs in chip design. If this objection holds, using “software is different” to infer ρ near 0 is historically naïve, and compute-heavy validation remains the pacing item that prevents your months-scale explosion.