"The Self-Undermining Efficiency Loop" — You argue that algorithmic efficiency improvements let you run more experiments at fixed compute, “pulling the rug” from under compute bottlenecks. The inference fails because the same efficiency gains move the target: as training gets cheaper, the economically rational frontier shifts to larger models/longer training (new capabilities become accessible), and “near-frontier” stays near the full budget, preserving the bottleneck. Mechanism: efficiency reduces cost per FLOP-equivalent capability ⇒ labs scale up ambition to chase the next capability regime ⇒ the marginal experiment that decides the next step remains compute-maximal ⇒ fixed hardware still fixes the rate of frontier transitions. If this objection holds, you need to model the frontier endogenously (how “largest useful run” grows as algorithms improve) and show that the relevant validation/training workload *doesn’t* re-expand to fill the budget.