[The Security Engineer] **Target claim:** “you might not need near-frontier experiments… you can extrapolate from smaller fractions of compute… the near-frontier bottleneck ‘isn’t convincing.’” **Failure mechanism (adversarial adaptation / Goodhart):** the adversary (including reckless competitors) will exploit any regime that legitimizes non-frontier extrapolation by doing the dangerous stuff at the frontier while presenting piles of ‘safe’ small-scale evidence. Frontier behavior is where emergent tool-use, jailbreak surface area, and deception risks spike; “we validated on 0.1% runs” becomes a safety theater artifact that systematically misses the failure modes you actually care about. **Consequence:** your attempt to dissolve the near-frontier constraint makes it easier to hide capability leaps and harder to detect them, raising the probability of surprise deployment of systems whose risk profile cannot be inferred from the small-scale regime your argument privileges.