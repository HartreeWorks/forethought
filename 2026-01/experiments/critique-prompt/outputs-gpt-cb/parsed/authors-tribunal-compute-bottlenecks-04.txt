> Progress may have continued despite fewer frontier experiments simply because scaling compute and better data drove gains that only occasionally needed frontier-scale validation.

**“The Near-Frontier Trend Doesn’t Falsify Bottlenecks”** — The paper argues that because the number of near-frontier experiments has decreased over a decade while progress continued, near-frontier experiments cannot be the bottleneck. The failure mechanism is confounding: progress over the past decade was heavily driven by *increasing training compute, better data pipelines, and architectural paradigm shifts* that may require only occasional frontier runs but still be compute-gated at key validation points. Moreover, “number of near-frontier experiments” is not measured; depending on how you count (aborted runs, hyperparameter sweeps, distributed training variations, multiple labs), it may not have decreased in the relevant sense. If this objection holds, the paper must replace the rhetorical “proves too much” move with a quantitative accounting tying historical progress rates to an estimated frontier-experiment budget.