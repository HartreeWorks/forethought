**"Parallel researcher copies face steep coordination and verification overheads"**  
A software intelligence explosion story leans heavily on scaling cognitive labor via many fast copies. But research does not parallelize linearly: you quickly hit coordination costs, duplicated work, integration complexity, and a growing burden of verifying results under distribution shift and subtle bugs. As systems become more capable and more complex, the cost of evaluation and the risk of regressions increase, which can force longer cycles and more conservative release practices. This can create an endogenous bottleneck that looks like “compute” only partly; it’s also about experiment design, oversight, and systems engineering. If marginal cognitive labor yields diminishing returns due to these overheads, then the effective elasticity between “research labor” and “compute” may be low even if experiments themselves were cheap.