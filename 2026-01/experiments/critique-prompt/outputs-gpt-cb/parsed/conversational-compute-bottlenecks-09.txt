**"Historical evidence can be read the opposite way: frontier compute has been the main driver"**  
The paper notes that near-frontier experiment counts may have fallen while progress continued, suggesting near-frontier scarcity isn’t binding. But a skeptic can argue the opposite: progress continued *because* frontier compute per run increased dramatically, and algorithmic improvements were often complementary to scale rather than substitutes for it. Many headline capability jumps align with scaling up training/inference, better data pipelines, and longer/stronger post-training—each compute-intensive. If scaling has historically been the dominant lever, then holding hardware fixed is a much stronger constraint than the paper implies. The inference “progress happened despite fewer near-frontier runs” can be misleading if a small number of gigantic runs plus lots of medium-scale ablations is exactly the compute-limited regime we should expect.