[The Game-Theoretic Defector] Target claim: “When your AI algorithms become twice as efficient, you can run twice as many experiments… this pulls the rug out from the sceptical argument which assumed an essential input was held fixed.” Failure mechanism: “algorithmic efficiency” becomes the metric everyone Goodharts—labs will report efficiency gains via narrower distributions (task-specific tricks, distillation that hides upstream compute, or offloading to proprietary pretraining) while the real bottleneck (frontier training + eval compute) stays fixed. Agent A (a competing lab) re-labels outsourced compute and amortized pretraining as “sunk,” causing System B (your ‘effective # experiments’) to appear to explode even though total required frontier FLOPs hasn’t budged. Consequence: you predict an SIE based on fake substitutability created by accounting tricks, then you’re blindsided when “more experiments” fails to translate into new capability jumps because the real scarce input was concealed.