> Scaling up AI "researchers" eats into the same compute budget needed for experiments and training, so adding more cognitive labour isn't free—it cannibalises the very resource it's supposed to supplement.

**“L and K Aren’t Independent Here”** — The paper attacks the compute-bottleneck objection by increasing substitutability between cognitive labor and compute, but the central inference assumes we can vary L upward while holding K fixed. In reality, “cognitive labour” in an AI-R&D-automation scenario is itself instantiated as inference compute (agent runs, tool use, long-horizon deliberation), so raising L *necessarily consumes K* unless you posit some separate non-compute substrate. That means the relevant constraint is not CES with separable inputs, but a budget constraint where agents and experiments draw from the same compute pool, often turning “more researchers” into fewer experiments. If this holds, the paper must reformulate the argument around an explicit compute-allocation model (agents vs training vs eval), and the claim that compute bottlenecks “likely don’t bite until late stages” becomes much harder to sustain.