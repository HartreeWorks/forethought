> Treating substitutability as fixed ignores that cheap automated ideas could shift the bottleneck toward costly empirical validation, making compute binding earlier than the model predicts.

**“Constant ρ Is Doing Hidden Heavy Lifting”** (attacks **Pivot P1: “Within −0.2<ρ<0 the predictions don’t differ much until cognitive labour grows by ~5 OOMs, so compute bottlenecks probably don’t block early SIE.”** This is load-bearing because the conclusion hinges on *stage-invariance* of the substitutability.) The paper treats ρ as roughly stable across massive regime change, but in AI R&D the complementarity can *increase* precisely when automation makes idea-generation cheap: the marginal idea becomes more dependent on costly validation, integration, and large-scale failure-mode discovery. If ρ endogenously drifts more negative as L explodes (because “cheap ideas” saturate and the remaining ones are increasingly empirical), then the “won’t bite until late” result can flip: compute becomes binding *earlier*, not later, even if −0.2<ρ<0 held in today’s regime. Concretely, an SIE defined as ≥5 OOM “effective training compute” in <1 year could stall after 1–2 OOM if the marginal returns become validation-dominated once automated labor scales. What would need to change is an explicit mechanism for ρ(L/K) (or a non-CES model) plus evidence from historical ML where marginal algorithmic gains became increasingly tied to large runs as low-hanging fruit was exhausted.