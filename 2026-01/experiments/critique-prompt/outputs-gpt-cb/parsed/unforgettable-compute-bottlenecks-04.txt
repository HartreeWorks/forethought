> Frontier AI progress may work more like unlocking discrete capability thresholds than smooth substitution, meaning compute constraints feel irrelevant until they suddenly become absolute blockers.

**"Thresholded Discovery, Not Smooth CES"** — The paper argues that **even if ρ<0, bottlenecks likely bite late** because **CES implies a smooth approach to a ceiling and small differences in ρ don’t matter for early OOMs**. But frontier ML progress often behaves like *threshold phenomena*: certain capabilities (robust tool-use, long-horizon planning, reliable self-improvement scaffolds) may require crossing discrete training/validation thresholds that simply cannot be met with constant compute, no matter how much cognitive labor you add. In that world, the “max speed” is not a smooth curve; it’s gated by hitting specific expensive experiments that unlock new regimes, and below the gate you can optimize forever without triggering the next phase transition. This reverses the paper’s comfort about early-stage acceleration: compute constraints can be irrelevant until suddenly they are absolute. If this holds, the analysis would need to model R&D as crossing compute-gated milestones (with lumpy minimum experiment sizes), not as continuous marginal substitution.