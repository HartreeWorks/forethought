> If trustworthy adoption requires verification that scales superlinearly with capability, then more AI-generated research breakthroughs just create a growing audit backlog that chokes the pipeline.

**"The Verification Tax Eats the Explosion"** — The paper argues that **automating AI R&D** implies **accelerating progress that outruns societal response**, because **the limiting factor is discovering better algorithms, not deploying them safely**. But an SIE requires not just invention; it requires *trustworthy adoption* of new training procedures, optimizers, architectures, and agent scaffolds—otherwise labs rationally slow-roll changes that might silently degrade reliability, security, or alignment properties. As capabilities rise, the cost of verifying that a new method is not deceptively misgeneralizing, data-poisoning-sensitive, gradient-hacking-prone, or jailbreak-amplifying can scale superlinearly and become the real “compute for experiments” bottleneck (e.g., massive eval suites, red-teaming, mechanistic checks, interpretability probes). This turns “more cognitive labor” into “more proposed changes that must be audited,” which can *slow* the pipeline rather than speed it. If this holds, the paper would need to integrate a verification/assurance production function where the marginal cost of safely integrating improvements grows with capability, potentially making compute bottlenecks bite early via evaluation rather than training.