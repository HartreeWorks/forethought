> The entire "implausibly low speed limit" argument hinges on fixing compute's share parameter at 0.5, but if frontier ML is more compute-heavy, the same elasticity values yield much lower ceilings.

A key load-bearing step is the translation from ρ estimates to a “max speed of AI software progress,” holding α=0.5 throughout. **Parameter sensitivity:** in the CES function, α (the share/weight on compute) is exactly what controls how quickly output saturates when K is fixed, and there’s no paper-specific reason AI R&D should be near α=0.5 rather than, say, α≫0.5 for frontier ML where experiments dominate costs. If α is 0.8 instead of 0.5, the same ρ produces a much lower ceiling on speedups from extra “labour,” undercutting the “implausibly low max speed” argument used to push ρ toward 0. If this critique holds, the paper’s central numerical intuition (e.g., “ρ=-0.2 implies ~32×”) is not robust enough to support the conclusion about bottlenecks arriving only in late stages.