> The critique's core bottleneck isn't generating better algorithms but the slow, compute-heavy cycle of retraining and deploying improved AI researchers, which could keep the feedback loop's gain below explosive thresholds.

**"An SIE requires not just faster discovery but faster deployment of better researchers"**  
Even if AI can accelerate algorithm discovery, the feedback loop requires those discoveries to rapidly produce *better AI researchers* (i.e., materially improved models/agents) that then further accelerate R&D. That step often involves expensive training, extensive post-training, infrastructure changes, and integration into tooling—each potentially compute- and time-heavy. If retraining and validating the next “generation” is slow under fixed compute, then the loop’s gain may be <1 over relevant time windows, preventing explosion dynamics. Put differently: accelerating “ideas per day” is not enough if “new effective researcher per idea” is bottlenecked by long, compute-heavy training cycles. This is a central structural objection because it targets the feedback loop itself, not merely the pace of incremental research within a fixed model generation.