**“Frontier-Only Algorithm Regime”** — The paper dismissed the “fixed number of near-frontier experiments” bottleneck by arguing progress hadn’t slowed even as frontier runs grew, implying frontier scarcity wasn’t binding. Influenced by that, 2030-era governance and lab roadmaps deprioritized capital-intensive testbeds (full-scale ablation clusters, redundant frontier runs) and instead maximized agent-driven idea generation. The mechanism that broke was a structural shift in research: once scaling hit tighter data/architecture limits, the remaining gains came from techniques whose failure modes only surfaced at frontier scale (e.g., stability at extreme context, rare-behavior tails, hardware-co-design constraints). The step-by-step cascade was stagnation disguised as motion: enormous volumes of agent proposals piled up, but without enough frontier slots to falsify them, teams chased phantom improvements, while competitors who *did* fund frontier validation captured the few real gains and consolidated market power. The paper missed that “frontier experiment scarcity” can become binding abruptly when the research frontier moves from smooth scaling to tail-risk and systems-level phenomena that are intrinsically non-downscalable.