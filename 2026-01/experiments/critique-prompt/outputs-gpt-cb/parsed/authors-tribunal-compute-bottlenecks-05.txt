> If algorithmic efficiency hits diminishing returns on actual capabilities—unlike raw compute—then a 5 OOM effective-compute jump could land far short of superintelligence, breaking the paper's core speed argument.

**“Effective-Compute Gains Don’t Equal Capability-Speed Gains”** — The concluding probability claim depends on a definition of SIE as “≥5 OOM increase in effective training compute in <1 year without more hardware,” implicitly treating algorithmic efficiency as fungible with raw compute for speeding capability gains. This breaks if efficiency improvements face diminishing returns or shift you onto different parts of scaling curves where *capability per effective compute* is not constant (e.g., data quality limits, optimization plateaus, or new bottlenecks like context length or eval reliability). Then “5 OOM effective compute” could correspond to far less than “months-to-superintelligence,” undermining the paper’s central implication that compute bottlenecks likely don’t prevent rapid takeoff early on. If this holds, the paper must ground “effective compute” in an explicit capability metric (task performance, economic value, autonomy) and show the mapping stays steep over the relevant range.