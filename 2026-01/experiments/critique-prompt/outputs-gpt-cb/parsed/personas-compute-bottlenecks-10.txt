The paper claims compute bottlenecks may not slow an SIE until late stages, partly because there are “routes to improving AI that don’t use compute-intensive experiments” and because algorithmic efficiency can compound. If that’s true, it doesn’t merely speed up one lab—it collapses barriers to entry by making capability jumps less dependent on owning frontier hardware, enabling many mid-tier actors to ride the same software wave. That increases the number of simultaneous near-frontier programs, multiplies deployment of partially tested systems, and turns “SIE” from a single fast takeoff into a chaotic proliferation event with many independent failure points. Your analysis treats “more routes” as purely beneficial for escaping bottlenecks, but in a world of many actors it also means more leakage, less centralized control, and faster diffusion of dangerous capabilities. If this objection holds, the main consequence of your own thesis is not “compute bottlenecks won’t stop SIE,” but “software-driven acceleration makes containment and coordination dramatically harder,” worsening the very societal-risk scenario you use to motivate urgency.