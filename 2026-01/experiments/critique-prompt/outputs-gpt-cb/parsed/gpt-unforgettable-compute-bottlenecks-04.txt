"The Frontier-Validation Trap" — You claim the bottleneck is “number of experiments,” and argue near-frontier experiments might not be needed because we can extrapolate from smaller runs and because progress didn’t slow while near-frontier count fell. The attacked inference is “progress can remain fast without near-frontier compute,” but the mechanism of modern capability gains is that *generalization failures and scaling breaks* often only appear at frontier regimes, so validation itself becomes frontier-bound even if idea-generation is cheap. Step-by-step: small-run results are systematically overconfident under distribution shift; automated researchers propose many tweaks; each tweak needs a frontier-scale confirmation to avoid shipping a mirage; with fixed compute, confirmations become the gating item no matter how clever the researchers are. If this objection holds, your rebuttal needs a worked-through pathway showing how to reliably certify frontier performance from sub-frontier experiments (with quantified error bars), not just the assertion that extrapolation “might” work.