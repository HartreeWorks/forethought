**"Your Near-Frontier Argument Proves the Opposite"** — The paper argues that **near-frontier experiments can’t be the bottleneck**, because **the number of near-frontier runs has decreased over 10 years while progress continued**. But that same history is consistent with the opposite mechanism: progress continued *because* the frontier was repeatedly reset by the few near-frontier runs that did happen (bigger models, better data, better infra), and the ecosystem learned to ride that wave with many cheap follow-ups—meaning the scarce near-frontier runs were precisely the indispensable keystone. On this interpretation, a decreasing count of near-frontier runs is not evidence they don’t matter; it’s evidence the field has been running on an increasingly thin “frontier budget,” which would make holding compute constant dramatically more constraining than the paper suggests. In other words, the historical trend could be read as increasing complementarity: more and more of progress hinges on a tiny set of huge runs. If this holds, the paper would need to show that frontier progress can be driven by sub-frontier extrapolation *without* periodic frontier resets, rather than inferring irrelevance from continued progress.