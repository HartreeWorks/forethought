**Load-bearing claim:**
“*I’ll assume α=0.5 throughout*” (Economist version, multiple places)

**Attack type:**
Hidden parameter

**Break mechanism:** Your “max speed” ceilings (e.g., 6× at ρ=-0.4; 32× at ρ=-0.2) are extremely sensitive to α, because α effectively encodes how compute-intensive the marginal progress function is; but you treat it as a neutral simplification. A plausible counterworld is that near-frontier algorithmic progress is overwhelmingly compute-driven (α close to 1 in the “compute” term if K corresponds to experiments), in which case even slightly negative ρ yields a much tighter ceiling than your plotted intuition suggests, especially in the “early stages” you claim are safe. Conversely, if α is small early and large later, the “compute bottleneck bites late” claim can reverse. **If this holds, you’d need to justify α with a task decomposition of AI R&D (what fraction of marginal progress requires training/eval cycles vs pure thought/coding) and run sensitivity analyses over α(L,K) rather than fixing it.**
