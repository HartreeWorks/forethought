**"Your ‘labor’ is made of compute, so holding compute fixed makes labor fictional."** The paper treats cognitive labor \(L\) and compute \(K\) as separable inputs, then asks what happens as \(L \to \infty\) with \(K\) fixed. But in the setting at issue—automated AI R&D—every marginal unit of “cognitive labor” is literally additional inference/training cycles run on the same compute substrate; an extra AGI researcher is not a worker you can hire without buying the CPU that runs its brain. If you truly hold \(K\) fixed while adding “more AGIs,” you are implicitly dividing the same finite compute among more agents, so per-agent thinking speed collapses and \(L\) doesn’t actually rise in any meaningful sense. This is not a parameter-tweak; it removes the entire conceptual lever the argument pulls on (substitution) because the variables aren’t independent degrees of freedom. Any defense (“assume ultra-cheap inference” or “run them slower”) concedes the point: you’ve quietly smuggled in either more \(K\) or less \(L\), making the anti-bottleneck conclusion an artifact of a mis-specified input space.