The “strongest link” framing—there are many routes to superintelligence, we only need one route not bottlenecked by compute—is used to argue that compute-bottleneck objections must hold across *all* routes, which is presented as unlikely. The hidden lemma is that there exists at least one route where the decisive capability gains can be achieved without consuming near-frontier training compute, i.e., that “route diversity” implies “compute non-binding.” Counter-model: every route that actually yields deployable capability increases must ultimately be instantiated in updated weights/policies that generalize in the real world, and the only reliable way to get that instantiation is through training/evaluating at or near the frontier; you can generate many theories and scaffolds with \(L\), but capability jumps bottleneck on the same compute-limited training step. Under this model, “strongest link” doesn’t help because all links share the same final common path: frontier training throughput. If this holds, the author must separate (i) routes to *ideas* from (ii) routes to *realized capability*, and demonstrate at least one realized-capability route whose compute demand does not scale with the frontier.