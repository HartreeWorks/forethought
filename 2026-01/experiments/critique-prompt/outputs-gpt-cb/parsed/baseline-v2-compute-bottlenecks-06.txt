"Assuming Algorithmic Efficiency Automatically Multiplies Experiment Throughput" — The paper claims that when algorithms become twice as efficient, “you can run twice as many experiments (holding capability fixed),” and that this “pulls the rug out” from the compute-bottleneck objection because the key input is not fixed after all. But the bottleneck objection is about *near-frontier capability gains*, where the relevant experiments often need to be run at or near the new frontier to validate behavior, safety, generalization, or emergent properties. Many algorithmic improvements also require costly evaluation, ablations, and robustness checks whose compute may scale with model size, dataset size, or deployment constraints. Additionally, efficiency gains can be “spent” on training larger models rather than increasing the count of experiments, so experiment throughput is not a guaranteed dividend. The paper’s inference from “algorithmic efficiency improves” to “compute bottlenecks loosen” is therefore incomplete without a model of how labs allocate compute and what fraction must be spent on frontier training and evaluation.