The paper operationalizes an SIE as “≥5 OOM increase in effective training compute in <1 year without more hardware” and suggests compute bottlenecks probably won’t block that early. **Parameter sensitivity:** “effective training compute” is treated as if it compounds cleanly from algorithmic improvements, but many improvements don’t multiply—some trade off against each other (e.g., lowering training compute increases evaluation needs, or improves sample efficiency but requires more optimizer state/longer contexts), and some gains are one-time architectural step-changes that don’t sustain exponential feedback. If compounding is weak or strongly coupled to extra evaluation/training stages, then even with ρ near 0 the system may fail to reach 5 OOM in a year, making the stated probability range (10–40%) hinge on an implicit multiplicativity assumption. If this critique holds, the paper’s bottom-line forecast becomes materially overconfident even under its own “compute bottlenecks are mild early” premise, because the metric chosen silently presumes a kind of compounding the production model doesn’t establish.