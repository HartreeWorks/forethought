> The "implausibly low max speed" argument is circular because the listed gains (stack optimisation, early stopping) face their own compute, saturation, and verification bottlenecks that could genuinely cap speedups at modest levels.

**“The ‘Max Speed Is Implausibly Low’ Argument Is Unanchored”** — The paper attacks negative-ρ estimates by claiming they imply implausibly low “max speed” (e.g., <10×), relying on intuition about what abundant cognitive labor could do (optimize stack, generate ideas, stop runs early, etc.). The failure mechanism is that many of those gains either (i) are themselves compute-intensive to validate, (ii) saturate quickly, or (iii) run into verification/robustness constraints where speed is limited by reliable evaluation rather than ideation. Without empirical calibration—e.g., historical speedups from additional researchers at fixed compute, or measured returns to automation on experiment throughput—“implausible” just restates the desired conclusion. If this holds, the paper must either provide quantitative priors from historical lab productivity or weaken its central claim to “we don’t know,” rather than “most economic ρ imply absurd ceilings.”