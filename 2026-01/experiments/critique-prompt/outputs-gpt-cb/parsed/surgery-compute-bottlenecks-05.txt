> Extrapolation from small experiments can break down catastrophically at capability thresholds, so the last decade's progress may just reflect hardware scaling masking a hidden validation bottleneck.

The post’s rebuttal to “near-frontier experiments are fixed” depends on the claim that “you might not need near-frontier experiments; you can extrapolate from smaller ones,” and that the near-frontier count falling over the last decade didn’t slow progress. **Quantitative cliff:** extrapolation error can be mild within a regime but become discontinuous at capability thresholds (new emergent behaviors, new failure modes, qualitatively different scaling exponents), so the adequacy of small experiments can collapse once models start doing long-context planning, autonomous tool use, or adversarial adaptation. The last decade’s progress is not evidence against frontier-experiment necessity because the field also expanded frontier compute; scarcity of near-frontier experiments could have been masked by hardware scaling even if algorithmic *validation* was bottlenecked. If this critique holds, the paper loses a main empirical-style prop for dismissing the “frontier experiments are the binding constraint” version of the compute objection.