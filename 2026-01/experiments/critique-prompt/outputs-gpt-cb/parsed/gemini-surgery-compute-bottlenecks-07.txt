The “max speed is implausibly low” argument (e.g., “below 10× seems implausible,” “below 30× seems kinda implausible”) is doing decisive work to push \(\rho\) upward into \([-0.2,0)\). But it smuggles in a hidden assumption that AI R&D is highly parallelizable in wall-clock time once you have enough cognitive labour, rather than being dominated by a critical path of sequential dependencies (design → train → analyze → redesign) whose steps each consume fixed minimum time on fixed compute. Counter-example: imagine a research regime where each iteration requires (1) one training run at scale, (2) one full eval battery, (3) post-hoc interpretability and red-teaming, each step taking a minimum of N days on fixed compute; even with infinite AGIs, you cannot compress those compute-time steps below their physical runtime, so speedup saturates at ~2–5× via better scheduling and early stopping, not 30×. This preserves the paper’s premise that lots of cognition helps, while snapping the inference that “max speed must be high, therefore \(\rho\) near 0.” If this holds, the author needs to replace intuition about “what abundant cognitive labour could do” with a critical-path model that upper-bounds wall-clock acceleration given fixed compute throughput and sequential experiment dependencies.