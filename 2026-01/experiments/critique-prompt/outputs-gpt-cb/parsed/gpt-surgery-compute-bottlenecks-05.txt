> Extrapolation from small experiments breaks down at exactly the critical thresholds where emergent capabilities appear, so compute bottlenecks resurface precisely when the stakes are highest.

**Load-bearing claim:**
“*you might not need near-frontier experiments… you might be able to extrapolate from experiments that use increasingly small fractions of the lab’s compute*” (Counterargument 5.2)

**Attack type:**
Countermodel (quantitative cliff)

**Break mechanism:** There are plausible phase-transition regimes where small-scale experiments are systematically misleading about large-scale behavior (emergent capabilities, optimization instabilities, data-curation interactions, tool-use failures), so extrapolation fails exactly where the high-stakes algorithmic decisions lie. In such a world, the *marginal* value of cognition without near-frontier validation collapses, creating a hard ceiling even if lots of useful work happens at small scale. Then “compute bottlenecks don’t bite early” becomes false once you reach the regime where qualitative behaviors appear only at scale. **If this holds, you’d need to argue (with concrete research workflows) why the key algorithmic improvements during an SIE are predictable from sub-frontier tests, or else incorporate a “scale-dependent validity” term that reinstates a compute bottleneck.**
