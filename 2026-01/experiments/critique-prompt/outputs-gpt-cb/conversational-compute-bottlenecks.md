1. **"The CES framing may be the wrong abstraction, but that doesn’t imply weak bottlenecks"**  
The paper spends substantial effort arguing that importing CES-style elasticities from manufacturing is dubious, but undermining that particular estimate doesn’t establish that compute bottlenecks are mild. AI R&D could be bottlenecked in ways that a CES model fails to capture (e.g., discrete “must-run” frontier trainings, long feedback cycles, or reliability constraints), which could be *more* binding than CES complementarity suggests. In other words, “CES extrapolation is heroic” is compatible with “compute limits still dominate,” because the true production function could be even harsher. The critique is that the paper often treats “economist argument is fragile” as evidence for “SIE likely survives compute limits,” when it may only justify agnosticism. A stronger case would require a positive, empirically grounded alternative model of how software progress scales with fixed compute.

2. **"Near-frontier experiments might be indispensable, not optional"**  
A key rebuttal is that progress might not require near-frontier experiments because you can extrapolate from smaller runs or optimize experimentation. But many algorithmic ideas only reveal their value (or failure modes) at scale: optimization stability, emergent capabilities, long-horizon credit assignment, tool-use reliability, and data/architecture interactions can be qualitatively different at frontier sizes. If that’s right, then the number of informative experiments is tightly coupled to the ability to run large trainings, which is compute-limited by design. The paper’s claim that “near-frontier experiments have declined yet progress continued” may conflate algorithmic progress with scale-driven gains and better engineering around a small number of huge runs. Even if you can do lots of small experiments, the binding constraint could remain “how many full-scale training shots can you afford.”

3. **"The ‘do the compute in your head’ substitution argument violates physical accounting"**  
The paper notes that, in principle, sufficiently capable cognitive labor could simulate experiments “in their heads,” implying compute and cognition are ultimately substitutable. But unless those “heads” are themselves running on physical compute, the claim is a category error: simulation requires operations, memory bandwidth, and energy somewhere. If the AGIs are implemented on the same hardware whose compute is “fixed,” then “thinking harder” cannot exceed the physical FLOP budget; it merely reallocates it. If the argument instead imagines non-digital minds or radically different substrates, then the scenario has quietly changed the hardware premise. So this point weakens the bottleneck objection only by relaxing the constraint (“fixed compute”) in a way that critics needn’t grant.

4. **"Software progress can increase compute demand, not reduce it"**  
The paper emphasizes algorithmic efficiency improvements that allow more experiments at fixed compute. A strong counterpoint is that many software advances *increase* appetite for compute: longer training to squeeze out capability, more extensive RL, larger context windows, heavier test-time compute, larger synthetic-data pipelines, and more elaborate agent scaffolds. In practice, labs often reinvest efficiency gains into pushing the frontier rather than banking them as spare experimental throughput. If the competitive equilibrium is “use all available compute,” then fixed hardware still caps progress because each “step” in capability keeps expanding to fill the budget. This undermines the idea that compute efficiency automatically loosens the compute bottleneck during the key phase where you’re racing up the capability curve.

5. **"Parallel researcher copies face steep coordination and verification overheads"**  
A software intelligence explosion story leans heavily on scaling cognitive labor via many fast copies. But research does not parallelize linearly: you quickly hit coordination costs, duplicated work, integration complexity, and a growing burden of verifying results under distribution shift and subtle bugs. As systems become more capable and more complex, the cost of evaluation and the risk of regressions increase, which can force longer cycles and more conservative release practices. This can create an endogenous bottleneck that looks like “compute” only partly; it’s also about experiment design, oversight, and systems engineering. If marginal cognitive labor yields diminishing returns due to these overheads, then the effective elasticity between “research labor” and “compute” may be low even if experiments themselves were cheap.

6. **"Evaluation, interpretability, and safety constraints can be the real rate-limiters"**  
The argument largely treats “pace of software progress” as gated by idea generation and empirical testing. But capability progress in deployed or usable systems may be gated by evaluation (detecting failures), interpretability, robustness work, red-teaming, and alignment mitigations—activities that can require massive compute themselves (e.g., adversarial training, extensive RLHF/RLAIF, automated auditing, large-scale eval suites). Even if you can propose many improvements quickly, you might not be willing (or legally able) to ship or rely on them until they pass expensive and time-consuming validation. This creates a bottleneck that does not vanish with more cognitive labor; it often becomes *harder* as capabilities rise and stakes increase. If these constraints bind, an “SIE in the lab” may fail to translate into an “SIE in the world.”

7. **"Data and environment interaction can bottleneck empirical progress independently of compute"**  
Compute is not the only empirical input: high-quality data, high-signal feedback, and rich environments for learning (including tool APIs, simulators, or real-world interaction) can become limiting. As models saturate existing text/code corpora, progress may require new data generation that is bottlenecked by human processes, real-world time, access constraints, or complex simulations that are themselves compute-heavy. If frontier gains increasingly come from embodied interaction, long-horizon tasks, or domain-specific proprietary data, then “more AI researchers” won’t substitute smoothly. This challenges the paper’s focus on compute substitutability as the decisive issue, because the real complementarity might be between cognition and scarce data/feedback channels. Under that view, compute bottlenecks might arrive “late,” but other bottlenecks could arrive early and still block explosion dynamics.

8. **"The ‘implausibly low max speed’ claim rests on intuition, not measurement"**  
The paper argues that economic elasticities imply max speeds (e.g., 2–10×) that feel implausibly low given what abundant cognitive labor could do. But this is largely an intuition pump, not an empirical estimate, and it risks double-counting: many proposed speedups (better ideas, better experiments, early stopping) still require running enough large experiments to validate and integrate them. Moreover, “max speed” depends on how you define “software progress” (loss, benchmarks, capability thresholds, or economically relevant performance), and different definitions can have very different ceilings. A skeptic can accept that “a lot of smart labor helps” while still claiming that without more compute you cannot cross certain capability thresholds quickly. Without operationalizing “progress” and grounding the speedup distribution in data, the “low max speed is implausible” objection is not decisive.

9. **"Historical evidence can be read the opposite way: frontier compute has been the main driver"**  
The paper notes that near-frontier experiment counts may have fallen while progress continued, suggesting near-frontier scarcity isn’t binding. But a skeptic can argue the opposite: progress continued *because* frontier compute per run increased dramatically, and algorithmic improvements were often complementary to scale rather than substitutes for it. Many headline capability jumps align with scaling up training/inference, better data pipelines, and longer/stronger post-training—each compute-intensive. If scaling has historically been the dominant lever, then holding hardware fixed is a much stronger constraint than the paper implies. The inference “progress happened despite fewer near-frontier runs” can be misleading if a small number of gigantic runs plus lots of medium-scale ablations is exactly the compute-limited regime we should expect.

10. **"An SIE requires not just faster discovery but faster deployment of better researchers"**  
Even if AI can accelerate algorithm discovery, the feedback loop requires those discoveries to rapidly produce *better AI researchers* (i.e., materially improved models/agents) that then further accelerate R&D. That step often involves expensive training, extensive post-training, infrastructure changes, and integration into tooling—each potentially compute- and time-heavy. If retraining and validating the next “generation” is slow under fixed compute, then the loop’s gain may be <1 over relevant time windows, preventing explosion dynamics. Put differently: accelerating “ideas per day” is not enough if “new effective researcher per idea” is bottlenecked by long, compute-heavy training cycles. This is a central structural objection because it targets the feedback loop itself, not merely the pace of incremental research within a fixed model generation.