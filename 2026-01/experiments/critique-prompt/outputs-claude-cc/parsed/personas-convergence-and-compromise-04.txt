[The Institutional Corruptionist] The paper assumes in section 2.3.1 that people could "design superintelligent AI systems to be smarter, more principled, more open-minded, and less prone to human biases than the wisest moral thinkers today" and use them for ethical reflection. This ignores that whoever controls the training process for these AI advisors will face overwhelming incentives to bias them toward conclusions favorable to the controller's existing preferences. The paper's own logic about preference stability suggests that people will select AI advisors who "broadly agree with their worldview" rather than challenge it. The result is not convergence toward moral truth but regulatory capture of the reflection process itselfâ€”each powerful faction will have "superintelligent advisors" that happen to vindicate their prior commitments, creating an arms race of sophisticated rationalization rather than genuine moral progress.