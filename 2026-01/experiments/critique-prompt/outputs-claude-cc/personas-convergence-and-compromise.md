1. [The Empirical Hardliner] The paper claims that "asymmetric growth might systematically favour altruistic values" because altruists save more and have more children, leading to their values dominating over time. This lacks any identified causal mechanism explaining *how* altruistic preferences transmit across generations or why they wouldn't be diluted by cultural mixing, assortative mating with non-altruists, or simple preference drift during child-rearing. The authors provide no falsifiable prediction about the timescale over which this selection effect would operate, no empirical evidence that altruists actually do save more or reproduce more in current populations, and no model of the selection coefficient required to overcome countervailing pressures. Without these, the claim is unfalsifiable speculation dressed as evolutionary reasoning, and the paper cannot legitimately use it as grounds for even modest optimism about future value convergence.

2. [The Game-Theoretic Defector] The paper's section 3.1 on moral trade assumes that parties will actually execute trades that make both better off by their lights, but ignores that any party holding non-discounting linear preferences has overwhelming incentive to defect on long-term cooperative arrangements. If Alice commits resources now for Bob's promise of future returns, and enforcement mechanisms are imperfect over cosmic timescales, Bob's dominant strategy is to accept Alice's resources and renege once enforcement becomes costly or impossible. The paper acknowledges superintelligence could enable "iron-clad contracts" but provides no mechanism for how contracts spanning billions of years across galactic distances would be enforced when the enforcing party itself might have drifted in values or ceased to exist. The entire edifice of optimism from moral trade collapses once you recognize that patient actors with linear utility functions face repeated prisoner's dilemmas with astronomical stakes and no credible commitment technology.

3. [The Mechanism Designer] The paper repeatedly invokes "collective decision-making processes" and "bargaining" as potential paths to mostly-great futures without ever specifying what these mechanisms actually look like. Section 3.5 casually mentions that "some such processes might well be predictably much better than others" but provides zero formal specification of any mechanism, no analysis of incentive compatibility, no discussion of what information is required for these mechanisms to function, and no proof that any mechanism satisfying reasonable participation constraints could even theoretically aggregate heterogeneous preferences over cosmic-scale resource allocation. Until the authors can write down a mechanism in mathematical form that provably achieves their claimed outcomes under stated assumptions, the phrase "trade and compromise" is doing load-bearing work that cannot be evaluated. This isn't a research contribution; it's a promissory note.

4. [The Institutional Corruptionist] The paper assumes in section 2.3.1 that people could "design superintelligent AI systems to be smarter, more principled, more open-minded, and less prone to human biases than the wisest moral thinkers today" and use them for ethical reflection. This ignores that whoever controls the training process for these AI advisors will face overwhelming incentives to bias them toward conclusions favorable to the controller's existing preferences. The paper's own logic about preference stability suggests that people will select AI advisors who "broadly agree with their worldview" rather than challenge it. The result is not convergence toward moral truth but regulatory capture of the reflection process itself—each powerful faction will have "superintelligent advisors" that happen to vindicate their prior commitments, creating an arms race of sophisticated rationalization rather than genuine moral progress.

5. [The Second-Order Catastrophist] Suppose the paper's recommendations succeed and society implements robust "collective decision-making processes" that prevent value-destroying threats and enable widespread moral trade. The paper fails to consider that such processes, by creating stable equilibria around current preference distributions, would lock in whatever moral errors happen to be prevalent at the time of implementation. If the decision-making apparatus is sufficiently robust to prevent threats and enable trade, it is also sufficiently robust to prevent later moral reformers from disrupting a stable but suboptimal equilibrium. The paper's own framework suggests that moral views continue to evolve with reflection; a system optimized for "trade and compromise" among current views actively prevents the disruption necessary for moral progress beyond current horizons. Success at the paper's proximate goals may guarantee failure at its ultimate goal.

6. [The Historical Parallelist] The paper uses the Wright brothers' achievement of powered flight as an analogy for how humanity might "hone in" on a narrow target despite astronomical odds. But the history of aviation actually undermines this analogy: the Wright brothers succeeded because they could iterate rapidly with cheap prototypes, receive immediate feedback about whether their designs flew, and learn from observable failure modes. Moral convergence toward a "mostly-great future" has none of these properties—there is no cheap way to test cosmic-scale value systems, no immediate feedback about whether one is approaching moral truth, and catastrophic moral errors may be indistinguishable from success for centuries. A better historical parallel is the history of religious and ideological movements, where confident claims of moral truth proliferated without convergence, and apparent consensus repeatedly fractured under pressure. The aviation metaphor systematically misleads about the epistemics of the actual problem.

7. [The Complexity Theorist] The paper's analysis of moral trade in section 3.2 treats different moral views as static positions that can be straightforwardly combined or traded between. This ignores that moral views embedded in social systems exhibit emergent properties that cannot be predicted from the views in isolation. When classical utilitarian communities interact with deontological communities through trade, the resulting hybrid institutions will develop their own internal dynamics, selection pressures, and drift patterns that neither founding view anticipated or endorsed. The paper's neat partition where "the first group turns the Milky Way into a common-sense utopia, and the second group occupies all the other accessible galaxies" assumes these systems can remain isolated, but any interaction at the boundary creates a complex adaptive system whose long-run behavior is fundamentally unpredictable from initial conditions. The clean separation required for the optimistic scenario is dynamically unstable.

8. [The Political Economist] The paper treats "concentration of power" as merely one "blocker" among several in section 3.5, but fails to analyze the political economy of who benefits from the status quo and who would benefit from the proposed interventions. The current distribution of power strongly favors those who would lose from genuine moral progress—those benefiting from factory farming, those whose wealth depends on preventing redistribution, those whose status depends on maintaining current hierarchies. These actors have concentrated interests in blocking convergence toward views that would disadvantage them, while the beneficiaries of moral progress (future persons, animals, digital minds) cannot organize or lobby. The paper's recommendations to improve "collective decision-making processes" will be captured by exactly those actors who benefit from current moral failures, ensuring that any implemented version serves incumbent interests while wearing the legitimizing language of moral progress.

9. [The Cognitive Scientist] The paper's section 2.3.1 assumes that access to superintelligent reflection would help people converge on accurate moral views, but this fundamentally misunderstands how human moral cognition works. Moral judgments are not conclusions derived from reflection but post-hoc rationalizations of intuitions generated by evolutionarily ancient neural systems. Providing people with better arguments doesn't change their intuitions; it provides them with more sophisticated rationalizations for whatever they already believed. The paper's own observation that people "strongly dislike the suggestions of their more-reflective selves" points toward this, but fails to draw the obvious conclusion: superintelligent moral advisors will be used to generate more compelling justifications for existing preferences, not to change those preferences. The entire convergence-through-reflection framework assumes a model of moral cognition that decades of empirical research has falsified.

10. [The Resource Economist] The paper's section 3.1 claims that transaction costs would be "extremely small relative to the gains" from moral trade in a technologically advanced society, but this ignores the fundamental problem of preference elicitation at cosmic scales. To execute trades between moral views, parties must accurately specify their utility functions over astronomically large outcome spaces, verify that counterparties have accurately specified theirs, and compute whether proposed trades actually satisfy both. The computational and communication costs of specifying preferences over 10^80 atoms worth of possible configurations, even with superintelligent assistance, scale with the complexity of the preference space, not with available compute. The paper assumes preferences can be cheaply communicated and verified, but for the kind of fine-grained moral views the paper itself argues are necessary for a "mostly-great future," the costs of specification alone may consume most available resources, leaving nothing to actually trade.