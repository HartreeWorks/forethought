1. **What is the probability that a “mostly-great” future occurs by default (conditional on survival and “no serious, coordinated efforts … to promote the overall best outcomes de dicto”)?**  
   - **Paper anchor:** “consider a reasonable probability distribution … conditional on *Survival*, but also … *no more serious optimisation pressure than today* … To the extent that … mostly-great futures [are] very unlikely … (say, <1%), then that moral view is fussy” (Sec. 3.1).  
   - **The flip:** If \(P(\text{mostly-great} \mid \text{survival, no de dicto optimisation}) \gtrsim 10\%\), then **prioritise survival/extinction-risk reduction**; if \( \lesssim 1\%\), then **prioritise deliberate “better futures” steering (moral/gov/institutional optimisation toward near-best outcomes)**.  
   - **What would answer it:** Explicit scenario distributions from structured expert elicitation + forecasting tournaments that condition on “survival” and hold “optimisation pressure” fixed, plus calibration against historical rates of “moral catastrophe” reversal vs persistence (Sec. 2.1) under rising abundance.

2. **What is the effective number of near-independent “value-critical” dimensions \(N\) (and their dependence structure) such that overall future value behaves roughly like a product of factors?**  
   - **Paper anchor:** “we can capture … eutopian fragility by seeing the value of the future as the *product of many factors* … mostly-great futures are rare” and the toy model where increasing \(N\) makes expected value collapse (Sec. 2.4).  
   - **The flip:** If effective \(N \le 2\) or factors are strongly correlated (so one kind of progress reliably fixes many dimensions), then **broad capability/prosperity growth and generic good governance** likely suffice; if effective \(N \ge 5\) with weak correlations, then **targeted work to avoid many distinct moral failure modes (multi-domain governance, moral circle/rights design, value reflection, etc.)** dominates.  
   - **What would answer it:** A decomposition study that maps candidate “moral catastrophe” axes in Sec. 2.3 into a minimal factor model (e.g., Bayesian factor analysis over a large set of moral views + future-world descriptions) and estimates independence/correlation between axes.

3. **How fat-tailed is the distribution of “value per unit resource” over feasible uses of cosmic-scale resources (i.e., does a tiny fraction of configurations capture most value)?**  
   - **Paper anchor:** “the distribution of value/cost … is probably sufficiently *fat-tailed* … [so] a mostly-great future is a narrow target” (Sec. 3.2), and “unless *most* available resources are configured for the *almost exactly most valuable* kind(s) of thing … most achievable value is almost certainly lost” (Sec. 3.2).  
   - **The flip:** If the top \(\le 0.1\%\) of resource uses plausibly deliver \(\ge 50\%\) of attainable value (extreme fat tail), then **precision alignment/targeting the “right” kind of value-creation** is pivotal; if instead value-efficiency is comparatively smooth (e.g., top 10% uses deliver <2× median value), then **scale/building/expansion and general welfare improvements** are robustly good.  
   - **What would answer it:** Cross-domain modeling of maximum experiential intensity and other intrinsic goods under advanced design (Sec. 3.2’s experience arguments), combined with compute-based search/upper bounds on achievable welfare per joule/bit and empirical priors from existing fat-tail measurements (interventions, consumer surplus, etc.) translated into future resource-use space.

4. **How soon, and how irreversibly, will early space settlement “capture essentially all resources that will ever be available to us,” creating lock-in of initial allocation norms?**  
   - **Paper anchor:** “The initial periods of settlement and resource appropriation … will involve capturing essentially all resources that will ever be available to us” (Sec. 2.3.4) and “If AI drives explosive industrial expansion, this … could … be within years or decades” (Sec. 2.3.4).  
   - **The flip:** If decisive resource capture is likely within \(\lesssim 50\) years and early allocations are hard to revise, then **prioritise near-term space/AI governance and constitutional “resource rights” design before expansion**; if capture is \(\gtrsim 200\) years away or allocations are readily renegotiable, then **prioritise slower moral reflection, institution building, and broad cooperation capacity** over urgent space-race governance.  
   - **What would answer it:** Integrated assessment combining (i) engineering/physics constraints on replication/industrial growth off-Earth, (ii) AI automation timelines for self-replicating industry, and (iii) political economy of path dependence (how often large property regimes get renegotiated once established).

5. **What is the probability that digital beings both (i) have moral status and (ii) become numerically/politically dominant (e.g., via “faster-growing … majority of voting power”), making “AI rights” decisions a primary determinant of future value?**  
   - **Paper anchor:** “there may be vast numbers of digital beings … dramatically outnumber biological beings” and “they soon become the large majority of voting power” (Sec. 2.3.2).  
   - **The flip:** If \(P(\text{digital moral patients} \land \text{dominant}) \gtrsim 50\%\), then **prioritise digital welfare/rights frameworks and preventing exploitation/lockout (including governance that handles voting/power transitions)**; if \(\lesssim 10\%\), then **prioritise other value dimensions (human welfare, space allocation, population ethics, etc.)** over digital-rights-centric strategy.  
   - **What would answer it:** A combined program of (a) empirical consciousness research tied to likely AI architectures, (b) scaling-law projections for economically valuable digital labor/agent replication, and (c) institutional analysis of enfranchisement/power transfer mechanisms under scenarios where digital agents become most of the workforce/population.

6. **Is the morally relevant “disvalue of bads” effectively unbounded or so asymmetric that even a \(\sim 10^{-22}\) fraction of resources used toward bads prevents a mostly-great future (making suffering-minimisation lexically dominant)?**  
   - **Paper anchor:** On separate-aggregation bounded views: “we don’t reach a mostly-great future if as little as *one resource in \(10^{22}\)* is used toward bads rather than goods” (Sec. 3.1.1 / 3.3), and “if … disvalue … is much greater … then an even smaller quantity of bads would be sufficient to outweigh any utopian future” (Sec. 3.3).  
   - **The flip:** If bads are unbounded below or asymmetry is so extreme that \(\text{(tolerable bad fraction)} \ll 10^{-22}\), then **prioritise preventing any large-scale suffering/“bads” channels (s-risk style strategy), even at the cost of slowing expansion/optimisation**; if bads are bounded/symmetric enough that \(\text{(tolerable bad fraction)} \gg 10^{-22}\), then **prioritise scaling goods and broad flourishing (including expansion) with reasonable safeguards**.  
   - **What would answer it:** A targeted moral-uncertainty analysis that (i) elicits where reflective-equilibrium judgments place upper/lower bounds and asymmetry, and (ii) stress-tests those judgments against the paper’s “scale” thought experiments (galaxy-scale mixtures of goods/bads) to infer the implied asymmetry/“tolerable bad fraction.”

7. **Which intertheoretic comparison/normalisation rule is correct enough to act on—given the paper’s demonstration that it can reverse whether “upside-focused” risk-taking beats “safety-focused” downside avoidance?**  
   - **Paper anchor:** “the approach to uncertainty critically matters for which option is best” and different normalisations make “upside-focused” vs “safety-focused” swap (Sec. 3.5; the comparison table/figure).  
   - **The flip:** If the correct intertheoretic comparisons give substantial effective weight to unbounded/high-upside theories (e.g., via variance normalisation or the paper’s pairwise-stake reasoning), then **prioritise upside-seeking strategies that increase chances of eutopia even with some dystopia risk**; if instead comparisons effectively normalise stakes so downside dominates (e.g., extinction–best normalisation), then **prioritise safety-focused strategies that minimise catastrophe probabilities even if they cap upside**.  
   - **What would answer it:** A formal decision-theory audit that tests candidate comparison rules against desiderata the paper highlights (e.g., avoiding dependence on “contingent fact about how big the affectable universe is,” Sec. 3.5) plus pre-registered surveys of stable moral judgments across cosmology/scale updates to see which normalisation best matches people’s cross-scale invariances.