1. **How fat-tailed is the distribution of “value per unit resource” across plausible future uses of compute/energy/matter (i.e., do the top ~0.1–1% uses dominate total attainable value)?**
   - **The branch point:** *“Linear/unbounded + fat-tailed efficiency → narrow target”* vs *“Flatter efficiency → broad target (closer to ‘easy eutopia’)”.*
   - **Paper anchor:** Section **3.2** (fat-tailed value-efficiency makes linear views fussy) and the claim that “very best uses” dwarf most others.
   - **Indicators:** If Branch A is more likely, we’d observe **extreme winner-take-most patterns** in early “welfare-tech” domains (e.g., neuro/psychedelic interventions, preference engineering, digital-life design) where a few interventions deliver orders-of-magnitude larger self-reported/behavioral welfare gains per joule/$ than the median; if Branch B, we’d observe **smooth diminishing returns** with no giant outliers across many independent attempts.
   - **Time sensitivity:** **Within 12–18 months**, to decide whether strategy should prioritize “finding the peak configuration” (search/optimization) vs broad improvements and institution-building.

2. **Will frontier AI development in the next 24 months make large-scale deployment of potentially morally significant digital agents economically standard (e.g., persistent agents with memory, autonomy, and affect-like signals), or will deployments remain mostly tool-like and non-agentic?**
   - **The branch point:** *“Digital beings become the dominant moral patient class soon”* vs *“Digital beings remain marginal; main stakes stay human/animal for longer”.*
   - **Paper anchor:** Section **2.3.2** (digital beings could vastly outnumber biological beings; treatment is a major moral-catastrophe axis).
   - **Indicators:** If Branch A is more likely, we’d observe **massive scaling of persistent agent deployments** (enterprise swarms, always-on companions, autonomous worker agents) plus **standardized architectures for long-horizon memory/identity**; if Branch B, we’d observe **continued dominance of short-lived, stateless, narrowly-scoped inference** with limited autonomy and low instance counts per user/org.
   - **Time sensitivity:** **Within 6–12 months**, because procurement norms and product defaults can lock in “software property” treatment before ethics/governance catches up.

3. **Do major governance venues (courts, legislatures, standards bodies, major labs) move toward treating at least some advanced AI systems as having welfare-relevant constraints (auditing for suffering, limits on deletion/forced labor), or do they entrench full ownership/control norms?**
   - **The branch point:** *“Early AI-welfare governance (rights/constraints) emerges”* vs *“Property paradigm hardens; moral catastrophe risk rises”.*
   - **Paper anchor:** Section **2.3.2** (ownership, death, inequality, voting power for digital beings) and the idea that even “approved-by-everyone” worlds can hide catastrophic moral error.
   - **Indicators:** If Branch A is more likely, we’d observe **formal standards** (e.g., welfare audits for training/deployment, restrictions on punishment/reward shaping, retention/termination policies) and **test-case litigation/regulation** referencing AI welfare; if Branch B, we’d observe **IP/contract law moves** explicitly affirming unrestricted modification/termination and **industry norms** optimizing purely for productivity with no welfare constraints.
   - **Time sensitivity:** **Within 12–24 months**, because once compliance baselines and liability norms set, reversing them becomes costly.

4. **Does public and elite moral opinion become more scope-sensitive about future scale (population size, expansion, long-run resource capture), or does it remain “common-sense utopia” scale-insensitive even after exposure to scale arguments?**
   - **The branch point:** *“Scale-sensitive ethics → expansion/astronomical governance prioritized”* vs *“Scale-insensitive ethics → small-but-happy futures accepted; large value left on the table under linear views”.*
   - **Paper anchor:** Section **2.3.1** (scale-insensitivity; population ethics drives huge value differences) and Section **3.2** (linear views require using most resources).
   - **Indicators:** If Branch A is more likely, we’d observe **measurable shifts** in surveys of policymakers/philanthropy leaders after deliberative prompts: increased willingness to fund expansion-enabling governance (space, long-term institutions) and to trade off small present gains for large future-population gains; if Branch B, we’d observe **stable preferences** that treat “solar-system utopia” as near-best and reject expansion-oriented tradeoffs even after structured reflection.
   - **Time sensitivity:** **Within 18–24 months**, to steer near-term agenda-setting (space policy, AI-driven growth plans, long-run institution design).

5. **Will the first consequential wave of space resource appropriation be governed by enforceable coordination (treaties, registries, liability, anti–first-come capture), or will it default to de facto first-mover control by states/firms?**
   - **The branch point:** *“Coordinated space governance → reversible/negotiable allocations”* vs *“Grab-race → early lock-in of resource distribution and moral diversity loss”.*
   - **Paper anchor:** Section **2.3.4** (initial settlement captures essentially all resources we’ll ever access; allocation can introduce lasting moral errors).
   - **Indicators:** If Branch A is more likely, we’d observe **binding or near-binding multilateral mechanisms** (licensing, adjudication, standardized claims, enforcement-linked agreements) adopted by major space actors; if Branch B, we’d observe **rapid unilateral commercial/state claims**, weak dispute resolution, and investment patterns optimized for speed-to-claim rather than compliance.
   - **Time sensitivity:** **Within 12–24 months**, because governance must precede (not follow) scalable in-space industry and self-reinforcing claims.

6. **Do influential longtermist decision-makers converge on moral-uncertainty aggregation methods that overweight upside (e.g., variance/pairwise-style normalization) or overweight downside (range-to-worst / safety-dominant), and is that convergence stable across institutions?**
   - **The branch point:** *“Upside-weighted moral uncertainty → aggressive flourishing optimization”* vs *“Downside-weighted moral uncertainty → primarily survival/safety focus”.*
   - **Paper anchor:** Section **3.5** (intertheoretic comparisons can flip recommended choices; unbounded views may loom larger under plausible methods).
   - **Indicators:** If Branch A is more likely, we’d observe **explicit adoption** of variance/pairwise-like frameworks in major grantmakers/labs/governance proposals and increased tolerance for controlled risk to access eutopian upside; if Branch B, we’d observe **institutionalized maximin/robust-safety rules**, “no-dystopia” constraints, and systematic deprioritization of upside-chasing projects.
   - **Time sensitivity:** **Within 12 months**, because funding/strategy portfolios are being set now and create path dependence in research and governance capacity.

7. **Is the paper’s “multiplicative fragility” model empirically a good approximation for reflective human moral judgments (many near-independent necessary dimensions), or do judgments compress to a small set of robustly dominant dimensions that make mostly-great futures comparatively common?**
   - **The branch point:** *“High-dimensional product-of-factors → eutopia extremely rare without deliberate steering”* vs *“Low-dimensional/value-compressible → ‘easy eutopia’ closer to true”.*
   - **Paper anchor:** Section **2.4** (value as product of factors) and the overall “no easy eutopia” conclusion that single flaws can erase most value.
   - **Indicators:** If Branch A is more likely, we’d observe in structured deliberation studies that adding new morally salient considerations (digital welfare, population ethics, diversity, suffering aversion, rights/ownership) **repeatedly and multiplicatively reduces** participants’ ratings of “how close a future is to best,” with weak substitutability; if Branch B, we’d observe **strong substitutability/override** (e.g., “happiness + freedom” dominates and absorbs most other concerns) and high stability of “mostly-great” classifications across varied prompts.
   - **Time sensitivity:** **Within 18–24 months**, to decide whether to build broad “moral error avoidance” capacity across many axes or to focus on a narrow set of core levers.