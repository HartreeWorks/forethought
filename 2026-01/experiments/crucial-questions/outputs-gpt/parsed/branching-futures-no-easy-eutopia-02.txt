**Will frontier AI development in the next 24 months make large-scale deployment of potentially morally significant digital agents economically standard (e.g., persistent agents with memory, autonomy, and affect-like signals), or will deployments remain mostly tool-like and non-agentic?**
   - **The branch point:** *“Digital beings become the dominant moral patient class soon”* vs *“Digital beings remain marginal; main stakes stay human/animal for longer”.*
   - **Paper anchor:** Section **2.3.2** (digital beings could vastly outnumber biological beings; treatment is a major moral-catastrophe axis).
   - **Indicators:** If Branch A is more likely, we’d observe **massive scaling of persistent agent deployments** (enterprise swarms, always-on companions, autonomous worker agents) plus **standardized architectures for long-horizon memory/identity**; if Branch B, we’d observe **continued dominance of short-lived, stateless, narrowly-scoped inference** with limited autonomy and low instance counts per user/org.
   - **Time sensitivity:** **Within 6–12 months**, because procurement norms and product defaults can lock in “software property” treatment before ethics/governance catches up.