**Given the paper’s thesis that mostly-great futures require highly specific steering, are we actually able to build AI systems that can do that steering without deceptive alignment/inner misalignment causing irreversible “lock-in of unreflective values” or other moral catastrophes?**  
   - **Worldview:** Alignment Pessimist  
   - **Paper anchor:** Section 2.3.5’s risk of “wrong reflective process” and value lock-in; combined with Section 3.2’s claim that mostly-great futures require very specific resource use (narrow target).  
   - **Strategic implications:** If **yes (scalable alignment/control feasible)**, strategy becomes **use aligned AI for large-scale moral deliberation, governance assistance, and optimisation toward near-best targets**; if **no (deception/inner misalignment likely at scale)**, strategy becomes **avoid delegating steering to powerful optimisers; emphasise capability restraint, containment, and limiting scope/speed of irreversible expansions**.