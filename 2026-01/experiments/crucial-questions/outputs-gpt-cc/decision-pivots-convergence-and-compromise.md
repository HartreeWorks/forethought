1. **What is the probability that, under “reasonably good conditions,” post‑AGI society achieves widespread, accurate, *motivational* convergence on promoting the good *de dicto* (WAM‑convergence)?**  
   - **Paper anchor:** “We’ll here assume that, given WAM-convergence, we will reach a mostly-great future… [but] we give our main argument against expecting WAM-convergence even under reasonably good conditions.” (Sections 2, 2.4)  
   - **The flip:** If \(P(\text{WAM} \mid \text{good conditions})\) > ~0.5, then **prioritise ensuring ‘good conditions’ (survival + avoid lock‑in/blockers) and letting reflection do the rest**; if < ~0.5, then **prioritise pluralistic governance/compromise mechanisms (trade, institutions, threat prevention) because convergence won’t rescue us**.  
   - **What would answer it:** Large-scale empirical “reflection divergence” studies using heterogeneous populations + AI-assisted deliberation (varying starting points and idealisation rules), plus formal models predicting convergence rates under different meta-ethical assumptions.

2. **At the key bargaining/lock‑in moment, what fraction of resources/power will be controlled by agents who partially AM‑converge to the correct moral view and are willing to spend “most of the resources they control” on it?**  
   - **Paper anchor:** “some meaningful fraction… would converge on the correct moral view and would be motivated to use most of the resources they control towards promoting the good… not less than (say) one in a million people.” (Section 3)  
   - **The flip:** If the “correct-view” coalition’s initial resource share \(s\) > ~1–5%, then **strategy: build institutions for moral trade/compromise to amplify that share into near-best outcomes**; if \(s\) < ~0.1–1%, then **strategy: prioritise preventing power concentration + targeted influence over pivotal power-holders (or technical control leverage), because trade from a tiny base won’t reach ‘mostly-great’**.  
   - **What would answer it:** Forecasting models of post‑AGI power distribution (compute/capital/military/AI ownership) combined with empirical estimates of “scope-sensitive altruistic” prevalence after strong AI-mediated reflection experiments.

3. **How close to “frictionless” will moral trade actually be—i.e., can superintelligence deliver enforceable, low-transaction-cost agreements robust enough to realise most available gains from trade?**  
   - **Paper anchor:** “superintelligence could enable iron-clad contracts… transaction costs would generally be extremely small relative to the gains.” (Section 3.1) and “The extent to which the hypothetical gains from frictionless trade are actually realised.” (Section 3.2)  
   - **The flip:** If enforceability/low-cost reliability \(r\) > ~0.9 across major actor types, then **strategy: invest in formal ‘moral trade’ infrastructure (contracting, constitutional mechanisms, dispute resolution) rather than trying to enforce one value set**; if \(r\) < ~0.5, then **strategy: shift toward centralised governance/alignment of the dominant decision system, because bargaining won’t scale safely**.  
   - **What would answer it:** Adversarial audits of “AI-enabled contracting” (formal verification + cryptographic commitment + automated enforcement) under realistic multi-agent threat models; historical-analogy modelling of enforcement failure rates under extreme capability asymmetries.

4. **What is the expected share of future resources that end up devoted to *executed* value-destroying threats in multi-party bargaining, and how does that vary with multipolarity and enforcement norms?**  
   - **Paper anchor:** “even small risks of executed threats can easily eat into the expected value…” and “If… only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost.” (Section 3.3)  
   - **The flip:** If executed-threat resource share \(t\) > ~0.1–1% (or even lower on some axiologies), then **strategy: prioritise threat prevention above ‘more trade’ (including reducing multipolar rivalry, building strong anti-extortion norms/commitments, and designing institutions to make threats non-credible)**; if \(t\) < ~0.01%, then **strategy: prioritise expanding compromise/trade capacity because it dominates expected value**.  
   - **What would answer it:** Game-theoretic + agent-based simulations calibrated on real-world extortion/blackmail dynamics; “red-team” experiments with AI agents negotiating under different commitment/enforcement regimes to measure emergent extortion equilibria.

5. **Which side of the paper’s axiological danger zone are we in: are bads “separately aggregated” and/or effectively unbounded/overweight relative to goods such that small threat rates make futures net-negative (or “worse than extinction”)?**  
   - **Paper anchor:** “On many views, bads weigh more heavily than goods… even if only a small fraction of resources are devoted to executed threats, most of the value of the future could be lost… [trade scenarios] could… even render them worse than extinction.” (Sections 3.3, 6) and the case split in 3.4.  
   - **The flip:** If the correct/decision-relevant axiology implies (separate aggregation or unbounded downside or heavy bad-weighting) such that the “critical threat rate” \(t^\*\) < ~0.1%, then **strategy: treat s-risk/threat suppression as the top priority even if it slows flourishing/expansion**; if \(t^\*\) > ~5–10%, then **strategy: focus on flourishing-maximisation via institutions for compromise and broad value satisfaction**.  
   - **What would answer it:** Explicit moral-uncertainty modelling with sensitivity to boundedness and aggregation rules, paired with empirical “moral parliament” elicitation of where reflective stakeholders place weight on separate aggregation/lexical bads; decision analyses computing \(t^\*\) under candidate axiologies.

6. **How concentrated will post‑AGI power be (effective number of decisive actors), and does it fall below the level where partial AM‑convergence plus trade is realistically available?**  
   - **Paper anchor:** “concentration of power… becomes less likely that the correct moral views are represented… and therefore less likely that we get to a mostly-great future via trade and compromise.” (Section 3.5)  
   - **The flip:** If effective decisive-actor count \(N_{\text{eff}}\) < ~10–100, then **strategy: prioritise interventions aimed at a small set of pivotal actors (elite governance, targeted alignment/control, preventing dictatorship capture)**; if \(N_{\text{eff}}\) > ~1,000+, then **strategy: prioritise scalable institutions for bargaining, representation, and anti-threat norms (since many views will have real bargaining weight)**.  
   - **What would answer it:** Forecasts from industrial-organisation/political-economy models of compute and AI deployment concentration; empirical tracking of frontier model ownership, inference chokepoints, and military/policy capture pathways under plausible AGI timelines.

7. **How soon do “most-important decisions” become effectively locked in, relative to the timescale on which asymmetric growth (“long views win”) could shift resources toward patient/altruistic values?**  
   - **Paper anchor:** “non-discounting values might not have time to win out… If the major decisions are made soon, and then persist, then there just won’t be time for this selection effect to win out.” (Section 2.3.3) and “early lock-in… when the most-important decisions are being made…” (Section 2.5).  
   - **The flip:** If lock-in time \(T_{\text{lock}}\) < ~10–30 years after transformative AI, then **strategy: prioritise urgent near-term shaping (AI governance, institutional design, pre-lock-in threat suppression) over slow cultural/selection-based approaches**; if \(T_{\text{lock}}\) > ~100–1,000 years, then **strategy: prioritise mechanisms that let patient/altruistic values compound (growth advantages, durable institutions, pluralism) rather than racing immediate lock-in fights**.  
   - **What would answer it:** Scenario analyses identifying which decisions are “reversible” under advanced tech (e.g., constitutions, expansion patterns, AI autonomy, irreversible cosmological engineering), plus technical work on the feasibility of later “constitutional resets” or value pluralism once expansion begins.