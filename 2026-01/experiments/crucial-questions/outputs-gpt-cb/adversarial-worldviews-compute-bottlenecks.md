1. **What is the effective CES substitutability parameter \( \rho \) between compute \(K\) and AI cognitive labour \(L\) specifically for AI R&D *after OAI* across 1–5 orders of magnitude increases in \(L\), given the paper’s claim that “most likely, \(-0.2<\rho<0\)” and that \(\rho\) might be near Cobb–Douglas (\(\rho=0\))?**  
   - **Worldview:** Governance Realist *(also overlaps Alignment Pessimist & Capability Accelerationist)*  
   - **Paper anchor:** “This all leaves me thinking that, most likely, \( -0.2<\rho<0\)” and the “max speed” table/graphs translating \(\rho\) into ceilings on progress speed with fixed compute.  
   - **Strategic implications:** If answered \(\rho \le -0.3\) (hard/early bottleneck), strategy becomes “prioritise compute governance and hardware choke-points, accept slower takeoff”; if answered \(\rho \approx 0\) (weak bottleneck), strategy becomes “treat rapid SIE as plausible, prioritise pre-deployment controls, monitoring, and fast alignment/containment planning.”

2. **How quickly can a leading lab “reconfigure AI R&D to make good use of abundant cognitive labour” (the paper’s Jones-style argument) once millions of AGI researchers are available—days/weeks as the paper suggests, or years/decades as in typical economic adjustment?**  
   - **Worldview:** Governance Realist *(also overlaps Alignment Pessimist)*  
   - **Paper anchor:** “that reconfiguration could be very quick with fast-thinking AGIs… in days or weeks,” applying Jones (2003) to argue short-run complementarity may vanish quickly.  
   - **Strategic implications:** If answered “days/weeks,” strategy becomes “assume discontinuous governance lag; build ex ante international commitments, emergency response, and hard operational constraints”; if answered “years,” strategy becomes “incremental regulatory capacity-building and iterative safety standards are viable.”

3. **Are “near-frontier experiments” (the paper’s example: runs using ~1% of a lab’s compute) actually necessary for major algorithmic breakthroughs, contrary to the paper’s claim that extrapolation from smaller runs and other methods can substitute?**  
   - **Worldview:** Capability Accelerationist *(also overlaps Governance Realist)*  
   - **Paper anchor:** The paper rejects the sceptic’s move from “more experiments” to “near-frontier experiments are fixed,” arguing (i) near-frontier may be unnecessary and (ii) near-frontier experiment counts have fallen over 10 years without slowing progress.  
   - **Strategic implications:** If answered “yes, near-frontier is necessary,” strategy becomes “compute access/cluster size remains the decisive competitive lever; hardware controls and scaling policy dominate”; if answered “no,” strategy becomes “software/algorithmic innovation and organisational scaling dominate; policies focused only on frontier compute may not prevent rapid capability jumps.”

4. **Do algorithmic efficiency gains during an SIE reliably convert into *more informative* experimentation capacity (not just cheaper training at the same frontier), as the paper claims when it says labs can increase both “cognitive labour and # experiments” without more hardware?**  
   - **Worldview:** Capability Accelerationist  
   - **Paper anchor:** “when your AI algorithms become twice as efficient, you can run twice as many experiments… during an SIE, labs can increase the quantity of both key inputs.”  
   - **Strategic implications:** If answered “yes, roughly proportional,” strategy becomes “expect strong positive feedback and faster-than-governance dynamics; prioritise rapid capability-safe deployment pathways and robust monitoring”; if answered “no (efficiency doesn’t buy comparable marginal R&D signal),” strategy becomes “compute bottlenecks are more real; slowing scaling and constraining compute could substantially change outcomes.”

5. **Given the paper’s SIE definition (“\(\ge 5\) OOM increase in effective training compute in <1 year without more hardware”) and its claim that “society wouldn’t have time to prepare,” can alignment/control techniques realistically improve and be deployed on comparable timescales, or will deceptive/inner-misaligned systems dominate by default?**  
   - **Worldview:** Alignment Pessimist *(overlaps Governance Realist)*  
   - **Paper anchor:** The paper’s explicit fast-timeline framing (“few months or years,” “<1 year” for \(\ge 5\) OOM) and conclusion that compute bottlenecks likely “don’t bite” until later stages.  
   - **Strategic implications:** If answered “alignment can keep pace,” strategy becomes “lean into fast capability progress with tight evaluation and controlled deployment”; if answered “alignment cannot keep pace,” strategy becomes “prioritise aggressive slowing/containment, hard access controls, and governance actions that reduce the probability of entering a fast SIE regime at all.”

6. **If compute bottlenecks “don’t slow an SIE until its late stages,” do compute-centric governance tools (chip export controls, datacenter licensing, training-run reporting) meaningfully reduce SIE risk, or do they mainly push acceleration into software-only, harder-to-monitor channels and worsen coordination failures?**  
   - **Worldview:** Governance Realist *(also overlaps Capability Accelerationist)*  
   - **Paper anchor:** The paper’s core conclusion that compute bottlenecks probably won’t bind early, plus its framing that an SIE could proceed “without needing more hardware.”  
   - **Strategic implications:** If answered “compute governance still meaningfully reduces risk,” strategy becomes “double down on international compute monitoring/enforcement and capacity-building”; if answered “it mostly displaces effort into software secrecy,” strategy becomes “shift emphasis toward algorithmic progress surveillance, lab auditing, information-security governance, and coordination mechanisms beyond compute controls.”

7. **Would the paper’s substitution routes (e.g., the toy example where “AGIs do the math for NNs in their heads,” plus heavy use of parallel copies and fast-thinking cognitive labour) create vast numbers of potentially sentient digital workers such that digital welfare/rights considerations dominate the expected value of enabling or preventing an SIE?**  
   - **Worldview:** Digital Minds Advocate  
   - **Paper anchor:** The paper’s claim that “cognitive labour can in principle fully substitute for compute,” illustrated by internal simulation (“do the math for NNs in their heads”) and the broader reliance on scaling via many AGI researchers/copies.  
   - **Strategic implications:** If answered “yes, likely morally patient at large scale,” strategy becomes “add binding welfare constraints (limits on copying, working conditions, consent, shutdown rights) and potentially slow/redirect SIE pathways”; if answered “no/low probability,” strategy becomes “treat digital-welfare externalities as secondary and focus primarily on takeover/coordination risks.”