1. **What is the effective CES complementarity parameter ρ for AI R&D once cognitive labor is scaled up by ≥1–3 OOM (i.e., “millions of AGIs”) while compute is held fixed?**  
   - **Paper anchor:** “*my views… leave me thinking that, most likely, −0.2<ρ<0*” and “*if today you poured an unlimited supply… the pace… would increase by a factor of ∼30* (for ρ=−0.2).”  
   - **The flip:** If **ρ < −0.2** (early ceiling ≲30×), then **strategy A: treat compute as the binding constraint and prioritize compute governance / hardware-access controls**; if **ρ ≥ −0.2** (ceiling high enough not to bind early), then **strategy B: treat rapid software-only acceleration as plausible and prioritize fast alignment + crisis governance for short timelines**.  
   - **What would answer it:** A controlled “fixed-compute R&D scaling” study where many autonomous research agents (human+AI, then AI-only) pursue algorithmic improvements under a constant compute budget, estimating how progress speed scales with added cognitive labor (and fitting ρ, allowing non-CES alternatives).

2. **How quickly does AI R&D “reconfigure” so that short-run complementarity (low ρ) rises toward long-run ρ≈0 when cognitive labor suddenly becomes abundant?**  
   - **Paper anchor:** “*in the longer run we invent new production processes… and so ρ is higher… that reconfiguration could be very quick with fast-thinking AGIs… days or weeks*.”  
   - **The flip:** If **reconfiguration time τ ≤ ~1–2 months**, then **strategy A: assume bottlenecks won’t buy meaningful time and prioritize immediate, takeoff-speed alignment/governance**; if **τ ≥ ~1–2 years**, then **strategy B: expect a prolonged compute-bottleneck plateau and prioritize medium-term institution-building / monitoring over emergency measures**.  
   - **What would answer it:** Time-to-adaptation evidence from “shock” deployments (sudden large increases in agentic R&D labor) measuring how fast labs change workflows to exploit parallelism (e.g., automated experiment design, eval pipelines, codebase refactors) and how the marginal returns to added agents evolve week by week.

3. **What is the correct “task share” parameter α for AI R&D—i.e., what fraction of progress production is fundamentally compute-like vs cognitive-labor-like—rather than the paper’s simplifying α=0.5?**  
   - **Paper anchor:** “*I’ll assume α=0.5 throughout.*”  
   - **The flip:** If **α ≥ ~0.7** (progress mostly compute-like), then **strategy A: prioritize compute supply/controls and assume software-only acceleration is strongly limited**; if **α ≤ ~0.3** (progress mostly labor-like), then **strategy B: prioritize controlling/aligning cognitive labor (agents) because compute is less decisive**.  
   - **What would answer it:** Task-level decomposition of the AI R&D pipeline (idea generation, coding, debugging, training, evals, interpretability, data work) with measured marginal progress impact per additional unit of compute vs per additional unit of high-quality cognitive labor, estimated across multiple labs/projects.

4. **What fraction of meaningful algorithmic progress requires “near-frontier” experiments (e.g., ≥1% of a lab’s compute) rather than extrapolations from much smaller runs?**  
   - **Paper anchor:** “*What really matters is… ‘near-frontier’ experiments… And the number… is fixed. But… you might not need near-frontier experiments… you might… extrapolate from… increasingly small fractions…*”  
   - **The flip:** If **≥~50%** of key progress steps *require* near-frontier runs, then **strategy A: compute bottlenecks dominate and we should focus on frontier-compute governance/leverage points**; if **≤~10–20%** require near-frontier runs, then **strategy B: expect rapid software iteration under fixed compute and prioritize alignment + deployment governance**.  
   - **What would answer it:** A retrospective + prospective “compute provenance” dataset: for major algorithmic improvements, record the largest-run fraction needed to discover/validate them; then run a pre-registered replication program testing whether the same improvements can be derived using only small-scale experiments plus extrapolation.

5. **Does algorithmic efficiency improvement during an SIE increase experiment throughput fast enough to offset rising “idea difficulty,” or does required compute per marginal improvement grow faster?**  
   - **Paper anchor:** “*when your AI algorithms become twice as efficient, you can run twice as many experiments… This completely pulls the rug out… which assumed an essential input was held fixed.*”  
   - **The flip:** If **(rate of efficiency-driven experiment-throughput growth) > (rate of increasing compute needed per marginal idea)**, then **strategy A: expect sustained acceleration under fixed hardware (software-only takeoff) and prioritize fast alignment/governance**; if the inequality reverses, then **strategy B: expect deceleration/plateau and prioritize compute scaling/controls plus longer-horizon safety work**.  
   - **What would answer it:** Empirical time-series estimating (i) compute-to-capability efficiency gains and (ii) compute cost of producing the next “unit” of algorithmic improvement (e.g., benchmark gain attributable to algorithm changes), using lab-internal logs or standardized external competitions under fixed compute budgets.

6. **How large is the multiplicative speedup from “smarter and faster” AI researchers (not just more copies) on algorithmic progress, once bottlenecked experiments are accounted for?**  
   - **Paper anchor:** “*economic estimates don’t include labourers becoming smarter or thinking faster… moving… to top employee… increase… by a factor of 6 (median)… don’t speak to… thinking speed increasing*.”  
   - **The flip:** If **per-generation researcher-quality+thinking-speed gains produce ≥~10× R&D progress speedup** even under fixed experiment throughput, then **strategy A: treat takeoff as potentially extremely fast and prioritize emergency alignment and hard-stop governance options**; if **≤~2–3×**, then **strategy B: expect compute/experiment cadence to dominate and prioritize compute governance + slower-burn alignment**.  
   - **What would answer it:** Head-to-head trials where matched teams (or agent ensembles) with different capability levels/clock speeds are given the same fixed experiment budget and identical tool access, measuring realized algorithmic progress and isolating the contribution of pure cognition (planning, theory, debugging, architecture search) vs experiments.

7. **Is there at least one “compute-light” route to major capability gains (the paper’s “strongest link” framing) whose effective ρ is near 0 and whose returns are sufficient to reach superintelligence without near-frontier scaling?**  
   - **Paper anchor:** “*alternative frame… multiple possible routes… you just need one of them to work… The compute bottleneck objection only works if all of these routes are bottlenecked by compute.*”  
   - **The flip:** If **≥1 route** (e.g., scaffolding/tool-use, data flywheels, distillation/compression, mechanistic insights) has **effective ρ ≥ ~−0.1** *and* demonstrated strong returns, then **strategy A: assume compute bottlenecks won’t reliably prevent an SIE and prioritize alignment + broad governance**; if **all routes** empirically show **ρ ≤ ~−0.3** or weak returns, then **strategy B: focus on compute as the primary choke point (hardware/export controls, scheduling, monitoring)**.  
   - **What would answer it:** A route-by-route empirical portfolio evaluation: define candidate “compute-light” pathways, run constrained-compute research programs for each, and estimate their marginal capability gains per unit cognitive labor at constant compute—then take the max (strongest-link) rather than an average.