**Does algorithmic efficiency improvement during an SIE increase experiment throughput fast enough to offset rising “idea difficulty,” or does required compute per marginal improvement grow faster?**  
   - **Paper anchor:** “*when your AI algorithms become twice as efficient, you can run twice as many experiments… This completely pulls the rug out… which assumed an essential input was held fixed.*”  
   - **The flip:** If **(rate of efficiency-driven experiment-throughput growth) > (rate of increasing compute needed per marginal idea)**, then **strategy A: expect sustained acceleration under fixed hardware (software-only takeoff) and prioritize fast alignment/governance**; if the inequality reverses, then **strategy B: expect deceleration/plateau and prioritize compute scaling/controls plus longer-horizon safety work**.  
   - **What would answer it:** Empirical time-series estimating (i) compute-to-capability efficiency gains and (ii) compute cost of producing the next “unit” of algorithmic improvement (e.g., benchmark gain attributable to algorithm changes), using lab-internal logs or standardized external competitions under fixed compute budgets.