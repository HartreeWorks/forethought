**Are “near-frontier experiments” (the paper’s example: runs using ~1% of a lab’s compute) actually necessary for major algorithmic breakthroughs, contrary to the paper’s claim that extrapolation from smaller runs and other methods can substitute?**  
   - **Worldview:** Capability Accelerationist *(also overlaps Governance Realist)*  
   - **Paper anchor:** The paper rejects the sceptic’s move from “more experiments” to “near-frontier experiments are fixed,” arguing (i) near-frontier may be unnecessary and (ii) near-frontier experiment counts have fallen over 10 years without slowing progress.  
   - **Strategic implications:** If answered “yes, near-frontier is necessary,” strategy becomes “compute access/cluster size remains the decisive competitive lever; hardware controls and scaling policy dominate”; if answered “no,” strategy becomes “software/algorithmic innovation and organisational scaling dominate; policies focused only on frontier compute may not prevent rapid capability jumps.”