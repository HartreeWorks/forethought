**What fraction of meaningful algorithmic progress requires “near-frontier” experiments (e.g., ≥1% of a lab’s compute) rather than extrapolations from much smaller runs?**  
   - **Paper anchor:** “*What really matters is… ‘near-frontier’ experiments… And the number… is fixed. But… you might not need near-frontier experiments… you might… extrapolate from… increasingly small fractions…*”  
   - **The flip:** If **≥~50%** of key progress steps *require* near-frontier runs, then **strategy A: compute bottlenecks dominate and we should focus on frontier-compute governance/leverage points**; if **≤~10–20%** require near-frontier runs, then **strategy B: expect rapid software iteration under fixed compute and prioritize alignment + deployment governance**.  
   - **What would answer it:** A retrospective + prospective “compute provenance” dataset: for major algorithmic improvements, record the largest-run fraction needed to discover/validate them; then run a pre-registered replication program testing whether the same improvements can be derived using only small-scale experiments plus extrapolation.