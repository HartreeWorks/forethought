**How much of frontier AI progress is driven by compute-intensive empirical iteration versus “compute-light” routes (theory, architecture insights, data curation, scaffolding, distillation, interpretability-driven debugging), and are any of these routes sufficient alone to sustain multi-OOM capability gains?** — The paper invokes a “strongest-link” framing where only one route needs favorable substitutability, but it does not quantify how potent these alternative routes are. If all major routes ultimately depend on large training runs (directly or indirectly), then the CES-style bottleneck framing becomes much more appropriate.