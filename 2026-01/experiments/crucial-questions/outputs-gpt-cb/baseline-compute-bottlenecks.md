1. **What is the empirically correct elasticity of substitution (ρ) between “cognitive labor” and compute specifically within frontier AI R&D, across large changes in L/K rather than the ~2× variation seen in most economic estimates?** — The paper’s core claim is that economy-wide ρ estimates are a weak guide and that AI R&D likely has higher substitutability (ρ closer to 0), but it provides no direct measurement in the relevant domain. If AI-R&D-specific ρ is actually strongly negative, the “max speed” ceiling could be low enough to substantially blunt or prevent an SIE.

2. **How does ρ change endogenously as AI R&D processes are reorganized by fast, abundant automated researchers (i.e., is there rapid “Jones-style” adaptation that pushes ρ toward 0 on short timescales)?** — A central counterargument is that complementarities observed in the short run may dissolve as methods and pipelines reconfigure, potentially within days or weeks under AI automation. If reconfiguration is slow or limited, then short-run bottlenecks could dominate exactly during the period when the paper expects early-stage acceleration.

3. **To what extent does algorithmic progress actually increase the number of informative experiments per unit of fixed hardware when the goal is improving near-frontier capability rather than running more small experiments?** — The paper argues that “compute” is not fixed because efficiency gains let you run more experiments, and that near-frontier experiments may not be necessary, but this is an empirical claim about how progress is generated. If near-frontier scaling experiments are disproportionately required for major capability gains, then fixed hardware could impose a much tighter constraint than the paper suggests.

4. **What is the real-world scaling relationship between experiment count/quality and marginal algorithmic progress (i.e., do “ideas get harder” in a way that makes progress sublinear even with massive cognitive labor)?** — The compute-bottleneck objection relies on increasing difficulty and diminishing returns as low-hanging fruit is exhausted, while the paper implies abundant cognition can keep idea generation productive. Measuring how returns to additional experiments and researcher-hours change with proximity to the frontier would determine whether acceleration persists or predictably stalls.

5. **How much of frontier AI progress is driven by compute-intensive empirical iteration versus “compute-light” routes (theory, architecture insights, data curation, scaffolding, distillation, interpretability-driven debugging), and are any of these routes sufficient alone to sustain multi-OOM capability gains?** — The paper invokes a “strongest-link” framing where only one route needs favorable substitutability, but it does not quantify how potent these alternative routes are. If all major routes ultimately depend on large training runs (directly or indirectly), then the CES-style bottleneck framing becomes much more appropriate.

6. **What are the binding non-compute constraints during automated AI R&D (data access/rights, evaluation and feedback latency, engineering throughput, memory/IO bottlenecks, coordination and verification costs), and do they effectively reintroduce a “fixed factor” analogous to compute?** — The paper focuses on compute versus cognition, but an SIE could be limited by other scarce inputs that do not scale with more automated researchers. If these constraints dominate, then increasing cognitive labor may not translate into proportionally faster progress even when GPU compute is ample or more substitutable than assumed.

7. **Does the CES production-function mapping (Y as “pace of software progress,” K as compute, L as cognitive labor) accurately represent the dynamics of AI R&D, or do alternative process models (e.g., search over discrete algorithmic breakthroughs, parallel portfolio effects, winner-take-most selection) yield qualitatively different predictions about bottlenecks and takeoff speed?** — The paper critiques parameter choices but largely accepts the CES framing as a reference point, despite acknowledging heroic extrapolation and mismatched domains. If CES is the wrong functional form for R&D, then conclusions derived from “max speed” ceilings and substitutability parameters may be misleading, changing how we should forecast and respond to potential SIE scenarios.