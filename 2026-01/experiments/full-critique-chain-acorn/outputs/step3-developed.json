[
  {
    "id": "c20",
    "title": "The Target-Size/Value-Loss Conflation",
    "expanded": "### Problem\nThe paper repeatedly infers that because there are **many large value-loss modes**, the set of mostly-great futures is therefore “narrow” (i.e., low measure/probability). That inference does not follow without an explicit claim about the *future-generating process*: “many ways to go wrong” establishes axiological fragility (large losses are possible), not dynamical typicality (large losses are likely).\n\nThe issue is clearest in the transition from the catalogue of potential “moral catastrophes” (Section 2.3) to the “tiny orange target” conclusion (Section 2.4). Merely listing many dimensions on which value *could* drop does not show that futures above the 0.5 threshold are rare *by default*.\n\n### Why it matters\nThis is not a peripheral methodological worry; it undermines the paper’s headline thesis (“no easy eutopia”). The introduction frames the core question probabilistically: “among all the futures humanity could achieve given survival, weighted by how likely those futures would be… what fraction… [are] mostly-great?” Yet the subsequent argument frequently slides from modal claims (“could be catastrophic”) to probabilistic conclusions (“mostly-great is rare”). If that slide fails, the paper becomes a cautionary inventory rather than an argument about target size.\n\n### Where it arises\n- Section 2.3 concludes: “There are therefore **many ways** in which the future could lose out on most value…” (population ethics) and similarly for digital beings, wellbeing, space governance, etc.\n- Section 2.4 then proposes a toy model in which value is “the product of N dimensions, all sampled from **independent uniform distributions**,” yielding mostly-great outcomes as rare.\n\nThe uniform-sampling assumption is already a substantive probabilistic model. It is not derived from any account of institutional learning, moral deliberation, technological path dependence, or selection effects.\n\n### What would address it\nThe paper needs to make (and defend) at least a coarse generative story linking the *number of possible flaws* to *their probability mass*. For example:\n- Specify a model of trajectory formation (institutional competence, moral reflection, coordination, AI governance) that induces a distribution over “flaw severities.”\n- Justify any independence/near-independence assumptions, or replace them with a causal model.\n- Show, under that model, that \\(P(V>0.5 \\mid \\text{Survival})\\) is small absent deliberate optimization.\n\nAbsent this, the central “target is small” conclusion is under-argued."
  },
  {
    "id": "c03",
    "title": "Upstream-Competence Dominance",
    "expanded": "### Problem\nThe paper’s pivotal example, “Common-sense utopia” (Section 2.2), builds in **extremely strong upstream competence**—scientific progress without catastrophe, collaboration replacing war, environmental restoration, and “minimal suffering among nonhuman animals and non-biological beings.” These are not merely scenic background; they are precisely the epistemic, institutional, and moral achievements that plausibly reduce the probability of the downstream moral blind spots the paper then treats as still highly salient.\n\nThe argumentative structure thus risks a non sequitur: the paper conditions on a world with unusually successful governance and coordination, but then treats the probability of catastrophic moral error as roughly unchanged.\n\n### Why it matters\nThe “no easy eutopia” conclusion is intended to hold *even under abundance* and *even if everyone is satisfied*. But the strongest evidence the paper offers for the pervasiveness of moral catastrophe comes from historical cases (Section 2.1) where upstream competence is precisely what was missing: weak institutions, limited scientific understanding, parochial epistemic norms, and entrenched domination. Conditioning on the paper’s common-sense utopia plausibly screens off much of that inductive support.\n\nIf downstream error probability is in fact strongly decreasing in upstream competence, then “mostly-great is rare conditional on survival and abundance” becomes much harder to sustain.\n\n### Where it arises\nThe “Common-sense utopia” description states:\n- “Scientific understanding and technological progress move ahead, **without endangering the world**.”\n- “Collaboration and bargaining **replace war and conflict**.”\n- “There is **minimal suffering** among nonhuman animals and non-biological beings.”\n\nImmediately after, Section 2.3 announces “Future catastrophes are easy,” presenting digital rights, population ethics, wellbeing theory, and space governance as if their failure modes remain comparably live.\n\n### What would address it\nTo preserve the intended conditional claim, the paper should argue for robust mechanisms by which advanced, stable, low-suffering societies systematically preserve deep moral error. This likely requires:\n- An account of how moral learning can stall under preference engineering, propaganda, lock-in, or principal–agent problems with AI.\n- Examples where high competence coexisted with severe moral blind spots (not merely “it happened historically,” but cases where the background competence was plausibly high).\n- A sensitivity analysis: how low must the conditional probability of each downstream error be, given upstream success, for the “mostly-great is rare” conclusion to survive?\n\nWithout such bridging, the paper’s strongest scenario threatens to undercut its own pessimistic inference."
  },
  {
    "id": "c15",
    "title": "The Eutopia/Extinction Commensuration Leak",
    "expanded": "### Problem\nThe paper operationalizes cardinal value via extinction gambles: “eutopia is over twice as good as some other future just in case a 50–50 gamble between eutopia and near-term extinction is better than a guarantee of that other future” (Introduction; reiterated in Section 3.1). This is presented as compatible with “non-consequentialist views.” However, for many rights-based or constraint-based views, extinction is not merely a low-value outcome; it can change the space of obligations, permissions, and constraints. As a result, **extinction gambles may be impermissible comparators**, even if outcomes are in some abstract sense rankable.\n\n### Why it matters\nThis is not a terminological worry; it affects the paper’s measurement backbone. The “mostly-great” threshold is defined as “at least 50% of its value,” where 0 is extinction and 1 is best feasible future (Section 3.1). The paper also motivates its conclusion with the intuition that “we should hope for a 60–40 gamble between eutopia and extinction” rather than many seemingly wonderful guarantees (Introduction). If the extinction gamble is ethically illicit or conceptually unstable for a wide class of views, then the 0–1 normalization and the 0.5 threshold cease to be neutral, and the paper’s comparisons across moral views and under moral uncertainty (Section 3.5) become suspect.\n\n### Where it arises\n- Introduction: the explicit bet-based definition and the “60–40 gamble” motivation.\n- Section 3.1: stipulating “guaranteed extinction has a value of 0,” and using extinction as the fixed zero-point for intertheoretic aggregation.\n- Section 3.5: normalization methods that repeatedly anchor scales to extinction/best-feasible endpoints.\n\n### What would address it\nThe paper needs either (i) an argument that extinction is a permissible calibrator across the relevant class of non-consequentialist theories, or (ii) an alternative operationalization.\n\nConcretely:\n- Clarify the target class of “non-consequentialist” views and whether they accept VNM independence in the presence of deontic constraints.\n- Replace extinction anchoring with a neutral baseline outcome that does not trigger special prohibitions (e.g., a minimally decent survival outcome), or use intra-theory scaling based on permissible tradeoffs.\n- Revisit the 0.5 “mostly-great” cutoff under the new scaling; the paper should show that the qualitative “narrow target” claim is robust to avoiding extinction wagers.\n\nAs written, the paper risks deriving quantitative “fussiness” from a comparator that many theories cannot coherently use."
  },
  {
    "id": "c02",
    "title": "The Factorization Fallacy",
    "expanded": "### Problem\nSection 2.4’s central intuition pump treats overall value as the product of “relatively independent” factors, illustrated by a toy model where each dimension is drawn from an **independent uniform** distribution. The paper’s intended lesson is that doing poorly on any one dimension can crush total value, making mostly-great futures rare.\n\nThe difficulty is that the paper’s own list of moral failure modes is not plausibly independent in the relevant sense. Many “dimensions” (digital rights, population ethics, space governance, wellbeing theory, value lock-in) are likely driven by common upstream variables: epistemic institutions, deliberative norms, governance quality, alignment success, and moral pluralism. If those upstream variables induce strong positive correlation, then the multiplicative “tiny orange target” geometry is misleading: competence clusters can make high performance on one dimension predictive of high performance on many others.\n\n### Why it matters\nThe multiplicative model is **load-bearing**: it is the main bridge from “single flaws can matter a lot” to “mostly-great outcomes are rare by default.” If correlations are strong, then the distribution of overall value is not the product-of-uniforms distribution the paper depicts (Figure in 2.4). Instead, we might expect a mixture distribution with a substantial mass in a “good governance” regime, in which many downstream issues are jointly handled tolerably well. That possibility directly threatens the “narrow target” conclusion.\n\n### Where it arises\n- Section 2.4: “value … is the product of N dimensions, all sampled from independent uniform distributions.”\n- The accompanying graph and the “small orange area” diagram, which visually encode the independence assumption.\n- The earlier catalogue in Section 2.3, which implicitly treats issues as separable “pitfalls” rather than outputs of shared institutional pathways.\n\n### What would address it\nThe paper should replace factorization-as-metaphor with an explicit causal structure. For example:\n- Introduce latent upstream variables (e.g., coordination capacity, quality of moral epistemology, robustness to lock-in, alignment success) and model downstream moral decisions as conditional on these.\n- Explore correlation regimes: show that even with plausible positive correlations, the mass above 0.5 remains small, or else qualify the conclusion.\n- Provide justification for independence where claimed (e.g., mechanisms that decouple population ethics from digital rights), rather than relying on the generic thought that “there are many issues.”\n\nAt minimum, the paper should acknowledge that the product model is not merely illustrative; it implicitly makes a probabilistic independence claim. Without defending or revising that claim, the main quantitative intuition about rarity is not supported."
  },
  {
    "id": "c16",
    "title": "The Survival/Flourishing Separation Breaks Under Value Lock-In",
    "expanded": "### Problem\nThe paper’s framing separates “Surviving” from “Flourishing”: Flourishing is “the expected value of the future conditional on Surviving” (Introduction; also the Surviving vs Flourishing graphic). This invites the methodological move: first focus on avoiding extinction, then ask how hard eutopia is among the surviving trajectories.\n\nHowever, several of the paper’s own mechanisms—especially “Wrong reflective process” and early lock-in (Section 2.3.5), and the claim that initial settlement “captur[es] essentially all resources that will ever be available” (Section 2.3.4)—suggest that survival pathways and value outcomes are tightly coupled. Policies and technologies that raise survival probability (centralized control, rapid expansion, powerful AI governance, early coordination) can simultaneously increase the risk of moral monoculture or irreversible value lock-in. Conversely, decentralized pluralism may preserve moral exploration but heighten existential vulnerability. Under such coupling, “conditional on survival” is not a stable reference class.\n\n### Why it matters\nThe paper’s practical upshot concerns prioritization: if Flourishing losses dominate Surviving losses, we should invest more in improving futures conditional on survival. But if survival interventions systematically reshape the flourishing distribution, then the decomposition into independent “red area” (extinction) and “blue area” (value loss) is potentially misleading. The relevant object is the **joint distribution** over (survival strategy, survival probability, conditional value). Otherwise, the paper risks concluding “eutopia is hard” from a reference class that is an artifact of particular survival policies.\n\n### Where it arises\n- Introduction: explicit definition of Flourishing as conditional on Surviving.\n- Section 2.3.4: early space resource capture creates lasting path dependence.\n- Section 2.3.5: “The first generation … might lock in their unreflective values.”\n- The broader “no serious, coordinated efforts” stipulation (Introduction; Section 3.1) interacts with survival policy, since many survival measures are inherently coordinated and steering-intensive.\n\n### What would address it\nThe paper should explicitly model survivorship selection. Concretely:\n- Distinguish multiple survival pathways (e.g., centralized aligned-AI governance; competitive multipolar governance; slow institution-building) with different conditional value profiles.\n- Evaluate whether “eutopia is rare” holds within each pathway, rather than only in an undifferentiated surviving set.\n- Address policy endogeneity: show that the recommended emphasis (survival vs flourishing work) is robust when interventions shift both survival probability and conditional flourishing.\n\nAbsent this, the Surviving/Flourishing partition risks obscuring precisely the kind of lock-in dynamics the paper itself treats as central to eutopian fragility."
  }
]