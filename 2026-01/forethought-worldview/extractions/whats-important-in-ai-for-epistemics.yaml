paper:
  slug: "whats-important-in-ai-for-epistemics"
  title: "What's Important in 'AI for Epistemics'?"

premises_taken_as_given:
  - claim: "AI capabilities are advancing rapidly and may automate almost all economically relevant tasks within ~10 years."
    confidence: "near-certain"
    evidence: "Stated as background context without argument; the paper is oriented around preparing for this eventuality."
  - claim: "Having excellent societal epistemic processes is very valuable, especially during the development and deployment of transformative AI."
    confidence: "near-certain"
    evidence: "Treated as foundational motivation; the paper never argues for why good epistemics matter, only assumes it."
  - claim: "AI takeover is a serious risk, and most of that risk comes from 'unforced errors' — people having poorly calibrated beliefs about the danger."
    confidence: "strong"
    evidence: "The author states their subjective probability of AI takeover would 'more than halve' if decision-makers had well-calibrated beliefs, presupposing takeover risk is substantial."
  - claim: "The bitter lesson (that methods leveraging compute tend to dominate those leveraging human knowledge) applies to AI epistemics work."
    confidence: "strong"
    evidence: "Used as a core filter for evaluating projects; projects that would be 'solved by default' with more compute are deprioritized."
  - claim: "Frontier AI companies and governments are the key actors whose decisions matter most around TAI development."
    confidence: "near-certain"
    evidence: "These are the actors the paper repeatedly targets for demonstration and influence, without justifying why they are the most important."
  - claim: "Alignment/safety research (scalable oversight, ELK, W2S generalization) is important and on the right track."
    confidence: "strong"
    evidence: "These research programs are cited as straightforwardly valuable without arguing for their validity; treated as established."

distinctive_claims:
  - claim: "AI-for-epistemics work should be evaluated along two primary axes: durability (won't be swamped by compute scaling or commercial investment) and demonstration value (convincing key actors of AI's epistemic potential)."
    centrality: "thesis"
    key_argument: "Small actors can't compete with industry on general capabilities, so they should focus on problems industry won't solve and on building early demos/evals that shift others' investment priorities."
  - claim: "There is an important and actionable distinction between AI epistemic capabilities in 'understanding' domains (forecasting, strategy, policy) versus 'building' domains (coding, R&D, robotics), and these can be differentially advanced."
    centrality: "load-bearing"
    key_argument: "Understanding domains have worse feedback loops, are more interdisciplinary, and more politically charged — creating systematic differences in what epistemic methods are useful, allowing targeted differential advancement."
  - claim: "Societal epistemics are importantly path-dependent: early investments in epistemic AI norms and capabilities can create self-reinforcing feedback loops (good or bad)."
    centrality: "load-bearing"
    key_argument: "Poor epistemics make it harder to identify what is truth-seeking; people are more willing to invest in novel methods before learning the methods might contradict their beliefs; early norms become entrenched."
  - claim: "Automated question-generation and question-resolution for forecasting is a core infrastructure need for AI epistemics, and prototypes should be built now."
    centrality: "supporting"
    key_argument: "This leverages computation rather than human knowledge, generates training data at scale, and is a prerequisite for the envisioned 'science of AI forecasting.'"
  - claim: "Indirect strategies (demos, evals, and arguments that motivate others to invest) may be comparably valuable to direct capability improvements, because the core advantage is ease of pursuit and the ability to 'wake people up.'"
    centrality: "load-bearing"
    key_argument: "Demos and evals have great feedback loops, don't need to generalize to future systems, and can motivate powerful actors (AI companies, governments) to invest in epistemic tools."
  - claim: "Past-casting (training models without recent data to test long-range forecasting) is a high-value research direction that requires early investment due to technical obstacles and commitment points."
    centrality: "supporting"
    key_argument: "Training chronologically requires committing before large training runs; dating past data is surprisingly difficult; small trials are needed to verify no capability compromise."
  - claim: "Experiments on what arguments/decompositions lead humans toward truth (vs. mislead them) are especially time-sensitive because human experiments can't be past-cast."
    centrality: "supporting"
    key_argument: "Unlike AI-only experiments, human-in-the-loop experiments require real-time forecasting questions and can't be run retroactively."
  - claim: "AI forecasting is more likely to be widely adopted than human forecasting has been, because AI culture already measures performance numerically and AI will eventually deliver decisive advantages over human experts."
    centrality: "supporting"
    key_argument: "The barriers to human forecasting adoption (cultural change, insufficient advantage over domain experts) are smaller for AI systems."
  - claim: "Good norms for AI-as-communicators should include laws against paying third parties to bias their AIs in your favor and against AIs systematically saying contradictory things to different audiences."
    centrality: "supporting"
    key_argument: "These represent concrete policy proposals for maintaining epistemic integrity in AI communication, analogous to advertising and disclosure regulations."

positions_rejected:
  - position: "Work on general AI agent capabilities (navigation, long-horizon tasks, etc.) is a good use of AI-for-epistemics resources."
    why_rejected: "Industry already has huge incentives to pursue these; small actors shouldn't work on problems others will solve better with more resources."
  - position: "Narrowly scoped, highly structured AI analysis tools (e.g., automated literature reviews of RCTs) count as 'ambitious epistemic assistance.'"
    why_rejected: "These are insufficiently ambitious — they can be automated without providing meaningful acceleration to overall strategic reasoning."
  - position: "Leveraging human domain knowledge into AI systems is a robust long-term strategy for improving AI epistemics."
    why_rejected: "The bitter lesson suggests compute-leveraging approaches dominate; human-knowledge-leveraging approaches get swamped as compute scales."
  - position: "Working on synthetic data generation for pretraining is good for the AI-for-epistemics community to prioritize."
    why_rejected: "It advances general AI capabilities too much; capabilities researchers are already incentivized to do it, and it may be net-negative by shortening timelines."
  - position: "Laws forbidding AIs from stating falsehoods are straightforwardly good."
    why_rejected: "Such laws could become tools for pushing political agendas depending on how 'blatant' falsehood is adjudicated and who controls enforcement."

methodological_commitments:
  - "Strategic reasoning about differential technology development — asking 'what capabilities should be advanced relative to others?' rather than 'what is generally good?'"
  - "Path-dependence and feedback-loop reasoning as the basis for tractability arguments (rather than e.g. cost-effectiveness estimates)"
  - "Concrete scenario painting / future visioning as an analytical tool (the 'painting a picture of the future' section envisions AI forecasting systems in detail)"
  - "Evaluation of interventions against explicit heuristics: durability, demonstration value, leveraging computation over human knowledge, avoiding duplication of industry work"
  - "Personal/informal epistemic style — the paper is explicitly framed as a 'personal take' and frequently qualifies confidence levels"
  - "Heavy reliance on the EA/rationalist intellectual ecosystem: references to superforecasting, scalable oversight, ELK, control evaluations, and rationalist blog posts as core intellectual infrastructure"

cross_references:
  - "project-ideas-epistemics"