

# Forethought Research: Worldview Primer

## Purpose
This document summarises the intellectual framework of Forethought Research across 56 papers, so that AI models critiquing Forethought's draft work can engage with their actual positions rather than naively questioning premises the team has deeply considered.

## How to use this primer when critiquing Forethought papers

- **Do not** spend critique space questioning Tier 1 premises in isolation. The team treats these as settled; objecting to them without substantial novel argument will not be useful.
- **For Tier 2 assumptions**, engage only if you have a genuinely new argument, novel evidence, or can identify an internal tension the team may not have noticed. Surface-level objections ("but maybe AI won't be transformative") add nothing.
- **Focus critique energy on Tier 3 questions**—areas where Forethought papers themselves disagree, flag uncertainty, or are actively investigating. Novel arguments here are especially valuable.
- **Look for unstated interactions** between premises. Forethought's most productive internal debates concern how their premises combine (e.g., how lock-in feasibility interacts with moral uncertainty about what to lock in).
- **Be concrete and constructive.** The team publishes rough research notes and explicitly invites feedback; they value specific counterarguments, overlooked considerations, and alternative framings over general scepticism.

---

## Tier 1: Foundational premises

These appear as "near-certain" across 3+ papers and no Forethought paper argues against them.

**1. AI will be transformative, likely among the most consequential developments in human history.**
Virtually every paper takes this as the motivating context. No paper entertains the possibility that advanced AI is unimportant. (*preparing-for-the-intelligence-explosion*, *agi-and-lock-in*, *ai-enabled-coups*, *viatopia*, and ~40 others.)

**2. The long-term future matters enormously, and near-term decisions—especially around AI—can shape it.**
Longtermism is the foundational ethical orientation. The expected value of the far future dominates, and path-dependent effects of present actions are real. (*beyond-existential-risk*, *persistent-path-dependence*, *introducing-better-futures*, *agi-and-lock-in*, *how-to-make-the-future-better*.)

**3. AGI will eventually be developed.**
All governance, lock-in, and intelligence explosion papers are conditional on AGI. The question is when and under what conditions, not whether. (*agi-and-lock-in*, *agi-and-world-government*, *intelsat-as-a-model-for-international-agi-governance*, *what-an-international-project-to-develop-agi-should-look-like*.)

**4. AI will eventually automate AI R&D, creating feedback loops in capability development.**
The intelligence explosion research programme treats AI-driven AI improvement as a matter of "when," not "if." (*will-ai-r-and-d-automation-cause-a-software-intelligence-explosion*, *three-types-of-intelligence-explosion*, *how-quick-and-big-would-a-software-intelligence-explosion-be*, *ai-accelerate-ai-progress-interviews*.)

**5. Lock-in of values, institutions, or power structures is technologically feasible and a serious concern.**
AGI-enforced institutions, digital minds, immortality, and defense-dominant space territories could fix civilisational trajectory for cosmic timescales. (*agi-and-lock-in*, *persistent-path-dependence*, *beyond-existential-risk*, *how-to-make-the-future-better*, *ai-enabled-coups*.)

**6. Power concentration is a core risk, distinct from and potentially comparable to misalignment risk.**
AI-enabled coups, de facto world government by AGI developers, and autocratic lock-in are treated as co-equal concerns alongside AI takeover from misalignment. (*ai-enabled-coups*, *agi-and-world-government*, *should-there-be-just-one-western-agi-project*, *preparing-for-the-intelligence-explosion*.)

**7. Existential risk reduction is genuinely important—but not the only longtermist priority.**
No paper disputes the value of reducing x-risk, but the "Better Futures" series and *beyond-existential-risk* argue that trajectory/flourishing work deserves comparable attention. (*introducing-better-futures*, *beyond-existential-risk*, *supplement-the-basic-case-for-better-futures*, *no-easy-eutopia*.)

**8. Expected value reasoning under uncertainty is the appropriate decision framework.**
Across technical, governance, and ethical papers, Forethought consistently applies EV decomposition, often multiplicative (S×F, product-of-factors). (*supplement-the-basic-case-for-better-futures*, *beyond-existential-risk*, *is-flourishing-predetermined*, *short-timelines-arent-obviously-higher-leverage*.)

---

## Tier 2: Strong working assumptions

### AI trajectory and intelligence explosion

- **A software intelligence explosion is plausible (~30–60% conditional on AI R&D automation with fixed hardware), and a full-stack IE is even more likely (~90%).** The key parameter is r (returns to software R&D), estimated at ~1–4. (*will-ai-r-and-d-automation-cause-a-software-intelligence-explosion*, *once-ai-research-is-automated-will-ai-progress-accelerate*, *three-types-of-intelligence-explosion*.)
- **Medium-speed takeoff (months to a small number of years) is the best-guess scenario.** Very fast takeoff (days/weeks) is judged very unlikely given training time lags; very slow takeoff underestimates feedback loops. (*a-global-convention-to-govern-the-intelligence-explosion*, *three-types-of-intelligence-explosion*, *how-quick-and-big-would-a-software-intelligence-explosion-be*.)
- **The IE decomposes into three feedback loops (software, chip technology, chip production) with distinct dynamics.** Software is fastest, chip production slowest. (*three-types-of-intelligence-explosion*, *how-suddenly-will-ai-accelerate-ai-progress*.)
- **Compute bottlenecks constrain but do not prevent a software IE.** The effective elasticity of substitution between cognitive labor and compute in AI R&D is higher than standard economic estimates suggest. (*will-compute-bottlenecks-prevent-a-software-intelligence-explosion*.)
- **An industrial explosion (self-replicating physical production) follows the intelligence explosion.** Initial robot doubling times ~1 year, potentially accelerating to days/weeks. (*the-industrial-explosion*, *preparing-for-the-intelligence-explosion*.)
- **Scaling laws reveal surprisingly poor resource-efficiency** (exponents of 11–20), though temporal progress remains impressive due to exponentially growing resource investment. (*the-scaling-paradox*.)

### Lock-in, path dependence, and the value of the future

- **The "wash-out" objection to longtermist trajectory work fails.** Multiple mechanisms (AGI-enforced institutions, immortality, designed beings, defense-dominant space) create persistent path dependence. (*persistent-path-dependence*, *agi-and-lock-in*.)
- **Eutopia is a narrow target—even "common-sense utopia" falls far short of the best feasible future.** The value of the future is well-modeled as multiplicative across independent factors, producing a highly right-skewed distribution. (*no-easy-eutopia*, *convergence-and-compromise*.)
- **Dichotomy (futures clustering at near-zero or near-max value) is false.** Resource division, moral uncertainty, and inadequacy of convergence/bounded-value arguments undermine it. (*beyond-existential-risk*.)
- **Space governance is a near-term priority** because first-mover advantage in extrasolar resources could be permanent. (*how-to-make-the-future-better*, *preparing-for-the-intelligence-explosion*, *space-debris-and-launch-denial*.)

### Governance and coordination

- **The US is the AI frontrunner, and any feasible international governance must be US-compatible.** This is treated as a binding political constraint across the international project series. (*what-an-international-project-to-develop-agi-should-look-like*, *intelsat-as-a-model-for-international-agi-governance*, *a-global-convention-to-govern-the-intelligence-explosion*.)
- **Intelsat is a better historical analogy for AGI governance than the Manhattan Project or CERN**, because AGI is primarily a commercial, general-purpose technology. (*intelsat-as-a-model-for-international-agi-governance*, *the-international-agi-project-series*.)
- **Interim/temporary institutional arrangements with built-in renegotiation are preferable to permanent designs**, given deep uncertainty about post-AGI conditions. (*agi-and-world-government*, *how-to-make-the-future-better*, *intelsat-as-a-model-for-international-agi-governance*.)
- **Differential technology development—selectively accelerating beneficial capabilities relative to dangerous ones—is a high-leverage strategy.** Restricting AI R&D automation while promoting "wisdom" capabilities (forecasting, deliberation) could improve the balance. (*international-ai-projects-and-differential-ai-development*, *ai-tools-for-existential-security*, *the-first-type-of-transformative-ai*.)
- **Political feasibility is a binding constraint on governance proposal design.** Forethought repeatedly rejects more ambitious alternatives as infeasible. (*a-global-convention-to-govern-the-intelligence-explosion*, *what-an-international-project-to-develop-agi-should-look-like*.)

### Moral philosophy and better futures

- **Improving the quality of surviving futures ("flourishing") deserves comparable priority to reducing extinction risk ("surviving").** The scale advantage of flourishing work is potentially enormous and it is more neglected. (*introducing-better-futures*, *supplement-the-basic-case-for-better-futures*, *beyond-existential-risk*.)
- **Moral convergence is unlikely to produce near-best futures by default.** Current moral agreement overstates deep convergence; reflection modeled as random walks suggests divergence. (*convergence-and-compromise*.)
- **Moral trade and compromise are the most plausible path to a mostly-great future**, not universal convergence on the correct moral view. (*convergence-and-compromise*.)
- **AI should ideally be morally uncertain and motivated to promote the good de dicto**, rather than aligned with any specific moral view. (*how-to-make-the-future-better*.)

### Epistemics and decision quality

- **Improving collective epistemics is a high-leverage, underinvested intervention for navigating AI risks.** Changing incentive landscapes (shifting to high-honesty equilibria) matters more than individual empowerment. (*design-sketches-collective-epistemics*, *whats-important-in-ai-for-epistemics*, *project-ideas-epistemics*.)
- **There is a "veil of ignorance" window for building epistemic norms** before AI's conclusions on controversial topics are known. (*project-ideas-epistemics*, *whats-important-in-ai-for-epistemics*.)

---

## Tier 3: Active research questions

These are areas where Forethought papers take different positions, explicitly flag uncertainty, or are actively investigating. **Novel critiques here are especially valuable.**

- **Whether centralising Western AGI development would be net positive or negative.** *should-there-be-just-one-western-agi-project* concludes "probably net bad" due to power concentration but with deep uncertainty; other papers (*what-an-international-project-to-develop-agi-should-look-like*) propose centralised international projects.

- **How likely and how fast a software-only intelligence explosion would be.** Estimates range from ~30% to ~60% for SIE occurring; the amount of progress compressed varies from 3 years to 10+ years in <1 year. (*will-ai-r-and-d-automation-cause-a-software-intelligence-explosion*, *how-quick-and-big-would-a-software-intelligence-explosion-be*.)

- **Whether AI takeover or human takeover is worse in expectation.** *human-takeover-might-be-worse-than-ai-takeover* argues AI may better live up to human moral standards, while *ai-enabled-coups* treats human seizure of power as a catastrophe. The relative weighting is unresolved.

- **The relative priority of survival vs. flourishing work.** The Better Futures series argues for comparable attention, but *is-flourishing-predetermined* finds a ~2× "predetermination penalty" favouring survival work, and *short-timelines-arent-obviously-higher-leverage* suggests different optimal strategies by timeline.

- **The elasticity of substitution between cognitive labour and compute in AI R&D.** *will-compute-bottlenecks-prevent-a-software-intelligence-explosion* argues ρ is much higher than manufacturing estimates; the companion SIE paper assumes Cobb-Douglas (ρ=0). This parameter is decision-relevant and unresolved.

- **Whether the current LLM paradigm is unusually safe.** *short-timelines-arent-obviously-higher-leverage* suggests pretraining on human text gives AI deep value understanding; *human-takeover-might-be-worse-than-ai-takeover* warns that the shift to agentic training could erode this.

- **How to handle digital minds' moral status.** *project-ideas-sentience-and-rights-of-digital-minds* explores RSP-style commitments and AI compensation schemes; the team acknowledges deep uncertainty about consciousness, preferences, and appropriate policy responses.

- **Optimal governance structure: coalition of democracies vs. broader international participation.** *agi-and-world-government* favours a democratic coalition; *a-global-convention-to-govern-the-intelligence-explosion* wants a broader convention; the tension between feasibility and legitimacy is live.

- **Whether accelerating AGI development (e.g., via an international project with larger training runs) could be net positive.** *what-an-international-project-to-develop-agi-should-look-like* argues yes (buys control); other papers emphasise the value of slowing the intelligence explosion.

- **The extent to which "backup plans" for misaligned AI (shaping dispositions, acausal trade, non-spitefulness) are tractable.** *project-ideas-backup-plans-cooperative-ai* argues they are worth serious investment; the team acknowledges deep uncertainty about generalisability of fine-tuning.

---

## Methodological orientation

Forethought's characteristic methodological moves include:

- **Back-of-envelope quantitative reasoning** (Fermi estimates, OOM calculations, Monte Carlo over uncertain parameters) preferred over formal proofs or no quantification at all.
- **Semi-endogenous growth theory** as the default framework for modelling technological feedback loops.
- **Scenario-conditional analysis**: conditioning on specific futures (intelligence explosion, medium-speed takeoff, US lead) and tracing implications, rather than assigning unconditional probabilities.
- **Historical institutional analogy** as a primary evidence source for governance design (Intelsat, UN Charter, NPT, etc.), with explicit attention to both analogies and disanalogies.
- **Political feasibility as a binding constraint**, repeatedly used to reject normatively superior but politically unrealistic proposals.
- **Systematic objection-and-response structure**, often with the strongest objections addressed first.
- **Iterative public disclosure**: publishing "rough research notes" for feedback, signalling epistemic humility and inviting engagement.
- **Expected value decomposition** into multiplicative factors (S×F, product of independent challenges) as a recurring analytical framework.
- **Moral pluralism under uncertainty**: drawing conclusions robust across multiple ethical frameworks rather than committing to one.

---

## Intellectual context

Forethought works within the **longtermist effective altruism** tradition, drawing heavily on Nick Bostrom, Toby Ord, Will MacAskill, Carl Shulman, and Holden Karnofsky. They extend this tradition in several directions: emphasising trajectory/flourishing over pure x-risk reduction, taking governance and institutional design as seriously as technical alignment, and producing concrete policy proposals grounded in historical precedent. Key external influences include semi-endogenous growth theory (Jones), compute-centric AI forecasting (Epoch AI/Davidson), the scaling laws literature, Rawlsian political philosophy, and international relations scholarship. They are notably more governance-focused and policy-oriented than most technical AI safety groups, and more willing to engage with political feasibility constraints than most academic longtermists.