# OpenAI Deep Research: q6-novelty-detection

**Model:** o3-deep-research-2025-06-26
**Generated:** 2026-01-20T14:25:35.130Z
**Response ID:** resp_0ac32fbad6b060ff00696f8f0a76288194978422c7a9034a5d

---

# Detecting Genuine Novelty vs. Sophisticated Recombination in LLM Outputs

## The Challenge of Novelty vs. Recombination

Large language models (LLMs) are *extremely* good at generating text that **sounds** insightful – often by remixing and paraphrasing the vast knowledge in their training data. True **novelty**, however, is harder to come by. By design, an autoregressive LLM pulls from learned patterns; it **follows the distribution of existing text** rather than leaping into the unknown ([www.researchgate.net](https://www.researchgate.net/publication/386218728_On_the_creativity_of_large_language_models#:~:text=to%20introduce%20other%20elements,autoregressive%20nature%20of)). In creativity terms, most LLM outputs show **combinatorial creativity** – unfamiliar combinations of familiar ideas – rather than fundamentally original insights ([www.researchgate.net](https://www.researchgate.net/publication/386218728_On_the_creativity_of_large_language_models#:~:text=tially%20trained%20to%20follow%20the,Shana)). This means an AI-generated critique can *feel* new, while actually just rehashing well-known arguments in fancy language.

Determining whether a critique offers a **genuine new insight** or just a sophisticated restatement is challenging even for humans. It requires knowing the state of prior discussions and spotting when the AI goes beyond them. LLMs themselves do not have an explicit index of “what the researcher already knows,” so they might propose an idea that was *novel to the model* but not to the field. In domains like philosophy and longtermist economics, many arguments have been debated extensively. An LLM without careful guidance might confidently present a common argument as if it were novel. Our goal is to have a grader discern the difference.

## Techniques for Novelty Detection in Text

**Textual novelty detection** is an established concept in NLP – often framed as checking if a candidate text contains information **not entailed by** a reference corpus ([direct.mit.edu](https://direct.mit.edu/coli/article/48/1/77/108847/Novelty-Detection-A-Perspective-from-Natural#:~:text=novelty%20with%20entailment,redundant)). In other words, if the critique’s content can be found (or logically inferred) in what’s already known, then it’s not novel. Classic approaches from information retrieval compute similarity to known documents or use entailment models: a critique is flagged *non-novel* if it is essentially a paraphrase of existing content ([direct.mit.edu](https://direct.mit.edu/coli/article/48/1/77/108847/Novelty-Detection-A-Perspective-from-Natural#:~:text=novelty%20with%20entailment,redundant)). Modern research extends this idea with advanced tools:

- **Retrieval + Comparison (Entailment)**: A practical approach is to retrieve related texts from a knowledge base (e.g. prior papers, known critiques) and have the model or an algorithm compare the critique against them. If the critique’s key points appear in the retrieved literature, it likely isn’t original. Recent work by Lin et al. (2024) followed this approach by introducing **RAG-Novelty**, a retrieval-augmented method that simulates how human reviewers assess novelty ([www.emergentmind.com](https://www.emergentmind.com/papers/2409.16605#:~:text=,LLM%20performance%20despite%20metadata%20biases)). Given a new paper’s abstract, their system pulls up similar prior abstracts and then uses an LLM to judge whether the new one contains ideas absent in the old. This method **outperformed baseline LLMs** on a benchmark of 15,000 paper pairs, highlighting that supplying context helps the model make better novelty judgments ([www.emergentmind.com](https://www.emergentmind.com/papers/2409.16605#:~:text=RAG)).

- **Structured “Review” of Novelty**: Going further, Afzal et al. (2025) developed a structured pipeline for automated novelty evaluation in peer review. Their system explicitly **extracts the key claims** from a submission, **retrieves relevant prior work**, and then performs a **structured comparison** – effectively emulating an expert reviewer ([arxiv.org](https://arxiv.org/abs/2508.10795#:~:text=Novelty%20assessment%20is%20a%20central,annotated%20reviewer%20novelty%20assessments%2C%20the)). By verifying each claim against the literature (e.g. “Has anyone made this claim before? If so, what do those sources say?”), the system arrives at an evidence-backed novelty assessment. This multi-step process yielded **75% agreement with human reviewers** on whether a paper’s contribution was novel or not ([arxiv.org](https://arxiv.org/abs/2508.10795#:~:text=182%20ICLR%202025%20submissions%20with,and%20code%20are%20made%20available)). It also produced detailed, literature-aware feedback, demonstrating that a step-by-step, evidence-based approach can closely model expert reasoning.

- **Semantic Embedding Similarity**: A lighter-weight technique uses vector embeddings to gauge novelty. We can represent the critique and a large set of existing texts (the original paper, related articles, known arguments) in an embedding space, and then measure distances. If the critique is **very similar** to something in this space, that’s a sign of low novelty. In NLP research, cosine similarity of embeddings and nearest-neighbor searches have been used to detect redundant or derivative text ([direct.mit.edu](https://direct.mit.edu/coli/article/48/1/77/108847/Novelty-Detection-A-Perspective-from-Natural#:~:text=document%2C%20a_,The%20RDV%20becomes)). This can catch cases where an LLM-produced critique is basically a paraphrase of the source material or common knowledge. However, **embedding similarity is an imperfect proxy** for novelty. There’s often a gap between *textual* similarity and *conceptual* originality ([huggingface.co](https://huggingface.co/papers/2505.24615#:~:text=crucial%20and%20challenging%20in%20academia,to%20train%20a%20%208)). An idea might be formulated in new words yet still not novel (yielding low similarity but no real insight), or conversely an inventive critique might use similar jargon as prior work (yielding higher similarity despite being novel in conclusion). In fact, one study found that although n-gram novelty (literally using new combinations of words) correlates somewhat with creativity, **over 90% of highly novel wordings were not judged creative by humans** ([arxiv.org](https://arxiv.org/abs/2509.22641#:~:text=relationship%20between%20this%20notion%20of,produce%20creative%20expressions%20than%20humans)). In short, **novel wording ≠ novel idea**. Thus, embedding methods can flag obvious cases of copying or rehashing, but **by themselves they miss the nuance**.

- **Idea-Level or Conceptual Matching**: To improve on raw embeddings, researchers are working on capturing *conceptual* similarity. For example, Liu et al. (2025) note that simply retrieving and cross-checking text isn’t enough, due to the “gap between textual similarity and idea conception” ([huggingface.co](https://huggingface.co/papers/2505.24615#:~:text=crucial%20and%20challenging%20in%20academia,to%20train%20a%20%208)). They harnessed LLMs to distill **idea-level representations**: instead of comparing surface text, the model generates a summary of the idea in a normalized form, and a lightweight retriever is trained on these summaries to find conceptually similar work ([huggingface.co](https://huggingface.co/papers/2505.24615#:~:text=associated%20with%20two%20new%20datasets,10FB)). This helps identify when an AI’s critique might be basically a known idea in different words. Such concept-aware retrieval can more reliably detect lack of novelty than keyword or embedding matching alone, according to their results.

In practice, a combination of these techniques may be ideal. For our use case (critiques in longtermist philosophy, AI governance, etc.), we could: *a)* maintain a knowledge base of existing arguments (from the paper itself and related literature), *b)* use embeddings or keyword search to fetch the most similar existing points when a new critique is generated, and *c)* ask the LLM to reason about whether the critique offers something not found in those references. This essentially turns the grader into a research assistant doing a mini-literature-review for each critique.

## LLM Self-Assessment and Comparative Evaluation

One might hope that an LLM could **assess the novelty of its own output**, given that it has encoded so much knowledge. In practice, this is unreliable. Studies have directly examined this: for example, Si et al. (2024) had GPT-4 act as a scientist generating research ideas and then evaluate them. The **LLM’s self-evaluation of novelty did not align well with human judgments**, and the model also showed a lack of diversity in generation ([arxiv.org](https://arxiv.org/abs/2409.04109#:~:text=statistically%20significant%20conclusion%20on%20current,and%20feasibility%20judgements%20result%20in)). This isn’t surprising – an LLM has no mental “cache” of everything it just regurgitated vs. truly invented. It also doesn’t *truly* know what a given human audience would consider novel. Often, models tend to **overestimate the originality** of their answers because if an idea wasn’t explicitly in their prompt or recent context, it “feels” new to them.

Given these limitations, a common strategy is to use **LLMs as separate judges** (LLM-as-a-judge frameworks) rather than trusting an answer to judge itself. For example, after generating a batch of critiques with Model A, we might ask Model B (or the same model in a new session) to rate each critique’s novelty. This was essentially the approach in many recent works that use LLMs to evaluate other LLM outputs. These LLM judges can provide structured feedback and scores, but we have to be careful: **LLM evaluators have their own biases and failure modes**.

Research in 2024 has catalogued several such biases. Chen et al. found that LLM judges are **vulnerable to superficial cues** – e.g. if a response includes a fake but authoritative-looking citation, or is written with extra polish, it might receive an unjustified higher novelty/quality score ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=,2024%20%2C%20%209)). In other words, an AI grader can be **fooled by “style over substance.”** Longer or more eloquent critiques might be rated as more novel due to a *verbosity bias*, even if they’re actually saying nothing new. There’s also a **position bias** (the order in which options are presented can sway judgments) and even a *“bandwagon”* effect where the model aligns with a stated consensus if it’s hinted ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=surrogate%20models%E2%80%94can%20dramatically%20inflate%20scores,2024)). Alarmingly, simple adversarial tricks have been shown to inflate LLM judge scores: one study demonstrated that inserting certain innocuous-looking trigger phrases or formatting tweaks could systematically boost the evaluated score of any response ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=misplacing%20credit%20for%20apparent%20novelty,This%20manipulation)). This means an unscrupulous critique-generator (or just a quirk of phrasing) could score higher not by being truly original, but by tripping the judge’s statistical wires.

**How can we combat these issues?** Recent work suggests a few best practices:

- **Relative (Pairwise) Evaluation:** Asking “Which of these two critiques is more novel?” is easier for an LLM judge than assigning an absolute novelty score in a vacuum. LLMs are quite good at **pairwise comparison**, often mirroring human rankings even when their absolute ratings drift ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=,ended%20creative%20writing)). In fact, even if an LLM’s scoring calibration is off (one model might consistently overshoot or undershoot human scores), its ability to sort outputs by novelty can remain reliable ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=,ended%20creative%20writing)). The SchNovel study also found that **pairwise methods outperformed pointwise methods** for detecting novelty in papers ([www.emergentmind.com](https://www.emergentmind.com/papers/2409.16605#:~:text=The%20paper%20thoroughly%20investigates%20several,of%20sampling%20diverse%20reasoning%20paths)). This suggests we could have the grader rank a set of generated critiques from most to least novel, or do tournament-style comparisons, instead of trusting a 1–10 score for each critique independently.

- **Chain-of-Thought and Justifications:** We can prompt the LLM judge to **explain its reasoning** – e.g. list what prior knowledge a critique draws on and what, if anything, is truly novel in it. By making the reasoning step explicit, we reduce the chance of snap judgments based on fluency. Research indicates that LLMs with a *self-rationalizing* approach produce more trustworthy evaluations. For example, humans preferred the detailed **rationales from an LLM judge with chain-of-thought** explanations over a baseline that just gave a bald score ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=2024%20%29.%20,16%20Trivedi%20et)). In our case, we might prompt the grader with something like: *“Critique X: [text]. **Analysis:** Does this critique bring up any point not found in the original paper or standard discussions in the field? Provide evidence from the paper or common knowledge for any claims.”* This forces the model to back up novelty judgments with specifics, making it both more interpretable and likely more accurate. 

- **Separating Generation and Evaluation:** It’s generally wise to isolate the critique generation from the critique evaluation, to avoid self-validation. If we use the same model, we ensure the **grader prompt has no knowledge of who wrote the critique or how**. Even better, we could use a different model (e.g., use GPT-4 to generate critiques, and Claude or another instance of GPT-4 to grade). This avoids any hidden “loyalty” where the model might go easy on its own answer. In technical terms, this helps prevent *self-enhancement bias* where an LLM might inherently favor its own phrasing or reasoning chain ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=%2A%20Self,This%20manipulation)).

- **Calibration and Perturbation:** Some advanced techniques treat the LLM as a **black-box evaluator to stress-test**. For instance, one can slightly perturb a critique (shuffle sentence order, remove an embellishment) and see if the novelty score changes dramatically ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=perturbed%20responses%2C%20isolating%20model%20preferences,3)) ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=,2024%20%2C%20%209)). A robust evaluator should give *similar* scores if the core idea didn’t change. Large swings mean the model was focusing on the wrong things (e.g., formatting). Researchers have even defined metrics like an Attack Success Rate: how often a known non-novel response can be tweaked (with irrelevant changes) to fool the judge into a high novelty rating ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=perturbed%20responses%2C%20isolating%20model%20preferences,3)). In practice, we can incorporate a mini-perturbation test for top-scoring critiques: rephrase them slightly or strip out citations, and see if the grader still deems them novel. If not, that’s a red flag that the original score was not robust.

In summary, LLM judges **can** help distinguish novel vs. derivative critiques, especially if used in a comparative mode with clear reasoning prompts. But we must be aware of their blind spots and use prompts and process design to counteract them. Absolute “novelty scores” from an LLM should be taken with a grain of salt – it’s better to see how it reasons and how consistent those judgments are under slight changes.

## The Role of Domain Knowledge

Novelty is **relative to what’s already known** in a domain. An insight that is groundbreaking in one context might be old news in another. Thus, assessing novelty in AI governance or longtermist philosophy demands a good grasp of those fields. If our grader LLM isn’t familiar with, say, the standard arguments about AGI timelines or the common analogies used in digital minds debates, it will struggle. It might misclassify a well-worn analogy as “creative” simply because it personally hasn’t seen it during training, or label a genuinely original thought as “not novel” because it superficially resembles existing ideas.

Giving the LLM **access to domain knowledge** can substantially improve novelty detection. The Afzal et al. approach above is essentially injecting domain knowledge via retrieval – the model doesn’t have to *remember* all relevant prior work if we supply it with a summary of prior work to compare against. More generally, we can employ a **Retrieval-Augmented Generation (RAG)** paradigm for our grader: when evaluating a critique, the system first *searches* a curated knowledge base of papers, blog posts, or even a summary of the original research draft, to gather any content related to the critique’s claims. This serves as a proxy for “what the researcher already knows.” The LLM then judges novelty with awareness of those references. If the critique’s main point appears in the retrieved text (or can be logically inferred from it), the LLM should conclude it isn’t offering a new insight ([direct.mit.edu](https://direct.mit.edu/coli/article/48/1/77/108847/Novelty-Detection-A-Perspective-from-Natural#:~:text=novelty%20with%20entailment,redundant)). On the other hand, if the critique raises a point and even a diligent search finds nothing similar, that’s strong evidence of novelty (though the point could still be wrong or speculative – novelty isn’t always positive!).

Crucially, domain knowledge often means the LLM must perform **multi-hop reasoning**. A novel combination of ideas might pull from different areas. Ghosal et al. (2022) note that determining a statement’s novelty may require **multiple premises, world knowledge, and reasoning** – a single source often isn’t enough ([direct.mit.edu](https://direct.mit.edu/coli/article/48/1/77/108847/Novelty-Detection-A-Perspective-from-Natural#:~:text=match%20at%20L401%20,same%20in%20the%20subsequent%20section)). For example, a critique about “AI labor economics post-AGI” might reference both economic theory and AI technical trends. The grader needs to connect dots across those knowledge sets to realize the insight (if any) is novel. Ensuring the model has access to broad context (or using a very knowledgeable base model) is key.

If the domain is truly niche or the knowledge is mostly in the **researcher’s head**, we might consider ways to *extract that knowledge*. One idea: before generating critiques, have the researcher or an assistant feed the system a “briefing” – e.g., *“Here are 5 common objections people raise on this topic…”* or *“Key assumptions and arguments in this field include X, Y, Z.”* This could even be done by having an LLM read relevant background materials (the original paper, influential articles) and then using that same session or context when asking it to judge a critique. Essentially, we bias the model to consider domain context heavily, so it’s less likely to declare something novel when it’s actually a known issue in the field.

Finally, we should note that domain specialization can cut both ways. A highly specialized model (or one given lots of context) will know what is established, but it might also be **more critical** – potentially calling *everything* “not novel” because it can find some tangential connection to prior work. Balancing this requires careful prompt tuning, possibly telling the model to focus on *significant* differences or to ignore trivial similarities. The model should weigh whether the critique’s idea is a straightforward extension of known ideas (probably not novel) or a **surprising departure** even if it uses known concepts in the setup. Achieving this nuance is part of prompt engineering and perhaps few-shot examples: we can show the grader examples like, *“Critique: ‘This is just like Bostrom’s argument about X.’ → Not novel.”* versus *“Critique: ‘Introduces a new analogy linking AI to a previously unrelated domain.’ → Novel.”*.

## Human-in-the-Loop and Current Limitations

Despite progress, **novelty detection is not a solved problem** – neither for AI nor humans. Even expert reviewers sometimes disagree on what counts as a “novel” contribution, since it can be subjective. Our LLM grader can significantly streamline the evaluation of critiques, but it shouldn’t be the sole arbiter, at least not initially. Here’s how we envision the human–AI collaboration and the known failure modes to watch out for:

- **Model Failure Modes:** We’ve touched on many of these. To summarize the key ones: **false positives** (the grader thinks a critique is groundbreaking when it’s actually commonplace) often arise from being fooled by phrasing, length, or lack of knowledge. **False negatives** (the grader overlooks a novel idea) can happen if the idea is subtle or if the model finds a vaguely related prior point and jumps to the conclusion “been done before.” The structured approaches help mitigate this by requiring evidence for claims. But an LLM might still err by citing something that isn’t truly the same idea (e.g. misconstruing a reference). There’s also the risk of **hallucinations**: an LLM judge might “recall” a non-existent paper that supposedly already made the critique’s argument. If we see the grader citing sources, we must verify them (perhaps by using an actual search engine).

- **Humans as Final Judges:** In our use case, the **researcher’s intuition and knowledge** remains extremely valuable. The grader should surface and highlight the most promising (or novel-seeming) critiques, but a human should review those top candidates. If the system claims a critique is novel, the researcher can verify: *Is it truly something I hadn’t considered? Is it valid?* This check is important because novelty alone isn’t enough – a critique could be novel but based on a misunderstanding. Human judgment is needed to filter *useful* novelty from the merely *unexpected*. Conversely, if the grader dismisses something as not novel, a human might occasionally catch a mistake (“Actually, this critique’s angle *is* different from the known ones.”). Having a human in the loop as a backstop will increase reliability.

- **Transparency and Trust:** One advantage of using an LLM (over a black-box ML classifier) is that we can ask it to **explain why** it labeled a critique as novel or not. This transparency is crucial for trust. If it gives a false positive, we’ll likely spot the spurious reasoning in its explanation (e.g., “It cited Smith 2020 as already making this point” and the researcher knows Smith 2020 said no such thing). We should design the grader to always output a brief rationale with references for its decision. This keeps the human in control and allows **course-correction** – essentially fine-tuning the grader’s understanding over time.

- **Small Sample Validation:** We only have ~20 human-rated critiques as a ground truth sample, which is very limited. This means any statistical evaluation of the grader (accuracy, etc.) will be rough. Instead of solely looking at aggregate metrics, we might do a **case-by-case validation**: see if the grader’s reasoning on those 20 examples matches the human rater’s reasoning. With so few samples, a qualitative review is as important as a quantitative one. We can use those examples to iteratively improve prompts. For instance, if the grader thought something was novel but the human didn’t, we examine why – maybe the grader wasn’t aware of a certain known argument. We can then adjust by adding that knowledge to the context for future judgments.

It’s worth noting that **human reviewers aren’t perfect either** – they get tired, they can be biased by how well-written a critique is (just like LLMs!). One fascinating finding in the 2025 study was that the LLM-assisted method for peer review actually *improved consistency* over ad-hoc human reviews ([arxiv.org](https://arxiv.org/abs/2508.10795#:~:text=182%20ICLR%202025%20submissions%20with,and%20code%20are%20made%20available)). By structuring the task and providing literature evidence, the AI approach reduced the chance that a human might overlook an existing work or apply an inconsistent standard. This suggests our grader could not only save time but also *prompt better human decision-making*. The researcher, armed with the grader’s evidence and rankings, might think more critically about each critique’s merit.

## Practical Recommendations for Our Use Case

To sum up the state-of-the-art insights and how to apply them to Forethought’s AI critique grading:

- **Leverage Retrieval for Context:** Plug in a searchable archive of relevant literature (papers, prior critiques, Wikipedia entries for key concepts). For each critique, automatically retrieve a few likely related passages. Have the grader consider these when evaluating novelty. This grounds its judgment in actual domain knowledge ([www.emergentmind.com](https://www.emergentmind.com/papers/2409.16605#:~:text=The%20RAG,computer%20science%20and%20quantitative%20finance)).

- **Ask for Evidence and Reasoning:** Design the grading prompt to require a justification: e.g., *“Explain whether this critique is novel. Cite any sources or known arguments that overlap with it.”* A chain-of-thought answer that lists overlaps or explicitly says “I could not find this idea in the provided sources” will make the verdict more trustworthy ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=2024%20%29.%20,16%20Trivedi%20et)).

- **Use Pairwise Comparisons or Ranking:** If evaluating a batch of critiques, get the model to rank them from most novel to least, or compare them in pairs (“Is critique A or B more original?”). This harnesses the model’s strength in relative judgments and yields a more stable assessment of novelty differences ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=,ended%20creative%20writing)).

- **Beware of Superficial Novelty:** Put in checks for style bias. Encourage the model (via instructions or few-shot examples) *not* to be swayed by jargon, length, or format. We can even test the grader by introducing decoy critiques that are verbose nonsense or that add fake citations, to ensure it doesn’t consistently rate those higher ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=,2024%20%2C%20%209)). If it does, refine the prompt (e.g., “Ignore stylistic flourishes and focus on content.”).

- **Integrate Human Review for Top Picks:** Rather than fully automating, use the grader as a triage tool. For example, if 50 critiques are generated, the grader might mark 5-10 as especially novel. A human researcher then reviews those in depth. This way, >75% of the truly valuable critiques (our target) hopefully appear in that top set, given the grader’s filtering, and the researcher can confirm them. The human can also give feedback on any false positives/negatives to further tune the system.

- **Continuous Calibration:** With such a small initial dataset, treat each deployment as an opportunity to learn. If the grader’s selection missed something good, or included something off-base, update the approach. This could mean adding a new example to a prompt or even a quick one-off rule (e.g., if it keeps calling a well-known argument novel, explicitly remind it of that argument in the context). Over time, the grader will align better with the researchers’ notion of novelty.

- **Keep an Eye on Validity:** Novelty is just one axis of critique quality – a novel *but incorrect or irrelevant* critique isn’t useful. Our grader should ideally incorporate a minimal check for relevance/validity. In practice, this may be a second stage where critiques that pass the novelty filter are then checked for coherence or feasibility (possibly by another LLM prompt). Human reviewers will anyway filter out the nonsensical ones, but it’s good to flag that *new ideas should still make sense*. Some of the literature above notes a trade-off: LLM ideas might be novel but less feasible ([arxiv.org](https://arxiv.org/abs/2409.04109#:~:text=statistically%20significant%20conclusion%20on%20current,and%20feasibility%20judgements%20result%20in)). We should be mindful of that balance.

In conclusion, **detecting genuine novelty in LLM outputs is partly an AI challenge and partly a knowledge problem**. State-of-the-art methods use LLMs augmented with information and carefully structured prompts to judge originality. They work best in tandem with human expertise. By combining retrieval of prior knowledge, forcing transparent reasoning, utilizing comparative evaluations, and maintaining human oversight, we can significantly boost the reliability of our AI critique grader. It won’t be infallible, but it can dramatically reduce the time researchers spend sifting through mundane or known critiques, letting them focus on the truly insightful 5-10% – which is exactly the outcome we need for Forethought’s mission.  ([arxiv.org](https://arxiv.org/abs/2508.10795#:~:text=182%20ICLR%202025%20submissions%20with,and%20code%20are%20made%20available)) ([www.emergentmind.com](https://www.emergentmind.com/topics/llm-as-a-judge-novelty-scores#:~:text=Human%20alignment%20remains%20the%20gold,corrected%20statistics%20and%20correlation%20analyses))

