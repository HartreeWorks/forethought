# OpenAI Deep Research: q5-adversarial-prompts

**Model:** o3-deep-research-2025-06-26
**Generated:** 2026-01-20T14:24:52.240Z
**Response ID:** resp_08663740334ff97300696f8f08ce88819698dc6ca520c28a42

---

# Devil’s Advocate Prompting and Critique Quality

**Devil’s advocate** instructions – explicitly telling the model to argue against a thesis – can indeed sharpen the quality of critiques in many cases. By default, large language models often give agreeable or mildly critical responses. Pushing the model into an adversarial role helps overcome this bias, leading to more thorough fault-finding. For example, an evaluation framework called **DEBATE** (2024) had one AI agent adopt a *Devil’s Advocate* stance to critique others’ answers, and it significantly outperformed previous single-agent evaluators on text quality benchmarks ([www.emergentmind.com](https://www.emergentmind.com/papers/2405.09935#:~:text=DEBATE%2C%20an%20NLG%20evaluation%20framework,influence%20the%20performance%20of%20evaluators)). Similarly, multi-agent “debate” approaches (having models argue contrasting viewpoints) have been shown to **improve reasoning and factual accuracy**. In one study, letting multiple GPT-based agents critique each other’s answers reduced logical errors and hallucinations, yielding more correct solutions than a single model working alone ([openreview.net](https://openreview.net/forum?id=QAwaaLJNCk#:~:text=indicate%20that%20this%20approach%20significantly,in%20language%20generation%20and%20understanding)). This suggests adversarial prompting can surface issues a neutral prompt might miss.

However, the benefits come with caveats. **Explicitly adversarial prompts** (“find all the flaws in this argument”) will make an LLM enumerate weaknesses, but not all of those weaknesses will be valid or important. **Overdoing the devil’s advocate** can push the model to conjure criticisms just to comply, sometimes nitpicking minor points or inventing flaws. Empirically, there are cases where naive critique prompts did *not* improve output quality – or even made it worse ([arxiv.org](https://arxiv.org/abs/2501.14492#:~:text=this%20benchmark%20using%20eight%20challenging,https%3A%2F%2Fgithub.com%2Ftangzhy%2FRealCritic)). For instance, a 2025 benchmark called *RealCritic* found that off-the-shelf models often underperformed when asked to self-critique or critique others, unless they had specialized reasoning training ([arxiv.org](https://arxiv.org/abs/2501.14492#:~:text=this%20benchmark%20using%20eight%20challenging,https%3A%2F%2Fgithub.com%2Ftangzhy%2FRealCritic)). The upshot is that *how* you prompt adversarially matters: simply demanding negativity can produce a greater quantity of critique, but not necessarily quality. 

## Steelman-Then-Critique Strategy

A promising technique is the **“steelman then critique”** approach. *Steelmanning* means the model first articulates the strongest possible version of the argument (the version a smart defender would make), before launching into the critique. This serves two purposes: it ensures the model fully understands the argument, and it prevents attacking a strawman. This strategy is widely recommended in human reasoning and debate – before attacking a position, acknowledge its best form ([www.alexlibre.com](https://www.alexlibre.com/p/if-youre-not-yet-using-ai-to-get#:~:text=,do%20you%20find%20it%20convincing)). In practice, prompting an LLM to *first summarize or reinforce an argument’s core merits, then find flaws* often yields more substantive and fair critiques. By restating the argument’s strongest points, the model is less likely to misrepresent it, and its subsequent criticisms target the real crux of the issue. 

Anecdotally, Forethought researchers have found that when GPT-4 is instructed to *“consider the strongest case for X, and now identify where that case might still fail,”* the critiques tend to be deeper. This approach reduces the chance of the model taking cheap shots at a weakened version of the argument. While we don’t have a formal paper on “steelman prompting” per se, it aligns with the idea of forcing the model into a more rigorous analytic mindset. By **bolstering then breaking** the argument, the model is essentially performing an internal debate – first arguing for, then against – which often exposes subtler logical gaps or hidden assumptions. It’s a qualitative improvement that resonates with best practices in philosophical discourse. As one writing guide puts it: *“Before attacking a position, ask: what’s the strongest possible version of this argument? ... Then address that.”* ([www.alexlibre.com](https://www.alexlibre.com/p/if-youre-not-yet-using-ai-to-get#:~:text=,do%20you%20find%20it%20convincing)). In short, *steelman-then-critique* prompts are likely to improve relevance and accuracy of critiques, by ensuring the model isn’t arguing with a caricature of the original idea.

## Adversarial Framing vs. Neutral Evaluation

How you frame the prompt – “find flaws” versus “give a balanced evaluation” – strongly influences the output. An **adversarial framing** (e.g. *“You are a skeptical peer reviewer, tasked with tearing this argument apart”*) will trigger the model to focus on negatives and potential failures. In contrast, a more neutral request (e.g. *“Evaluate the strengths and weaknesses of this argument”*) yields a mix of pros and cons. There’s evidence that the adversarial framing surfaces issues that a neutral tone might gloss over. For example, instructing a model to be a *“Devil’s Advocate”* in complex decision-making tasks led it to anticipate failure modes that a straightforward plan missed – improving success rates by ~3.5% in one study ([openreview.net](https://openreview.net/forum?id=4BpEfStZBD#:~:text=suitability%20and%20results%20of%20their,experimental%20results%20suggest%20that%20our)) ([openreview.net](https://openreview.net/forum?id=4BpEfStZBD#:~:text=introspection,needed%20to%20achieve%20a%20task)). This shows that a critical posture can make the model more vigilant in its reasoning.

That said, adversarial framing can also introduce **biases** in the critique. The model, eager to comply, might exaggerate problems or ignore positive aspects entirely. In human terms, it’s the difference between an argumentative skeptic and a fair-minded evaluator. Each has its place. For our use case (critiquing longtermist and AI governance research), a purely adversarial prompt might yield a long list of “gotchas” – some insightful, others off-base. A neutral or dual-sided prompt might produce a more measured analysis but risk overlooking certain flaws out of an overly balanced treatment. One possible compromise is a two-part prompt: first ask for a brief *impartial summary or assessment*, then explicitly ask for *counterarguments and weaknesses*. This way the model outputs a well-rounded view but still gives a dedicated section to critical analysis. 

Empirically, a few studies hint at the trade-off. Researchers have observed that when GPT-4 is told to **“just critique”**, it sometimes latches onto fringe or speculative criticisms (since it must say *something* negative). On the other hand, when asked to **“evaluate this argument’s validity,”** models sometimes hedge and provide only mild, generic comments. The *RealCritic* benchmark mentioned earlier actually found that without careful prompting or training, models in a self-critique setting might fail to identify issues they otherwise would in direct Q&A ([arxiv.org](https://arxiv.org/abs/2501.14492#:~:text=this%20benchmark%20using%20eight%20challenging,https%3A%2F%2Fgithub.com%2Ftangzhy%2FRealCritic)). This suggests that *framing* is critical: the prompt needs to encourage thorough fault-finding *without* completely discarding the context or truth of the original argument. In practice, we likely need to iterate on phrasing to find the sweet spot between an aggressive red-team and a neutral analyst.

## Red-Team Techniques in Prompting

**Red teaming** in AI usually means stress-testing a model or system to find vulnerabilities or flaws. We can borrow this concept for generating critiques: effectively, *"red-team"* the research draft with the model. Techniques from AI red-teaming that carry over include prompting the model to consider **extreme scenarios, corner cases, or adversarial counterarguments**. For example, you might prompt: *“Imagine you strongly oppose this paper’s thesis – from that viewpoint, what weaknesses or catastrophic failure points can you find?”* This pushes the model to think like an adversary. Another technique is **role-playing**: instruct the LLM to act as a specific persona with a mandate to criticize (e.g. a devil’s advocate ethicist, or a skeptical economist). Role prompts can elicit a more intense and focused critique because the model is mimicking an antagonist perspective deeply ([medium.com](https://medium.com/%40minh.tran2_11384/a-review-of-llm-adversarial-prompting-44364903bf30#:~:text=This%20is%20one%20of%20the,in%20restrictions)) ([medium.com](https://medium.com/%40minh.tran2_11384/a-review-of-llm-adversarial-prompting-44364903bf30#:~:text=Prefix%20Injection,source)).

Multi-agent frameworks are essentially automated red-teaming: one model generates an argument, another model attacks it. As noted, recent research demonstrates big gains from such setups. Yilun Du et al. (2024) had multiple LLMs debate each other’s answers and found it **significantly improved factual accuracy and reasoning** – even in cases where all models initially were wrong, the debate enabled them to converge on a correct answer through critique and revision ([devpress.csdn.net](https://devpress.csdn.net/aibjcy/68dbdcec8867235e138acb6e.html#:~:text=%E8%BF%91%E6%9C%9F%E6%9C%89%E5%A4%A7%E9%87%8F%E7%A0%94%E7%A9%B6%E8%87%B4%E5%8A%9B%E4%BA%8E%E6%8F%90%E5%8D%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%8B%E5%AE%9E%E5%87%86%E7%A1%AE%E6%80%A7%E5%92%8C%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E3%80%82%E8%BF%99%E4%BA%9B%E6%96%B9%E6%B3%95%E5%8C%85%E6%8B%AC%E4%BD%BF%E7%94%A8%E5%B0%91%E6%A0%B7%E6%9C%AC%E6%88%96%E9%9B%B6%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%80%9D%E7%BB%B4%E9%93%BE%EF%BC%88chain,chain%20of%20thought%EF%BC%89%E5%92%8C%E5%8F%8D%E6%80%9D%E6%9C%BA%E5%88%B6%EF%BC%88reflection%EF%BC%89%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%99%BA%E8%83%BD%E4%BD%93%E4%BB%A5%E5%8F%8A%E5%A4%9A%E8%BD%AE%E8%BE%A9%E8%AE%BA%E5%AF%B9%E4%BA%8E%E5%AE%9E%E7%8E%B0%E6%9C%80%E4%BD%B3%E6%80%A7%E8%83%BD%E9%83%BD%E6%98%AF%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E7%9A%84%E3%80%82%20%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E5%88%9D%E5%A7%8B%E6%9F%A5%E8%AF%A2%EF%BC%8C%E5%B0%BD%E7%AE%A1%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%20%E5%B1%9E%E4%BA%8E%E5%90%8C%E4%B8%80%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%AE%83%E4%BB%AC%E4%BB%8D%E4%BC%9A%E6%8F%90%E5%87%BA%E5%A4%9A%E6%A0%B7%E5%8C%96%E7%9A%84%E7%AD%94%E6%A1%88%EF%BC%88%E6%88%91%E4%BB%AC%E4%B9%9F%E7%A0%94%E7%A9%B6%E4%BA%86%E6%B7%B7%E5%90%88%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%83%85%E5%86%B5%EF%BC%8C%E5%A6%82ChatGPT%E5%92%8CBard%EF%BC%89%E3%80%82%E5%9C%A8%E8%BE%A9%E8%AE%BA%E5%B9%B6%E5%AE%A1%E6%9F%A5%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%9B%9E%E7%AD%94%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%E8%BF%99%E4%BA%9B%E6%A8%A1)) ([openreview.net](https://openreview.net/forum?id=QAwaaLJNCk#:~:text=indicate%20that%20this%20approach%20significantly,in%20language%20generation%20and%20understanding)). This “society of minds” approach is directly analogous to a red team vs. blue team exercise, and it showed that *even without human intervention*, adversarial dialogue can refine outputs. 

For our single-model setting, we can simulate this by sequential prompting: e.g., *“Give an argument for X.”* (Model produces argument.) Then: *“Now, as a devil’s advocate, critique the above argument.”* Finally: *“Respond to that critique or revise the argument.”* This **iterative role-play** draws on red-team methodology. Anthropic’s **Constitutional AI** is another inspiration – their model generates an answer, then a second pass critiques it against a set of principles (in effect, an internal red team), and the answer is revised ([medium.com](https://medium.com/%40minh.tran2_11384/a-review-of-llm-adversarial-prompting-44364903bf30#:~:text=set%E2%80%9D%20to%20generate%20outputs,source)). They found this led to safer and more aligned outputs than a single-pass approach ([medium.com](https://medium.com/%40minh.tran2_11384/a-review-of-llm-adversarial-prompting-44364903bf30#:~:text=set%E2%80%9D%20to%20generate%20outputs,source)). We might not need Anthropic’s specific safety focus, but the general idea holds: having the model *critique itself or another model* in the loop catches issues that a one-shot approach misses. 

In practice, employing red-team prompting means encouraging the model to **stress-test the argument from multiple angles**. For instance, prompt it to consider the **strongest counter-evidence** (what data would most contradict the paper’s claims?) or to apply different frameworks (utilitarian, deontological, economic, etc.) to see if the argument holds up. These are analogous to how red-teamers try diverse attack strategies. The key is to be explicit: without clear instructions, the model might stick to surface-level critiques. But if you say “List the top 3 potential failure modes or counterarguments this proposal hasn’t addressed,” you channel a red-team mindset. When done well, this can uncover exactly the logical gaps or hidden assumptions that real expert reviewers might point out.

## Failure Modes: Overcriticism and Straw Manning

While adversarial prompting often yields more critiques, it can also lead to **failure modes** like overcriticism and straw-manning. **Overcriticism** is when the model starts picking apart trivial points or inventing flaws that aren’t actually relevant – essentially critiquing for the sake of it. A strongly negative prompt can push an LLM to “find something wrong” even if the argument is solid, because the instruction biases it towards *anything* that could be construed as a flaw. This might manifest as the model focusing on minor phrasing issues, extremely unlikely edge cases, or hyper-technical quibbles, instead of the core arguments. **Straw-manning** is a related risk: if the model misinterprets or simplifies the argument into a weaker form, it will then criticize that malformed version. An adversarial stance without proper context can inadvertently distort the original point, leading the model to rebut claims the author never made. 

These failure modes have been noted in various guises in research. One study on iterative self-refinement observed that models tend to **amplify their own biases** when critiquing themselves without fresh input – essentially reinforcing mistaken assumptions ([www.emergentmind.com](https://www.emergentmind.com/topics/self-refinement#:~:text=A%20central%20challenge%20in%20self,2024)). That can be seen as a form of straw-manning their own answers. Another finding is that if the same model both generates and evaluates a piece of text, it may either be too lenient or perversely too harsh due to “self-bias” issues ([www.emergentmind.com](https://www.emergentmind.com/topics/self-refinement#:~:text=A%20central%20challenge%20in%20self,2024)) ([www.emergentmind.com](https://www.emergentmind.com/topics/self-refinement#:~:text=Another%20failure%20mode%20is%20reward,26)). In human terms, it’s like an author who either can’t see their own mistakes or who obsessively flags every tiny issue. In the *RealCritic* benchmark mentioned, *classic GPT models in a self-critique mode sometimes performed worse* than just giving an answer, highlighting that unguided critique can derail the solution ([arxiv.org](https://arxiv.org/abs/2501.14492#:~:text=this%20benchmark%20using%20eight%20challenging,https%3A%2F%2Fgithub.com%2Ftangzhy%2FRealCritic)). This underperformance might be due to the model second-guessing itself on correct reasoning or focusing on irrelevant negatives.

To **mitigate straw-manning**, the steelman approach discussed is very helpful – it forces faithful representation of the argument before critique. Including a **summary step** in the prompt like “First, summarize the argument in your own words,” or “Confirm what the author is claiming,” can ensure the model has the right target. If the summary is accurate, the chances of straw-manning drop. We could even have the model’s summary explicitly approved (by a human or by another model) before proceeding to critique.

To **mitigate overcriticism**, we can adjust the instructions to prioritize important issues: e.g. *“critique the argument, focusing on any significant logical gaps or unsupported assumptions – ignore minor stylistic issues.”* By emphasizing *significance*, we guide the model away from nitpicking. Another tactic is to allow the model to express a **calibrated judgment** about the argument before diving into flaws. For example: *“On a scale of 1 to 10, how convincing is this argument? Explain briefly. Now identify any weaknesses that led you to that rating.”* This way, if the model actually found the argument largely solid, it might say “I’d give it an 8/10” and then only list genuinely impactful weaknesses, rather than scrabbling for any flaw. Such calibration steps were explored in recent research – prompting models to reflect on their confidence or uncertainty can make their critiques more honest and proportional ([arxiv.org](https://arxiv.org/abs/2510.24505#:~:text=Accurate%20confidence%20calibration%20in%20Large,specific%29%3F%20Analysis%20shows%20confidence)) ([arxiv.org](https://arxiv.org/abs/2510.24505#:~:text=%282%29%20How%20to%20critique%3A%20self,distribution%20settings%2C%20advancing%20LLM%27s%20reliability)).

Finally, it’s worth reviewing the critiques the model produces, perhaps even with a second model or an automated check, to catch any spurious points. In a multi-step pipeline, after the model generates a critique, we could ask (either the same model or another instance): *“Are these critiques valid and relevant? Which, if any, are trivial or based on misreading?”* This kind of **critique-of-critique** approach is exactly what some researchers advocate for robust oversight: *“when direct evaluation is hard, a critique of a critique can be easier”* ([arxiv.org](https://arxiv.org/abs/2502.04675#:~:text=response%20to%20this%20challenge%2C%20we,these%20hypotheses%20and%20suggest%20that)). In fact, experiments with recursive critique (critique of critique of critique) show promise in catching poor criticisms and converging towards more reliable judgments ([arxiv.org](https://arxiv.org/abs/2502.04675#:~:text=recursively%20held%2C%20suggesting%20that%20when,promising%20direction%20for%20scalable%20oversight)). So, building in that redundancy can help filter out the model’s worse tendencies, ensuring that what ends up in front of a human researcher are substantive points, not noise.

## Calibration Effects of Adversarial Prompting

Adversarial prompting can also affect a model’s **calibration** – meaning how well its level of confidence or certainty in its answers aligns with reality. When a model is in “devil’s advocate” mode, it might sound very confident about problems in the argument, even if those are speculative. Conversely, if a model repeatedly critiques its own answers, it might start hedging or lowering its expressed confidence. We want a grader that not only finds issues, but also correctly gauges their importance and how likely it is that the author’s argument is wrong. That’s a form of calibration: not every flaw is fatal, and not every argument is equally flawed. 

Interestingly, some prompting techniques explicitly improve calibration by using self-critiquing. A 2024 study on *Fact-and-Reflection (FaR)* prompting found that having the model first state facts it knows, then reflect on them to answer a question, **significantly improved its calibrated confidence** – reducing calibration error by ~23% in their experiments ([arxiv.org](https://arxiv.org/abs/2402.17124#:~:text=expected%20LLM%20calibration%2C%20they%20also,on%20our%20multi)). The idea is that the reflection step makes the model more aware of uncertainties or gaps, so it doesn’t overstate its answers. Similarly, research on *Confidence Calibration via Critique* (Zong et al., 2025) introduced a method called **Self-Critique** and a training pipeline called **CritiCal**. They basically have the model generate a critique of its own answer and use that to adjust its confidence. The results were impressive: the CritiCal-trained model **outperformed even GPT-4** on some complex reasoning tasks, with far better calibrated confidence in its answers ([arxiv.org](https://arxiv.org/abs/2510.24505#:~:text=%282%29%20How%20to%20critique%3A%20self,distribution%20settings%2C%20advancing%20LLM%27s%20reliability)). In other words, teaching the model to critique itself led to answers that not only were more accurate, but whose *stated confidence levels matched accuracy* more closely.

What does this mean for our use case? It suggests that incorporating a self-reflection or uncertainty-estimation step could be valuable. For instance, after generating a critique, we might ask the model: *“How confident are you that these critiques identify real issues? Are there counterarguments that would weaken your critiques?”* This nudges the model to consider the limits of its own criticisms. An adversarial prompt on its own won’t do this – it will produce critiques with a confident tone by default. But a calibrated grader might say, “The argument has some weak points regarding X and Y, although these might be addressed if Z; overall, the critique is moderate.” Such nuance is crucial for researcher trust. Otherwise, an uncalibrated adversarial grader could erroneously flag every argument as deeply flawed (false positives), or if tuned too leniently, miss real problems (false negatives). We want the grader to *know when it doesn’t know*. Techniques like *FaR prompting* or *self-critique prompts* explicitly target this, and we should consider them in designing our system.

One caveat: adversarial prompting without reflection can actually *decrease* calibration on individual instances ([arxiv.org](https://arxiv.org/abs/2402.17124#:~:text=yet%20to%20be%20thoroughly%20explored,them%20to%20generate%20the%20final)). If the model is told to go full force, it might not express appropriate uncertainty. So, the best approach is to combine the adversarial stance with a reflection or rating mechanism. For example, instruct the model to provide its critique *and* also comment on how confident it is that the flaws it found are serious. This combined approach leverages the strength of devil’s advocacy (surfacing issues) with a check on overstatement. The empirical research so far strongly indicates that LLMs can self-monitor their confidence when asked in the right way – and doing so makes their outputs much more reliable. We’ll aim to bake this into the grader prompt structure.

## Practical Recommendations for Prompting Critiques

Given the above insights, here are some concrete prompting strategies for our LLM grader:

- **Use a Multi-Step Prompt:** Break the task into stages. For example: *“First, briefly summarize the author’s argument in a few sentences. Make it as convincing as possible (steelman it). Second, assume the role of a critical reviewer and list the potential weaknesses or counterarguments. Third, assess how serious these weaknesses are overall.”* This prompt forces the model to **understand, critique, and calibrate**. The summary/steelman ensures fidelity to the original argument, the critique stage generates the adversarial analysis, and the final assessment stage prevents runaway negativity by putting the critiques in perspective.

- **Explicit Devil’s Advocate Role:** Don’t be shy about literally instructing the model to be adversarial *in one of the steps*. For instance: *“Now, play devil’s advocate: what might someone disagreeing with this argument say?”* or *“As a skeptic, find as many flaws or gaps as you can.”* Our domain is philosophical/economic arguments, which often have many assumptions – an adversarial lens will help dig those out. Just ensure this comes *after* the model has shown comprehension of the argument. By compartmentalizing roles (e.g. “You were the summarizer, now you are the critic”), we leverage the model’s ability to follow persona-based instructions ([medium.com](https://medium.com/%40minh.tran2_11384/a-review-of-llm-adversarial-prompting-44364903bf30#:~:text=This%20is%20one%20of%20the,in%20restrictions)). This typically leads to more *focused* critiques, as the model leans into the skeptical persona strongly.

- **Guided Critique Criteria:** When prompting for critique, guide the model on **what to critique**. For example: *“Focus on logical consistency, empirical support, and possible unintended consequences. Avoid mere stylistic critiques.”* By specifying criteria, we target the critique to be substantive. This is drawn from red-teaming best practices – having a checklist of areas to attack can ensure thorough coverage. If the research piece is about AI governance, you might add: *“Consider technical feasibility, economic incentives, and ethical assumptions separately.”* Structured prompts like this have the model evaluate the argument along multiple axes, reducing the chance it overlooks something important.

- **Iterative Critique and Revision:** Consider an iterative loop where the model critiques, then possibly revises its critique. For example: *“List three significant weaknesses. Now, for each weakness, check if it is indeed present in the text or if the author might have already addressed it.”* This second step forces the model to reflect on its own critique points – acting as a second-order critic. If a critique was a stretch, the model might catch itself (“actually, on point 2, the author did mention a counterpoint, so this might not be a fatal flaw”). Empirically, this mimics the *critique-of-critique* approach that showed success in oversight contexts ([arxiv.org](https://arxiv.org/abs/2502.04675#:~:text=response%20to%20this%20challenge%2C%20we,these%20hypotheses%20and%20suggest%20that)). It helps filter out low-quality criticisms automatically.

- **Multi-Model or Ensemble Approaches:** If resources allow, use different models or multiple prompt instances to get diverse critiques. One simple version: ask GPT-4 and another model (say, Claude or an open-source LLM) to critique the same content, then compare. Even using GPT-4 with different phrasing or random seed can produce varied perspectives. Then you can either manually or with another AI merge the critiques (or take the union of points). Research in multi-agent debate suggests heterogeneity can help – e.g., ChatGPT and Bard debating produced better results than either alone ([devpress.csdn.net](https://devpress.csdn.net/aibjcy/68dbdcec8867235e138acb6e.html#:~:text=%E7%9B%B8%E6%AF%94%E4%B9%8B%E4%B8%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E7%A7%8D%E4%BA%92%E8%A1%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%8F%97%E3%80%8A%E5%BF%83%E6%99%BA%E7%A4%BE%E4%BC%9A%E3%80%8B%EF%BC%88Society%20of%20Mind%EF%BC%8CMinsky%EF%BC%8C1988%EF%BC%89%E5%92%8C%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%90%AF%E5%8F%91%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%A4%9A%E4%B8%AA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%EF%BC%88%E6%88%96%E6%99%BA%E8%83%BD%E4%BD%93%20%EF%BC%89%E5%88%86%E5%88%AB%E6%8F%90%E5%87%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%9E%E7%AD%94%E5%92%8C%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E5%A4%9A%E8%BD%AE%E8%BE%A9%E8%AE%BA%E5%85%B1%E5%90%8C%E8%BE%BE%E6%88%90%E4%B8%80%E4%B8%AA%E4%B8%80%E8%87%B4%E7%9A%84%E6%9C%80%E7%BB%88%E7%AD%94%E6%A1%88%E3%80%82%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E6%9F%A5%E8%AF%A2%EF%BC%8C%E5%A4%9A%E4%B8%AA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%E9%A6%96%E5%85%88%E7%8B%AC%E7%AB%8B%E7%94%9F%E6%88%90%E5%90%84%E8%87%AA%E7%9A%84%E5%80%99%E9%80%89%E7%AD%94%E6%A1%88%E3%80%82%E7%84%B6%E5%90%8E%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%20%E9%98%85%E8%AF%BB%E5%B9%B6%E6%89%B9%E8%AF%84%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%99%E4%BA%9B%E5%86%85%E5%AE%B9%E6%9B%B4%E6%96%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E7%AD%94%E6%A1%88%E3%80%82%E8%BF%99%E4%B8%80%E8%BF%87%E7%A8%8B%E5%9C%A8%E5%A4%9A%E4%B8%AA%E8%BD%AE%E6%AC%A1%E4%B8%AD%E9%87%8D%E5%A4%8D%E8%BF%9B%E8%A1%8C%E3%80%82,%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E5%88%9D%E5%A7%8B%E6%9F%A5%E8%AF%A2%EF%BC%8C%E5%B0%BD%E7%AE%A1%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%20%E5%B1%9E%E4%BA%8E%E5%90%8C%E4%B8%80%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%AE%83%E4%BB%AC%E4%BB%8D%E4%BC%9A%E6%8F%90%E5%87%BA%E5%A4%9A%E6%A0%B7%E5%8C%96%E7%9A%84%E7%AD%94%E6%A1%88%EF%BC%88%E6%88%91%E4%BB%AC%E4%B9%9F%E7%A0%94%E7%A9%B6%E4%BA%86%E6%B7%B7%E5%90%88%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%83%85%E5%86%B5%EF%BC%8C%E5%A6%82ChatGPT%E5%92%8CBard%EF%BC%89%E3%80%82%E5%9C%A8%E8%BE%A9%E8%AE%BA%E5%B9%B6%E5%AE%A1%E6%9F%A5%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%9B%9E%E7%AD%94%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%E8%BF%99%E4%BA%9B%E6%A8%A1%20%E5%9E%8B%E5%87%A0%E4%B9%8E%E6%80%BB%E6%98%AF%E4%BC%9A%E6%94%B6%E6%95%9B%E5%88%B0%E4%B8%80%E4%B8%AA%E6%9B%B4%E5%87%86%E7%A1%AE%E7%9A%84%E4%B8%80%E8%87%B4%E7%AD%94%E6%A1%88%E3%80%82%E8%BE%A9%E8%AE%BA%E7%BB%93%E6%9E%9C%E4%B9%9F%E6%9B%B4%E5%B0%91%E5%8C%85%E5%90%AB%E6%A8%A1%E5%9E%8B%E5%86%85%E9%83%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E7%9A%84%E9%94%99%E8%AF%AF%E4%BA%8B%E5%AE%9E%E3%80%82%E8%BF%99%E6%98%AF%E5%9B%A0%E4%B8%BA%E9%9A%8F%E7%9D%80%E8%BE%A9%E8%AE%BA%E7%9A%84%E8%BF%9B%E8%A1%8C%EF%BC%8C%E5%8D%95%E4%B8%AA%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B%E5%80%BE%E5%90%91%E4%BA%8E%E5%AF%B9%E4%B8%8D%E7%A1%AE%E5%AE%9A%E7%9A%84%E4%BA%8B%E5%AE%9E%E4%BA%A7%E7%94%9F%E5%88%86%E6%AD%A7%EF%BC%8C%E5%B9%B6%E6%9C%80%E7%BB%88%E5%9C%A8%E7%AD%94%20%E6%A1%88%E4%B8%AD%E7%9C%81%E7%95%A5%E8%BF%99%E4%BA%9B%E5%86%85%E5%AE%B9%EF%BC%88%E8%A7%81%E5%9B%BE12%EF%BC%89%E3%80%82Notes%EF%BC%9A%E5%9B%BE12%E8%AF%B4%E6%98%8E%E4%BA%86%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E8%BE%A9%E8%AE%BA%E8%83%BD%E6%9C%89%E6%95%88%E5%87%8F%E5%B0%91%E4%BC%A0%E8%AE%B0%E7%94%9F%E6%88%90%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%AE%9E%E9%94%99%E8%AF%AF%EF%BC%8C%E6%8F%90%E9%AB%98%E5%86%85%E5%AE%B9%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C%E5%87%86%E7%A1%AE%E6%80%A7%E3%80%82%20%E6%9C%80%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%EF%BC%8C%E8%BE%A9%E8%AE%BA%E7%9A%84%E4%BD%9C%E7%94%A8%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AF)). For an automated grader, an ensemble of critiques could be more robust. You might prompt one “agent” to focus on ethical issues, another on logical structure, etc., then aggregate their findings.

- **Avoiding Pitfalls in Instructions:** Be careful not to push the model into **confabulation**. If you say “find flaws at all costs,” it may start to hallucinate things the author *might* have said or assumed. Our prompts should remind the model to **stick to the content**. E.g.: *“Based on the text, what are possible weaknesses... If something is not explicitly in the text, do not speculate it as a flaw.”* Keeping critiques grounded in the actual paper or post will make them more legitimate. This is especially important in philosophical content where it’s easy to invent straw positions – the model should critique what’s there or what logically follows from what’s there, not make up new claims to knock down.

- **Calibration and Tone:** We likely want the grader’s output to not only list critiques but also convey how **important** each critique is. In the prompt, you can ask the model to rank the issues or tag them (e.g. minor, moderate, major concern). This ties into the calibration aspect. A prompt snippet could be: *“For each identified weakness, state whether it is a minor issue, a significant concern, or a fatal flaw.”* During development, we can compare this against human judgments to see if the model’s labels match our sense of severity. Ensuring the AI doesn’t cry wolf (label everything as “major flaw” when it’s not) will increase trust in its grading. If we notice it over-labeling, we can tweak the prompt to be more conservative or even provide an example (few-shot) of what counts as major vs. minor.

- **Testing with Human-Rated Samples:** Since we have ~20 human-rated critiques, we should use those to A/B test our prompting strategies. Try generating critiques for those 20 using a neutral prompt, a devil’s-advocate prompt, and a steelman-critique prompt. See which version aligns best with the human ratings of “valuable vs noise.” Over a small sample, >75% agreement might be too high a bar to see immediately, but we can qualitatively check if the adversarially prompted critiques contain the key points humans noted. This will help empirically validate whether these techniques are actually improving quality in our specific domain. It’s possible that for simpler arguments a straightforward prompt works fine, whereas for very complex longtermist arguments, the steelman+critique prompt shines. Our evaluation should catch that nuance.

In summary, our recommendation is to use a **structured, multi-step prompting approach** that combines neutral understanding with adversarial analysis, and to include calibration (priority/severity) in the output. This harnesses the best of devil’s advocate prompting while curbing its excesses. It’s aligned with what research and practical testing tell us: LLMs can provide high-quality critiques when guided to do so in a thoughtful manner, but they need clear instructions to avoid going off the rails or staying too superficial.

## Key Papers and Resources for Further Reading

For deeper evidence and ideas, here are some relevant works:

- **Kim et al. (2024) – *DEBATE: Devil’s Advocate-Based Assessment and Text Evaluation*** ([www.emergentmind.com](https://www.emergentmind.com/papers/2405.09935#:~:text=DEBATE%2C%20an%20NLG%20evaluation%20framework,influence%20the%20performance%20of%20evaluators)). Introduces a multi-agent evaluator with a dedicated critic agent, showing improved performance in assessing generated text quality.

- **Du et al. (2024) – *Improving Factuality and Reasoning in LLMs through Multiagent Debate*** ([openreview.net](https://openreview.net/forum?id=QAwaaLJNCk#:~:text=indicate%20that%20this%20approach%20significantly,in%20language%20generation%20and%20understanding)). Demonstrates that having multiple models critique and revise answers in a debate-style setup markedly improves accuracy and reasoning, compared to single-model reasoning.

- **Madaan et al. (2023) – *Self-Refine: Iterative Refinement with Self-Feedback***. Proposes letting the model iteratively criticize and refine its output. It showed gains on tasks like explanation generation (though with some limits on reasoning tasks). Summarized in an *Emergent Mind* article ([www.emergentmind.com](https://www.emergentmind.com/topics/self-refinement#:~:text=Self,formalized%2C%20extended%2C%20and%20evaluated%20self)).

- **Zheng et al. (2024) – *Critic-CoT: Boosting Reasoning via Chain-of-Thought Critics***. Introduces a method where a second “critic” chain-of-thought checks the first model’s reasoning step by step, leading to more reliable answers. This is an implementation of stepwise think-and-critique in math/logic problems.

- **Zong et al. (2025) – *CritiCal: Can Critique Help LLM Confidence Calibration?*** ([arxiv.org](https://arxiv.org/abs/2510.24505#:~:text=%282%29%20How%20to%20critique%3A%20self,distribution%20settings%2C%20advancing%20LLM%27s%20reliability)). Investigates using self-critiques to calibrate a model’s confidence. They present a training method that makes models better at knowing when they might be wrong – relevant for producing honest critiques.

- **Wang et al. (2024) – *Devil’s Advocate: Anticipatory Reflection for LLM Agents*** ([openreview.net](https://openreview.net/forum?id=4BpEfStZBD#:~:text=suitability%20and%20results%20of%20their,experimental%20results%20suggest%20that%20our)) ([openreview.net](https://openreview.net/forum?id=4BpEfStZBD#:~:text=introspection,needed%20to%20achieve%20a%20task)). Explores an agent that anticipates potential failures (playing devil’s advocate with itself while planning). It’s more about interactive tasks, but it supports the idea that introspective, adversarial reasoning improves outcomes.

- **Anthropic (2023) – *Constitutional AI: Harmlessness via AI Self-Critique*** ([medium.com](https://medium.com/%40minh.tran2_11384/a-review-of-llm-adversarial-prompting-44364903bf30#:~:text=set%E2%80%9D%20to%20generate%20outputs,source)). While focused on safety, this work is a concrete example of using an AI’s own critiques (based on a set of principles) to improve and filter its answers. It’s a successful real-world deployment of critique-then-revise.

- **RealCritic Benchmark (Tang et al., 2025)** ([arxiv.org](https://arxiv.org/abs/2501.14492#:~:text=this%20benchmark%20using%20eight%20challenging,https%3A%2F%2Fgithub.com%2Ftangzhy%2FRealCritic)). Offers a suite of tasks to evaluate how well models can critique and improve answers. Their findings (that reasoning-trained models outperform standard ones in critique tasks) underscore the importance of technique in prompting critiques.

- **Emergent Mind “Stepwise Think-Critique” (2025)** ([www.emergentmind.com](https://www.emergentmind.com/topics/stepwise-think-critique-stc#:~:text=1,Principles)) ([www.emergentmind.com](https://www.emergentmind.com/topics/stepwise-think-critique-stc#:~:text=1,17%20Dec%202025)). A great overview of the stepwise think-and-critique paradigm, citing multiple papers on how alternating generation and critique leads to better multi-step reasoning. Good for inspiration on structured prompting.

Each of these resources can provide more nuanced insight into adversarial prompting and critique generation. Experimenting with the ideas from these papers in our context (philosophical and economic arguments about AI) will likely require some tuning, but the core lesson is consistent: **structured, adversarial reasoning improves output quality**, as long as we manage its pitfalls. By applying these techniques, we aim to have our LLM grader reliably surface the most valuable critiques – the kind that a human researcher would find perceptive – while filtering out the noise.