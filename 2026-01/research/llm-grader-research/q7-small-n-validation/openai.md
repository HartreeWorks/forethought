# OpenAI Deep Research: q7-small-n-validation

**Model:** o3-deep-research-2025-06-26
**Generated:** 2026-01-20T14:31:37.571Z
**Response ID:** resp_0266b5ab6469dff400696f8f0bf4888190820623fbce42de71

---

# Validating an LLM Grader with ~20 Samples: Metrics and Statistical Techniques

Validating a grader with only around 20 human-rated critiques is challenging, but there are methods to maximize rigor. **Small-N evaluation requires careful choice of metrics and explicit uncertainty estimates.** Below we outline appropriate metrics, how to compute confidence intervals, and strategies like bootstrapping, Bayesian inference, and sequential testing to make the most of a tiny validation set.

## Performance Metrics for Agreement

**Choose metrics that reflect agreement quality**, not just raw percentage:

- **Accuracy (Percent Agreement):** The simplest metric – e.g. “grader agreed on 15 of 20 = 75%.” Accuracy is intuitive but can be misleading. It doesn’t adjust for chance agreement and can be overly optimistic if one class dominates ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=to%20simply%20calculate%20observed%20agreement,5%5D%2C%20were%20proposed)). For instance, if most critiques are “noise,” a grader that labels *everything* as noise might hit 75% accuracy by luck.

- **Cohen’s Kappa:** A more robust inter-rater reliability measure that accounts for chance agreement ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=to%20simply%20calculate%20observed%20agreement,5%5D%2C%20were%20proposed)). Kappa ranges from -1 to 1 (0 means no better than chance). Using kappa is advisable with small samples because it compensates for class imbalance – it “corrects” observed agreement by expected random agreement ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=to%20simply%20calculate%20observed%20agreement,5%5D%2C%20were%20proposed)). *Example:* If the grader and researcher both say “not valuable” for most critiques, raw agreement could be high, but kappa will discount the agreement that’s expected by chance ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=to%20simply%20calculate%20observed%20agreement,5%5D%2C%20were%20proposed)). (Historically, Cohen introduced kappa to handle raters possibly guessing due to uncertainty ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/#:~:text=While%20there%20have%20been%20a,about%20what%20level%20of%20kappa)).) 

- **Precision, Recall, F<sub>1</sub> (for “valuable” critiques):** If the class distribution is skewed (say only 5 of 20 critiques are truly valuable), accuracy can be misleading. In such cases, treat it as a classification task and evaluate precision/recall. **Precision** = of critiques the grader flagged as valuable, how many did the human also find valuable; **Recall** = of the truly valuable critiques, how many did the grader catch. **F<sub>1</sub>** is the harmonic mean of precision and recall. This focuses evaluation on the positive “valuable” class. It prevents a trivial strategy of labeling everything “noise” from looking good (which would yield high accuracy but 0 recall for valuable critiques).

- **Rank Correlation (if using scores):** If the grader assigns continuous scores or rankings to critiques (e.g. a relevance score), use **Spearman’s rank correlation** or **Pearson’s correlation** with the human’s scores. This captures how well the grader’s ordering of critique quality matches the researcher’s. With n≈20, Spearman/Pearson above ~0.45 would be needed to reach statistical significance (since df=18) – that’s a fairly large correlation due to the small sample. In practice, report the correlation and maybe its confidence interval, but be cautious interpreting moderate correlations with so few points.

**Tip:** In reports, consider presenting multiple metrics. For example: “Accuracy = 80% (16/20 correct), Cohen’s κ = 0.58 (moderate agreement), F<sub>1</sub> on valuable-class = 0.70.” This paints a fuller picture of performance.

## Confidence Intervals for Small Samples

With only 20 data points, **any performance estimate has high uncertainty**. Always accompany metrics with a confidence interval:

- **Binomial Proportion CI:** For accuracy (or any proportion of agreement), avoid the naive ±1.96√(p(1−p)/n) Wald interval – it’s unreliable for n=20 ([repository.upenn.edu](https://repository.upenn.edu/items/ab6bcab6-4515-48a1-82e2-a9311957aa08#:~:text=We%20revisit%20the%20problem%20of,Based%20on%20this%20analysis%2C%20we)) ([repository.upenn.edu](https://repository.upenn.edu/items/ab6bcab6-4515-48a1-82e2-a9311957aa08#:~:text=recommend%20the%20Wilson%20interval%20or,use%20of%20the%20Jeffreys%20interval)). Instead use **exact or Wilson score intervals**. For example, if 16 of 20 critiques match (80%), the 95% Wilson interval is roughly ~**58% to 92%** – extremely wide. (In fact, with 16/20 observed, the true agreement could plausibly be well below 75% because the lower bound is ~0.58.) Even at the target threshold of 75% (15/20 correct), the 95% CI spans roughly **53% to 89%** by Wilson’s method – essentially “anywhere from a coin-flip to near-perfect” agreement. This wide interval reflects the uncertainty from only 20 samples ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). Using Wilson or the exact Clopper–Pearson method ensures proper coverage (the interval is conservative) ([www.statsmodels.org](https://www.statsmodels.org/0.9.0/generated/statsmodels.stats.proportion.proportion_confint.html#:~:text=Notes)). Statistical literature specifically **recommends Wilson or Jeffreys (Bayesian) intervals for small n** because they give more reliable results than the standard normal approximation ([repository.upenn.edu](https://repository.upenn.edu/items/ab6bcab6-4515-48a1-82e2-a9311957aa08#:~:text=are%20presented%2C%20each%20with%20its,use%20of%20the%20Jeffreys%20interval)).

- **Inter-rater Reliability CI:** For Cohen’s κ, an analytical standard error exists, but with n=20 it’s not very trustworthy (asymptotic assumptions break down) ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=the%20standard%20error%2C%20leading%20to,bootstrap%20approach%2C%20as%20suggested%20by)) ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=categories%20%28Table%C2%A02%29,There%20was%20no%20relevant)). A safer approach is to compute a **bootstrap confidence interval** for kappa (explained below) ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=study%20and%20recommended%20using%20bootstrap,distribution%20of%20alpha%20is%20unknown)). If κ comes out to say 0.60 on 20 samples, a 95% CI via bootstrap might be something like 0.20–0.85 (very wide). Report this interval to show uncertainty. *Kappa significance:* You can also test if κ > 0 (better than chance) by checking if the CI lower bound is above 0 ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=diagnostic%20accuracy%20studies%20,lies%20with%20a%20given%20probability)). For substantial agreement (often κ > 0.6 as a rule-of-thumb), you’d want the lower bound above 0.6 ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=Therefore%2C%20a%20confidence%20interval%20can,Landis%20and)) – unlikely with such a small n.

- **Correlation CI:** If using correlation, compute a confidence interval via Fisher’s *z*-transform. For example, an observed Pearson r=0.5 with n=20 has 95% CI roughly ±0.30 (very approximate). So r=0.5 would give a CI from about 0.2 to 0.8 – again too wide to pin down precisely. Be cautious: with n=20, you need *r* > 0.44 for significance at α=0.05. Any moderate correlation may not reach that threshold.

- **Significance Tests:** Instead of (or in addition to) CI, you can do small-sample hypothesis tests:
  - For accuracy: use an **exact binomial test**. For instance, test H0: *p* = 0.75 (75% agreement) vs H1: *p* > 0.75. With n=20, you’d need something like 19 or 20 correct to reject H0 at 5% significance (15 or 16 out of 20 would *not* be statistically above 75% given the small n) ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). Similarly, to test vs a baseline (say 50%), use binom.test – e.g. 15/20 correct vs 50% yields p ≈ 0.02 (significant) ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=proportions%20being%20subject%20to%20high,test%20cases%20in%20my%20example)) ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)), but 12/20 vs 50% would be p ~0.21 (not significant). In general, with 20 samples only **large deviations** from the null will test as significant.
  - For kappa: a standard approximate test exists for H0: κ=0, but better to construct the CI or use permutation tests. With very small n, a permutation test (randomly shuffling one rater’s labels many times to see how often you’d get a kappa as high as observed) could be an alternative to get an exact p-value for κ.
  - For correlation: use a **t-test** (the usual Pearson correlation significance test). This is equivalent to testing if correlation = 0 given n. Again, because df=18, you need a fairly large correlation to be significant.

**Bottom line:** With n ≈ 20, any result needs to be tempered with “± margin of error.” It’s good practice to say, for example: *“Grader achieved 80% agreement (95% CI ~58–92%), which does not conclusively exceed our 75% target due to the small sample.”* Reporting the uncertainty makes it clear whether the >75% criterion is confidently met or just observed by point estimate.

## Bootstrap Resampling for Confidence Intervals

When analytic formulas are unreliable, **bootstrapping is your friend** – especially with tiny datasets:

- **How to bootstrap:** Resample the 20 critiques with replacement to create, say, 1000 pseudo-datasets of size 20. For each resample, compute the metric of interest (accuracy, κ, F<sub>1</sub>, etc.). Then use the distribution of those 1000 bootstrap metrics to derive a confidence interval (e.g. take the 2.5th and 97.5th percentiles for a 95% CI).

- **Why bootstrap:** It makes minimal assumptions and works well for complex metrics. For example, the distribution of F<sub>1</sub> or Cohen’s κ is not easy to derive theoretically for n=20, but bootstrapping approximates it. Simulation studies **recommend bootstrap intervals for kappa** to get correct coverage ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=study%20and%20recommended%20using%20bootstrap,distribution%20of%20alpha%20is%20unknown)), since standard errors can underestimate variability in small samples ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=the%20standard%20error%2C%20leading%20to,bootstrap%20approach%2C%20as%20suggested%20by)). Bootstrapping was found to yield confidence intervals for κ with near-nominal coverage even when n is small ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=Fleiss%E2%80%99%20K%20and%20Krippendorff%E2%80%99s%20alpha,nominal%20order%2C%20Krippendorff%E2%80%99s%20alpha%20is)) ([bmcmedresmethodol.biomedcentral.com](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0200-9#:~:text=categories%20%28Table%C2%A02%29,There%20was%20no%20relevant)).

- **Number of resamples:** Common practice is at least 1,000 resamples (many use 5,000+ for better precision). With 20 points, you might not gain much beyond 1,000–2,000 resamples, but be sure to use a sufficient number to stabilize the interval estimates.

- **Pitfalls:** For very small n, bootstrap samples can be quirky (e.g. some resampled sets may pick the same critique many times and miss others). This reflects genuine uncertainty (our dataset might not be representative of the true distribution). The resulting CIs may be quite wide or even discrete (especially for binary outcomes, you might get only a few distinct accuracy values possible in each bootstrap). That’s okay – wide intervals are an *honest* indication that with 20 data points we haven't pinned down performance. **If the bootstrap CI includes your target (75%), you cannot be confident the grader truly exceeds that target.** 

- **Example:** Suppose the grader got 14/20 correct (70% accuracy). A bootstrap might show a 95% CI of roughly 45%–90% (indicating we’re so uncertain that true performance could be below 50% or above 90%). Even if the point estimate was 70%, we wouldn’t trust that it’s definitively above 75%. In contrast, if the grader got 19/20 (95%), the bootstrap CI might be, say, 76%–99%, giving more confidence performance is above 75%. (This aligns with the earlier binomial test analysis that needing 19/20 to “prove” >75%.) 

In summary, use bootstrapping for any metric where you lack a simple exact method. It’s an essential tool for small-sample stats and will **provide a more realistic error bar** on your grader’s performance ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)).

## Bayesian Approaches for Small-$n$ Validation

**Bayesian inference can be very useful with limited data**, as it allows incorporating prior knowledge and yields direct probability statements about performance:

- **Beta-Binomial modeling:** If we treat each critique’s evaluation as a Bernoulli trial (success = grader agrees with researcher), we can put a **Beta prior** on the grader’s true agreement rate *p*. A common non-informative choice is Beta(1,1) (uniform). After observing $x$ agreements out of $n=20$, the posterior for *p* is Beta($x+1$, $n-x+1$). For example, with 16/20 correct, the posterior is Beta(17,5). We can then derive a **Bayesian credible interval** for *p* (e.g. the 95% highest posterior density interval). This credible interval will be similar to the Wilson interval – in fact, the **Jeffreys interval** (from a Beta(0.5,0.5) prior) is a recommended objective Bayesian interval with good frequency properties ([repository.upenn.edu](https://repository.upenn.edu/items/ab6bcab6-4515-48a1-82e2-a9311957aa08#:~:text=are%20presented%2C%20each%20with%20its,use%20of%20the%20Jeffreys%20interval)).

- **Probabilistic statements:** The Bayesian approach shines in giving intuitive statements. For instance, using the Beta(1,1) prior above, you could say: *“Based on 16/20 correct, there is a 75% posterior probability that the grader’s true accuracy exceeds 75%.”* Or vice versa, *“There’s a 25% chance it’s actually below 75%, given this data.”* This is often more directly actionable for decision-making than a frequentist p-value. If that probability isn’t high enough for comfort, you might decide the grader isn’t reliable **yet**. Bayesian analysis thus can incorporate the uncertainty into a single metric: $P(p > 0.75 \mid \text{data})$.

- **Informative priors:** If you have prior beliefs or related data (e.g. maybe on a similar task the LLM grader achieved ~70% agreement), you can encode that in a prior (say Beta($\alpha,\beta$) with mean 0.70). With only 20 data points, the prior can noticeably influence results – which can be either a feature or a bug. It’s a feature if you genuinely have external evidence about likely performance (it stabilizes the estimate), but it’s a bug if you choose an arbitrary optimistic prior (you might then overestimate performance). Use informative priors sparingly and do sensitivity analysis (try a couple different priors and see how conclusions change).

- **Bayesian decision threshold:** You can formalize the success criterion in Bayesian terms: for example, “Grader is acceptable if $P(p > 0.75) > 0.8$ given the data.” Then evaluate that with your posterior. This might be more appropriate than a sharp cutoff on observed accuracy. It acknowledges uncertainty: maybe the grader got only 14/20 (70%), failing the point estimate check, but perhaps that yields, say, a 40% chance that true performance is >75%. You might decide you need more data in that case rather than flatly rejecting the grader.

- **Beyond proportions:** Bayesian methods can be applied to other metrics as well (though it gets more complex). For example, one could do a Bayesian model for Cohen’s kappa (treating the 2×2 contingency table with a Dirichlet prior) or for a correlation (using e.g. a Beta prior on $r$ under certain assumptions). However, these are more advanced. With small data, a Bayesian *hierarchical* model might even combine multiple graders or multiple papers’ critiques if available, but that’s beyond scope. For a single grader’s binary success/fail outcomes, the Beta-Binomial is the straightforward approach.

In short, a Bayesian perspective can **make the most of small sample data**. It allows you to naturally incorporate uncertainty and even stop early if confidence is high. Just ensure any prior is justified. If you have no strong prior info, stick to an uninformative (Jeffreys or uniform) prior, which essentially “lets the data speak,” albeit with the understanding that with 20 data points the data doesn’t speak very loudly.

## Using the Small Sample for Calibration vs. Testing

With only ~20 human ratings available, you face a dilemma: **should you use them purely to evaluate the grader, or also to tune it?**

- **Hold-out testing (no tuning):** Ideally, you’d treat these 20 as a final hold-out test set to get an unbiased evaluation of the grader. You’d fix the grader’s prompts or scoring rules *before* seeing these 20, then simply measure performance. This avoids any overfitting. The drawback is you get no opportunity to improve the grader if it underperforms – you have to accept whatever results you get, and you’ve exhausted your budget of human evaluations.

- **Using data for calibration:** It might be tempting to “peek” at these 20 samples to adjust the grader – for example, maybe you notice the grader tends to mis-classify a certain type of critique, so you refine the prompt and re-grade those. Or if the grader outputs a continuous score for critique quality, you might choose an optimal threshold on that score (e.g. pick the threshold that best separates valuable vs noise on these 20) to maximize alignment. This *calibration* can significantly improve performance **on these 20** – but it means you no longer have an unbiased test. You’d be optimising on noise and idiosyncrasies specific to those samples. The measured 75% agreement could become an overestimate of general performance.

- **Trade-off:** Using the data for calibration is essentially “training on the test set,” which invalidates a strict evaluation. However, with such a small dataset, a **middle ground** is to use **cross-validation**. For example, do a 5-fold cross-validation: split the 20 critiques into 5 folds of 4; in each fold, use 16 for calibration (tune your threshold or prompt) and then evaluate on the 4 held-out. Do this for all folds and aggregate results. This way each data point gets used for evaluation exactly once, providing a more objective performance estimate of a calibrated grader. Keep in mind, though, that with 20 samples the variance of cross-validated performance will be high ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). One extreme outlier can swing a fold’s result dramatically. You might repeat the cross-val multiple times with different random splits to gauge stability.

- **Calibration set vs. test set:** Another approach is to explicitly split the data: e.g. use 10 samples as a “calibration” or development set and reserve the other 10 as a final test. But 10/10 splits make each set even smaller, so neither the tuning nor the test will be very reliable. Still, if there are obvious calibration needs (like choosing a probability threshold for classification), using part of the data for that might be worthwhile. Just be transparent: for instance, “We calibrated the grader’s sensitivity on 10 labeled examples, then evaluated on a separate 10.”

- **Awareness of overfitting:** Any time you adjust the grader based on these data, **the agreement on those same data is an overestimate** of future performance. For example, setting a threshold to exactly match the human decisions on a subset could give you 90% agreement on that subset – but that’s because you explicitly fit to it. Always test on data not used in making any decisions, if possible.

- **When data is extremely scarce:** In some cases, using all 20 for evaluation might show the grader is at (say) 60% – below your bar. In such a scenario, you might decide it’s better to turn those into training data to try to improve the grader (via fine-tuning or prompt engineering), then later gather a *new* small test set to re-evaluate. The cost is additional labeling, but it might be necessary if the baseline result is poor. On the other hand, if the grader is already around the 70–80% range, you might refrain from any tuning to preserve the clean test.

**Recommendation:** If at all possible, treat these ~20 as a pure evaluation set to make a yes/no decision about the grader. If the grader fails, consider the whole project in a pilot phase – improve the grader (even using those 20 as feedback), then get another batch of ratings to formally evaluate the improved system. With such a high-stakes criterion (>75% agreement), you don’t want to fool yourself by inadvertently overfitting to 20 quirky examples. The trade-off is tough, but lean toward keeping the evaluation unbiased. As one expert puts it, with very small test samples the results are so variable that *“hardly anything can be concluded”* – you’re better off planning for more data if needed ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)) ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=,test%29%20sample%20size)).

## Power and Detectable Effect Size with *n*≈20

One important question is: **what effect sizes can 20 samples reliably detect?** In statistical terms, the “power” of your evaluation is low. A few scenarios:

- **Meeting the 75% agreement bar:** Suppose the grader’s true long-run accuracy is exactly 75%. With 20 samples, you’d expect on average 15 correct. But random fluctuation is large – there’s a good chance you’d see  thirteen or fourteen correct (65–70%), or conversely 16+ correct (80%) just by luck ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). That means 20 samples is **not enough to confidently distinguish 75% from, say, 65% or 85%**. You’d need many more samples to be sure. In fact, to statistically **conclude** the grader exceeds 75%, you saw that you’d basically need 19/20 right. That implies the true accuracy likely had to be well over 90% to achieve that in a fair test. In other words, the grader would have to overshoot the target by a large margin for a sample of 20 to catch it with high confidence.

- **Detectable difference:** If you were comparing two graders or a grader vs. baseline, 20 each is very low power. Roughly speaking, differences smaller than about **20 percentage points** in accuracy will not be significant. For instance, distinguishing 70% vs 80% accuracy with n=20 per group is borderline – the confidence intervals overlap heavily (each ±15–20%). But distinguishing 50% vs 80% is easier: that large gap might show significance with n=20 (e.g. 15/20 vs 10/20 correct is a noticeable difference). For inter-rater measures: a difference between κ=0.2 and κ=0.6 likely wouldn’t be reliably detected with 20 samples – both would have wide error bars. Similarly for correlation: small correlations (r ~0.2–0.3) won’t be distinguishable from zero; only **large effects** (r ~0.5 or more, which is a big effect in most fields) will stick out with 20 points.

- **Power analysis:** To formalize this, one could do a power calculation. For example, for a one-sample proportion test: if the true p = 85% and H0 p = 75%, what’s the chance we get a significant result at α=0.05 with n=20? As an approximation: the test would require 19/20 as a success to pass. The probability of ≥19 successes if true p=0.85 is only around 18% (very low power). Even if true p=90%, the chance of getting ≥19/20 is ~39% ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). In fact, not until the true performance is ~95% do you get ~73% chance to see 19 or 20 correct ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). This shows that **with n=20, the test is extremely underpowered unless the grader is dramatically better than the threshold**. Conversely, if the grader were only 60% accurate in reality, there’s also a non-trivial chance it might luck into 15/20 correct (~7.5% chance under p=0.6, by binomial calculation) and appear to meet the bar when it actually doesn’t ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=proportions%20being%20subject%20to%20high,test%20cases%20in%20my%20example)). 

- **Effect size in context:** In psychometrics or benchmarking contexts, an effect that requires 20 vs 200 samples to detect is considered **huge**. Most interesting improvements (say one AI model vs another) are more subtle and require larger *n* to resolve. A rule of thumb: with n=20, margin of error on a proportion is on the order of ±20% (as we saw). With n=100, it shrinks to ~±10%; with n=400, ±5%, etc. So if you truly need a precise verification that performance >75% with high confidence, you’d likely want dozens or hundreds of samples, not just 20 ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=,intervals%20for%20your%20performance%20estimates)). Researchers in similar settings note that *for comparing classification methods, you’d want “several hundred” samples per method* – with 20, differences could easily be due to chance ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=,intervals%20for%20your%20performance%20estimates)).

In practice, acknowledge this limitation. If your grader gets, say, 78% on 20 samples, it *suggests* it’s around the target but you can’t be sure it’s not actually lower. If it gets 95%, you have more cushion. If it gets 60%, it’s clearly underperforming. So use a **common-sense effect size lens**: only a **big shortfall or big exceedance** is actionable given the noise. Moderate differences will require more data or further validation.

## Sequential Testing to Optimize Sample Use

When human labeling is precious, **sequential testing** can help you stop early. The idea of sequential analysis is to look at the data as it comes in and decide whether you’ve seen enough to reach a conclusion or should gather more. It’s well-suited when you have an upper cap (like 20 samples) but could stop sooner if evidence allows.

- **Simple pass/fail boundaries:** Since your success criterion is 75% agreement (15 out of 20), you can set up **stopping rules**:
  - **Stop for failure:** If at any point the grader has made too many mistakes to possibly reach 15 correct, you can stop and declare it won’t meet the bar. For example, if after 12 samples the grader has already 6 wrong (i.e. only 6/12 correct), even if it got all remaining 8 correct, the maximum it could reach is 14/20 – below 75%. You may as well stop at that point ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)) and save the effort on the remaining 8 – the grader clearly won’t pass.
  - **Stop for success:** Conversely, if at some point the grader has already secured the needed number of correct answers, you might stop. For instance, if after 18 samples the grader has 15 correct, even if it got the last 2 wrong it would end up exactly at 15/20 = 75%. If you consider 75% as meeting the bar, you could stop at 18. (If you require “>75%” strictly, you’d actually want 16 correct; in that case you’d stop early only if the grader hits 16 correct before reaching 20.) Stopping as soon as the criterion is mathematically locked in can save a couple of samples in lucky cases.

- **Sequential Probability Ratio Test (SPRT):** The above intuitive rules are a special case of Wald’s SPRT, which is a formal method for sequential hypothesis testing. SPRT sets two hypotheses (H0 and H1, e.g. H0: *p* = 0.75, H1: *p* = 0.90) and then defines boundaries to accept or reject H0 as data accrues ([ucb-stat-159-s21.github.io](https://ucb-stat-159-s21.github.io/site/Notes/sprt.html#:~:text=be%20greater%20than%20,More%20generally%2C%20we%20have)) ([ucb-stat-159-s21.github.io](https://ucb-stat-159-s21.github.io/site/Notes/sprt.html#:~:text=the%20resulting%20likelihood%20ratio%20will,More%20generally%2C%20we%20have)). Wald showed that **SPRT is the most sample-efficient test for a given error rate** – it minimizes the average number of samples needed ([shinjaehyeok.github.io](https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/stcd-tutorial/#:~:text=If%20there%20are%20only%20two,beta)). In practice, using SPRT means after each new rating, you compute a likelihood ratio and check if it’s high enough to accept H1 (grader is good) or low enough to accept H0 (grader is not good) ([ucb-stat-159-s21.github.io](https://ucb-stat-159-s21.github.io/site/Notes/sprt.html#:~:text=the%20resulting%20likelihood%20ratio%20will,More%20generally%2C%20we%20have)) ([ucb-stat-159-s21.github.io](https://ucb-stat-159-s21.github.io/site/Notes/sprt.html#:~:text=For%20any%20,beta%5C%29%20against%20the%20alternative%20%5C%28H_1)). If neither, you continue and take another sample. The test guarantees the false-alarm and miss rates do not exceed chosen α, β thresholds, and often **saves ~50% of samples on average** compared to fixed-N testing ([www.isixsigma.com](https://www.isixsigma.com/capability-indices-process-capability/50-sampling-savings-sequential-test-method/#:~:text=the%20outcome%20of%20the%20observations,reject%20the%20test%20hypothesis)).

- **Applying sequential test to our case:** You could set H0: *p* = 0.75, H1: *p* = 0.90 (for example), and choose, say, α = β = 0.1 (10% error rates). The SPRT will give you stopping boundaries (numbers of correct vs wrong at each step) at which you stop. Alternatively, use the simple logic above with a bit of buffer. For instance, decide in advance: *“We will draw critiques one by one until either the grader gets 6 wrong (failing) or 16 right (passing), or we hit 20 total.”* These numbers (6 wrong or 16 right) essentially implement an SPRT-like boundary (they correspond to the scenarios we discussed). This procedure controls errors roughly near the desired rates and uses at most 20, but potentially fewer. If the grader is very good, you might pass it in, say, 16 samples. If it’s very bad, you might fail it in as few as, say, 12-15 samples. If it’s borderline, you’ll likely go the distance to 20.

- **Caution with sequential decisions:** If you choose sequential stopping, **plan the criteria in advance** and stick to them. You can’t peek at the data arbitrarily without adjusting error probabilities. The nice thing about SPRT is it inherently accounts for continuous monitoring – you don’t pay a penalty for stopping early when using the proper boundaries ([www.isixsigma.com](https://www.isixsigma.com/capability-indices-process-capability/50-sampling-savings-sequential-test-method/#:~:text=Historically%2C%20sequential%20testing%20results%20from,The%20basis%20for%20Wald%E2%80%99s)) ([shinjaehyeok.github.io](https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/stcd-tutorial/#:~:text=If%20there%20are%20only%20two,beta)). In contrast, if you were to look at the results after each sample and stop on a whim, you’d inflate the chance of a false conclusion (“p-hacking” by early stopping). So either use a formal SPRT design or adopt the straightforward rule “stop if success impossible or guaranteed” to maintain logical consistency.

- **Example outcome:** Imagine the grader’s true accuracy is 85%. In an SPRT, there’s a good chance you’d stop before 20 – maybe the grader gets to 16–0 (16 correct, 0 wrong) and you’ve already met the pass boundary. Conversely, if the true accuracy is 60%, you might hit the fail boundary of 6 wrong long before 20. If the true accuracy is right near 75%, you’ll likely end up using all 20 because it will teeter on the decision boundary. That’s fine – sequential testing ensures you spent as little labeling effort as needed to reach a confident decision in each scenario ([shinjaehyeok.github.io](https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/stcd-tutorial/#:~:text=If%20there%20are%20only%20two,beta)).

In summary, **sequential testing is worth considering** to conserve labeling effort. It requires a bit more setup (decide your boundaries and error tolerance up front), but with only 20 samples total, even saving a few matters. The theory behind it (Wald, 1945) guarantees that, on average, it uses the fewest samples for the same confidence level ([shinjaehyeok.github.io](https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/stcd-tutorial/#:~:text=If%20there%20are%20only%20two,beta)). Just be sure to predefine the stopping rule to keep the test statistically sound. 

---

**In conclusion:** With ~20 human-rated samples, focus on robust metrics (use kappa or F<sub>1</sub> as needed to account for chance and class imbalance), always report uncertainty (confidence intervals or Bayesian credible intervals), and don’t over-interpret small differences ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=You%27ll%20notice%20that%20there%20is,perfect)). Bootstrap to get error bars on complex metrics, and consider a sequential strategy to possibly reduce the sample cost. Recognize that any result will have a lot of wiggle room – >75% agreement on 20 samples is not a guarantee without further evidence. The key is to use these statistical tools to quantify “how sure are we?” and be transparent about the uncertainty. As a rule, **the agreement of a single LLM grader with a researcher on just 20 critiques should be taken as a preliminary indicator**, not final proof – but with careful statistical validation, you can at least ensure that those 20 data points are squeezed for all the insight they can offer. ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=,test%29%20sample%20size)) ([stats.stackexchange.com](https://stats.stackexchange.com/questions/86238/is-cross-validation-still-valid-when-the-sample-size-is-small#:~:text=,intervals%20for%20your%20performance%20estimates))