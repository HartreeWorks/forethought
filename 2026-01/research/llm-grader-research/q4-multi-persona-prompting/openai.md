# OpenAI Deep Research: q4-multi-persona-prompting

**Model:** o3-deep-research-2025-06-26
**Generated:** 2026-01-20T14:25:54.236Z
**Response ID:** resp_0c4a9c2ae91918c200696f8f08197081938f111da44c60f573

---

# Multi-Persona Prompting for Critique Generation – Does It Boost Quality or Diversity?

## More Diverse Critiques via Multiple Perspectives

Prompting an LLM with different expert **personas** can indeed produce a broader range of critique points. Studies on creative ideation show that role-play prompts yield **more diverse outputs** than generic prompts. For example, prompting a model in parallel as a *mechanical engineer*, *biologist*, *artist*, etc., led to a wider variety of design concepts compared to a single prompt with no persona ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=professional%20personas%2C%20and%20,and%20update%20design%20concepts%20gradually)). Similarly, research on synthetic data generation found that simply assigning a persona (even a basic label like “doctor” or “historian”) increases lexical diversity of the generated text compared to no persona prompt ([aclanthology.org](https://aclanthology.org/2025.findings-emnlp.1146/#:~:text=different%20sizes%20with%20fine,length%20cutoff%20in%20the%20prompt)). In short, personas push the model to tap into different knowledge and styles, reducing the chance that all critiques sound the same.

Crucially, **each persona brings its own frame of reference**. An economist persona might highlight incentive or market issues the author missed, whereas a philosopher persona could question ethical assumptions. In practice, Forethought’s research topics span philosophy and economics, so using both personas could cover more angles. Evidence suggests the greatest diversity gains come from either **parallel prompts** (each persona produces an independent critique) or a **sequential approach** (one persona critiques, then another builds or adds new critiques), rather than mashing personas together all at once ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=and%20personas%2C%20and%20designing%20the,and%20update%20design%20concepts%20gradually)). In one experiment, giving the model multiple personas *simultaneously* (“collective prompting”) was actually less effective – likely because the model blended voices and lost distinct perspectives ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=design,step%20mostly%20using%20the%20professional)). To maximize diversity, it’s better to let each persona speak separately or in turn.

## Impact on Critique Quality and Thoroughness

Beyond diversity, there’s promising evidence that multi-persona prompting can **improve the quality and accuracy** of outputs on complex reasoning tasks. By having multiple “agents” or personas discuss and critique a problem, an LLM can catch its own errors and refine answers in ways a single-pass response might not. Recent research on *multi-agent debate* shows that two or three AI personas debating each other often produce more correct and well-justified answers ([www.emergentmind.com](https://www.emergentmind.com/articles/2305.14325#:~:text=The%20results%20consistently%20show%20that,incorrect%20answers%20to%20reach%20the)). For instance, one study found that when three model instances answered a question and then critiqued each other’s answers, they ended up correcting many initial mistakes – reaching a consensus that outperformed a single model’s answer (even a single model using self-reflection) ([www.emergentmind.com](https://www.emergentmind.com/articles/2305.14325#:~:text=The%20results%20consistently%20show%20that,incorrect%20answers%20to%20reach%20the)). In tasks requiring logic or factual accuracy, this “society of minds” approach significantly enhanced reasoning and reduced hallucinations ([proceedings.mlr.press](https://proceedings.mlr.press/v235/du24e.html#:~:text=indicate%20that%20this%20approach%20significantly,Our%20approach%20may%20be)). Essentially, one persona can notice flaws or overlooked details in another’s critique, leading to a more thorough final evaluation of the research.

Applied to our use case, **multi-persona critiques could surface more valid issues** in a research paper. One persona might question a macroeconomic model’s assumptions while another challenges the philosophical consistency – together providing a more **comprehensive review**. That said, quality gains from persona prompting seem most pronounced when the task benefits from cross-examination or varied reasoning. In straightforward NLP tasks (like factual Q&A or basic text classification), adding a persona has shown *minimal impact on performance* ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=prompts%20provided%20to%20LLMs%20can,is%20worth%20noting%20that%20these)). In fact, one study found that for standard question-answering, telling the model to be a certain persona didn’t change accuracy much – it mostly just changed the stylistic tone. So the benefit for “quality” depends on the nature of the task: for critical analysis and reasoning (our domain), personas can help by introducing multi-faceted thinking, whereas for tasks with a single correct answer, personas won’t magically improve correctness.

## Choosing and Describing Personas

An important practical question is how to **specify personas** in the prompt. The good news is that you often don’t need an elaborate backstory – concise cues like “As a veteran economist, …” or “From a philosopher’s perspective, …” can be enough for the model to shift style and focus. Research on persona-based text generation found that adding *fine-grained details* to a persona (e.g. a full biography or very specific traits) yields little additional diversity or benefit beyond what a simple role label provides ([aclanthology.org](https://aclanthology.org/2025.findings-emnlp.1146/#:~:text=different%20sizes%20with%20fine,length%20cutoff%20in%20the%20prompt)). In other words, *“Think like an economist” vs. a paragraph describing the economist’s entire career* made minimal difference in output diversity in those tests. The language model already has prior knowledge of how an “economist” might talk or what they’d emphasize, so a brief tag is usually effective.

That said, domain context can guide what details matter. If the critique needs a certain tone or sub-specialty, a bit more specificity can help (“a behavioral economist concerned with inequality” might steer the critique to those issues). The literature suggests starting simple: use a clear role name or title, and only add specific attributes if you find the output isn’t adopting the desired viewpoint. In one design ideation study, researchers simply prefaced prompts with titles like “Mechanical Engineer:” and it successfully steered the model’s knowledge usage ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=range%20of%20professional%20personas,domain%2C%20and%20their%20associated%20cognitive)). They note that future work could try richer persona descriptions (e.g. specifying the persona’s knowledge base or cognitive style) for potentially more nuanced outputs ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=range%20of%20professional%20personas,domain%2C%20and%20their%20associated%20cognitive)). For now, a short role description works well – it avoids overwhelming the model with too much persona detail, which could even be counterproductive if it constrains the model unnaturally. 

In summary, **start with high-level personas** that align with the expertise you want. Forethought’s topics naturally suggest personas like “longtermist philosopher,” “AI policy expert,” “skeptical statistician,” etc. You can always iterate if the persona’s critiques seem off-target. Keep in mind that the model’s interpretation of a persona may include some clichés: e.g. an “AI ethicist” persona might over-index on ethical principles. If that happens, you can refine the prompt (or choose a different angle) to get the style you want.

## Debating and Combining Persona Critiques

One intriguing approach is to not only generate separate critiques from each persona, but also have the personas **interact or debate**. This moves into the territory of multi-agent systems: essentially letting the LLM run an internal panel discussion. In practice, this could mean Persona A writes a critique, Persona B responds or adds counterpoints, Persona C weighs in, and so on, before forming a consolidated evaluation. Such structured debate can improve the **depth and reliability** of the critique, as each persona can vet others’ arguments. For example, if the “philosopher” overlooks a practical issue, the “economist” might point it out, or if one persona makes a factual claim, another can fact-check it. Research confirms that engaging multiple agents in iterative critique–revise cycles can yield better outcomes than a single-pass response. Du et al. (2024) demonstrated that a debate protocol with three GPT-4 agents led to higher problem-solving accuracy and fewer reasoning errors, compared to a single agent ([proceedings.mlr.press](https://proceedings.mlr.press/v235/du24e.html#:~:text=indicate%20that%20this%20approach%20significantly,Our%20approach%20may%20be)). Essentially, **agents acting as peer reviewers for each other** produce a more polished final answer.

However, implementing multi-persona debate requires careful prompting to be effective. If you simply tell the model “Now debate between an economist and philosopher,” you might get a lively dialogue, but it isn’t guaranteed to converge on truth or even stay on task. Successful multi-agent frameworks impose some structure: e.g. each agent makes an initial statement, then they take turns critiquing one specific point, and perhaps a final “moderator” summarizes. Without structure, there’s a risk of the dialogue going in circles or fixating on minor points. Notably, one analysis found that **unstructured debates can devolve into agreement bias**, where agents start conceding to each other’s claims even when those claims are wrong ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Unstructured%20debate%20,incorrect%E2%86%92correct%20over%20one%20round)). In fact, in a naive debate setting with multiple LLM agents, correct answers were sometimes overturned by incorrect but confident arguments from a peer – a form of AI groupthink ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Unstructured%20debate%20,incorrect%E2%86%92correct%20over%20one%20round)). To avoid this, one agent could be assigned as a **skeptic or fact-checker** whose role is to verify claims, or you can require that each round of critique focuses on finding flaws (not just agreeing).

**Multi-persona debates can improve quality, but they need refereeing.** Some studies introduce a “judge” model or voting mechanism at the end to decide which argument was most convincing or correct ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Decision%20aggregation%20strategies%20include%20majority,bounded%20number%20of%20refinement%20rounds)). In our grading use case, this could map to using a final LLM (or heuristic) to pick the most insightful critique from the persona outputs. The bottom line is that persona-vs-persona dialogues are powerful for exposing weaknesses (each persona naturally challenges the others’ blind spots), yet they should be constrained with clear instructions. For example, you might prompt: *“Persona A, give your critique. Persona B, identify any errors or overlooked issues in A’s critique and expand.”* A round or two of that is typically enough – studies find diminishing returns after about 1–2 critique-and-revision cycles ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Studies%20consistently%20find%20that%20one,11%20Jul%202025)). Beyond that, the discussion can start to **repeat or “over-correct”** perfectly valid points ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Central%20meta,credibility%20tracking%20remain%20open%20challenges)). So a practical recommendation is to use at most a couple rounds of persona interaction, then finalize the critiques.

## Failure Modes and Pitfalls of Persona Prompting

While persona prompting is promising, it’s not a free lunch – there are notable failure modes to be aware of:

- **Stereotyped or superficial responses:** If a model doesn’t have deep knowledge for a given persona, it might fall back on surface-level tropes. For instance, a "philosopher" persona might produce overly abstract platitudes, or a "skeptic" persona might just negate everything without substantive reasoning. Research has noted that LLMs, when assigned socio-demographic personas, can mirror or amplify stereotypes present in their training data ([www.researchgate.net](https://www.researchgate.net/publication/381125818_Two_Tales_of_Persona_in_LLMs_A_Survey_of_Role-Playing_and_Personalization#:~:text=match%20at%20L3068%20deep,biases%20found%20in%20LLMs)). In our context, that might manifest as an *“economist”* critique focusing solely on monetary aspects even when irrelevant, simply because the model thinks “that’s what economists do.” Mitigation: choose personas tied to an area where the model is likely well-informed, and consider nudging it with context. Also, explicitly ask for “rigorous justification” to push beyond clichés.

- **Persona confusion and blending:** When using multiple personas simultaneously, the model can get confused about its voice. If you ask for a critique from *multiple* roles in one response, the LLM might merge them or oscillate chaotically. The **collective prompting** strategy (all personas at once) tended to be less effective in studies, likely due to this blending effect ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=design,step%20mostly%20using%20the%20professional)). It’s usually safer to partition personas – e.g. separate outputs or clearly labeled sections for each persona’s thoughts. Similarly, if you do a debate, always indicate which persona is “speaking” at each turn to keep the voices distinct.

- **Loss of coherence or focus:** A persona acting in isolation might fixate on its domain perspective to the detriment of the overall critique. For example, the “policy analyst” persona may dwell on regulatory details even if the bigger issue is technical feasibility. Without a unifying vision, you risk **fragmented critiques** that each stay in their silo. One way to counter this is to have a final integration step (either done by an LLM or a human editor) that **synthesizes the persona-specific critiques** into a coherent overall review. In the absence of that, be aware that each persona’s output is intentionally partial. This isn’t necessarily “bad” – it might be exactly what we want (diverse partial insights) – but it means a single persona’s critique might miss the forest for the trees.

- **Groupthink and persuasion bias:** When personas interact, there’s a danger that one strong-willed persona (or a convincingly phrased but incorrect argument) sways the others. We see analogues of human groupthink: one agent might overly defer or “agree” with another agent’s flawed point, leading the whole debate astray ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Positive%20Synergy%20of%20Diversity%3A)) ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Unstructured%20debate%20,incorrect%E2%86%92correct%20over%20one%20round)). In one experiment, over half the time an initially correct answer was changed to an incorrect one after agents debated, due to persuasive but wrong arguments ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Unstructured%20debate%20,incorrect%E2%86%92correct%20over%20one%20round)). This failure mode suggests that **debate protocols need checks** – for instance, requiring each critique to provide evidence, or using an external fact-check on contested claims. If we just rely on the personas themselves to self-correct, occasionally they’ll reinforce each other’s errors (an echo chamber effect). 

- **False confidence of expertise:** A persona prompt can imbue the output with a confident tone (“As a policy expert with 30 years experience, I assure you…”). This can be double-edged: while it lends authority to the critique, the content might still be incorrect or unjustified. Users (or even the graders) might be more likely to trust a critique that *sounds* expert without scrutinizing it. Ethically, we should remember the LLM isn’t actually an economist or philosopher – it’s play-acting one. Experts have warned about this “**illusion of authority**”; if the persona output contains an error, it might be harder to spot because the style is convincing ([www.forbes.com](https://www.forbes.com/sites/lanceeliot/2023/07/20/prompt-engineering-amplified-via-an-impressive-new-technique-that-uses-multiple-personas-all-at-once-during-your-generative-ai-session/#:~:text=personas%20or%20fake%20roles,AI%20is%20an%20expert%20in)). As a mitigation, you might instruct the LLM persona to cite sources or acknowledge uncertainty when appropriate, to avoid blustery but hollow assertions.

Being mindful of these pitfalls can help in crafting prompts and filters. For example, to avoid stereotypes, test the persona on a neutral prompt first (see if it outputs stereotypes or balanced reasoning). To combat coherence issues, consider a final prompt like “Summarize the key critiques from all these perspectives,” which the LLM can do to ensure nothing crucial contradicts. And for groupthink, using a final verification step (maybe a non-persona “referee” prompt that checks each critique’s logic) can catch issues.

## Evaluating Persona-Generated Critiques

When it comes to **evaluating the critiques** produced by multi-persona prompting, the fundamental criteria remain similar – clarity, relevance, insightfulness, and accuracy of the critique. However, there are a few additional considerations given the persona setup:

- **Evaluate each critique on its own merits:** If you generate, say, four persona-based critiques, it’s useful to assess each individually *and* in aggregate. Individually, does the economist’s critique make valid economic points? Does the philosopher’s critique identify a novel ethical concern? Each should be judged by how well it fulfills a “good critique” in that domain. In fact, you might even use slightly customized rubrics – e.g., for the technical persona, you’ll weight factual accuracy and technical relevance more, while for a futurist persona you might expect more speculative but horizon-expanding ideas. Since we ultimately want the **best overall critiques**, each persona’s contribution should be seen as one candidate in the pool.

- **Measure diversity and coverage:** One reason we use personas is to increase the diversity of feedback. Thus, an evaluation of the *set* of critiques should consider whether they cover **complementary aspects**. You wouldn’t want four personas all pointing out the same minor flaw. There are metrics from research for this: for example, measuring semantic similarity or overlap between critiques to quantify redundancy. In the design-ideation study, they evaluated diversity by checking how much each persona’s ideas drew from that persona’s unique knowledge base versus overlapping with others ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=reveal%20that%20LLMs%20generate%20more,of%20the%20current%20step%20alone)). For us, a simple check could be counting how many distinct issues were raised across all persona critiques. If persona prompting is working, we expect higher issue coverage than a single generic critique would have.

- **Awareness of stylistic differences:** Persona outputs might differ in tone and length. A skeptic might write a terse, critical paragraph; a professor persona might deliver a verbose essay. This means if you use an **LLM-based grader**, it mustn’t unfairly favor one style over another (e.g., longer critiques might seem more detailed and get higher scores by a naive length-based metric). To counter this, you might instruct the grader to focus on substance over style. If humans are evaluating, simply prepare them that different voices will come through. We want them to discern the value of the content, not the eloquence per se.

- **Combine automated and human evaluation if possible:** Given our small sample of human-rated critiques (~20) for validation, we’ll likely rely on an LLM grader for scaling. It could be wise to have the grader also perform a **“collective” evaluation**: after scoring individual critiques, have it identify if a particular critique brought up a point none of the others did. This could be an indicator of a valuable unique insight. Some multi-agent systems use a form of *self-voting*, where the AI agents vote on the best answer among them ([openreview.net](https://openreview.net/forum?id=N58BZj5JB7#:~:text=and%20that%20they%20can%20reason,methods%20by%20a%20large%20margin)). We could mimic that by asking the LLM (in a separate run) something like, “Which one of these critiques is the most insightful and why?” Such meta-evaluation might surface the standout critique reliably, or at least serve as a check against our grader’s ranking.

Importantly, if we find the multi-persona method yields very **heterogeneous critiques**, we might adjust our evaluation expectations. A generic single critique might be expected to cover A, B, and C moderately well. In contrast, persona critiques might have one that covers A in depth, another B in depth, etc., with little overlap. In that case, a fair evaluation might acknowledge that each persona output is intentionally narrower. We shouldn’t penalize the philosopher persona for not discussing economic growth models – that’s the economist’s job. So if using a numeric scoring rubric for “comprehensiveness,” apply it within the scope of that persona’s domain. One way is to prepend context to the grader: e.g. “You are evaluating a critique written from a philosophical perspective; assess its quality in identifying philosophical issues, clarity of argument, and originality.” Tailoring the evaluation context in this way ensures each persona is evaluated by *the standards of that perspective*, which is more meaningful.

Lastly, keep an eye on whether the grader itself might exhibit bias towards certain personas. It’s possible an LLM grader (if not instructed otherwise) might consistently give higher scores to, say, the most eloquent writing or the persona that aligns with the grader’s training data biases. To mitigate this, explicitly instruct the grader to value **novel insight, correctness, and relevance**. We want it to recognize a sharp but simply worded critique as superior to a flowery but vacuous one. If needed, we could even use multiple graders – e.g. a “philosopher grader” and an “economist grader” for the respective critiques – but that may be overkill. The key is to ensure the evaluation process remains **aligned with our goal: finding the critiques that are most useful to the research team**, whether they came from persona X or Y.

## Key Takeaways and Recommendations

- **Persona prompting can enhance diversity:** Adopting multiple personas (economist, philosopher, etc.) tends to yield a wider range of critique points ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=and%20personas%2C%20and%20designing%20the,and%20update%20design%20concepts%20gradually)). This helps uncover angles a single voice might miss.

- **Quality may improve with structured dialogue:** Letting personas critique and refine each other’s outputs can improve reasoning and catch errors, as seen in multi-agent debate research ([www.emergentmind.com](https://www.emergentmind.com/articles/2305.14325#:~:text=The%20results%20consistently%20show%20that,incorrect%20answers%20to%20reach%20the)) ([proceedings.mlr.press](https://proceedings.mlr.press/v235/du24e.html#:~:text=indicate%20that%20this%20approach%20significantly,Our%20approach%20may%20be)). Use a structured approach (turn-taking, a final arbiter) to avoid chaotic discussions.

- **Use simple persona cues:** In most cases, a short role description (“As a policy analyst, ...”) is sufficient to invoke the persona’s perspective. Fine-grained backstories aren’t usually necessary and yield diminishing returns ([aclanthology.org](https://aclanthology.org/2025.findings-emnlp.1146/#:~:text=different%20sizes%20with%20fine,length%20cutoff%20in%20the%20prompt)). You can always add detail if the persona’s output needs tuning.

- **Watch for pitfalls:** Be alert to persona outputs that are stereotyped, off-focus, or overly agreeable to each other. Limit the number of debate rounds (usually 1–2 is enough) to prevent convergence on a wrong idea or endless nitpicking ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Studies%20consistently%20find%20that%20one,11%20Jul%202025)) ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Central%20meta,credibility%20tracking%20remain%20open%20challenges)). If one persona consistently overpowers others in debate, consider balancing the prompt or using a voting mechanism to ensure no single perspective suppresses valid critiques from others.

- **Evaluate with context in mind:** When scoring or selecting critiques, account for the fact that each persona had a particular remit. Evaluate how well they did *within that remit* and also consider the uniqueness of their contribution across all critiques. Diversity of thought is a feature here, so an ideal outcome is that each persona’s critique excels in its domain and together they provide a well-rounded evaluation of the research.

By leveraging multi-persona prompting thoughtfully, we can generate critique sets that are **richer and more robust** than one-dimensional feedback. The evidence suggests a real benefit in our philosophically and economically complex domain – as long as we manage the process to avoid known failure modes. This should ultimately feed into our LLM grading pipeline by giving it higher-quality content to judge, and with the grader calibrated appropriately, we stand a good chance of exceeding that 75% agreement benchmark with human assessments. 

**Sources:**

1. Feng, W. et al. (2025). *Enhancing design concept diversity: multi-persona prompting strategies for large language models.* **Design Science, 11**, e52. (Finding that parallel or sequential prompts with different professional personas yield more diverse ideas) ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=professional%20personas%2C%20and%20,and%20update%20design%20concepts%20gradually)) ([www.cambridge.org](https://www.cambridge.org/core/product/3B346E253508337A4EE899499BE49D9B/core-reader#:~:text=design,step%20mostly%20using%20the%20professional))

2. Kambhatla, G. et al. (2025). *Measuring Lexical Diversity of Synthetic Data Generated through Fine-Grained Persona Prompting.* Findings of EMNLP 2025. (Finding that persona prompts increase lexical diversity, and that detailed persona backstories add minimal extra diversity) ([aclanthology.org](https://aclanthology.org/2025.findings-emnlp.1146/#:~:text=different%20sizes%20with%20fine,length%20cutoff%20in%20the%20prompt))

3. Du, Y. et al. (2024). *Improving Factuality and Reasoning in Language Models through Multiagent Debate.* ICML 2024. (Demonstrating that multiple LLM agents debating answers can correct mistakes and outperform single-model reasoning) ([www.emergentmind.com](https://www.emergentmind.com/articles/2305.14325#:~:text=The%20results%20consistently%20show%20that,incorrect%20answers%20to%20reach%20the)) ([proceedings.mlr.press](https://proceedings.mlr.press/v235/du24e.html#:~:text=indicate%20that%20this%20approach%20significantly,Our%20approach%20may%20be))

4. Emergent Mind AI Research. “Multi-Agent Critique & Revision.” (2025). *Summary of multi-agent debate frameworks and their performance.* (Notes that diversity in agent reasoning improves error detection, but also discusses failure modes like agreement bias and over-correction in debates) ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Positive%20Synergy%20of%20Diversity%3A)) ([www.emergentmind.com](https://www.emergentmind.com/topics/multi-agent-critique-and-revision#:~:text=Unstructured%20debate%20,incorrect%E2%86%92correct%20over%20one%20round))

5. Forbes (L. Eliot). “Prompt Engineering… Using Multiple Personas All At Once” (2023). (Discussion of multi-persona prompting conceptually – highlights that more personas can help but don’t guarantee better answers, cautions about trusting persona-simulated expertise) ([www.forbes.com](https://www.forbes.com/sites/lanceeliot/2023/07/20/prompt-engineering-amplified-via-an-impressive-new-technique-that-uses-multiple-personas-all-at-once-during-your-generative-ai-session/#:~:text=The%20gist%20is%20that%20the,It%20might%2C%20it%20might%20not)) ([www.forbes.com](https://www.forbes.com/sites/lanceeliot/2023/07/20/prompt-engineering-amplified-via-an-impressive-new-technique-that-uses-multiple-personas-all-at-once-during-your-generative-ai-session/#:~:text=personas%20or%20fake%20roles,AI%20is%20an%20expert%20in))

6. Qin, L. et al. (2024). *Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization.* arXiv 2023. (Survey noting that persona-based role-play can introduce biases and stereotypes if not managed) ([www.researchgate.net](https://www.researchgate.net/publication/381125818_Two_Tales_of_Persona_in_LLMs_A_Survey_of_Role-Playing_and_Personalization#:~:text=match%20at%20L3068%20deep,biases%20found%20in%20LLMs))

7. Lahoti, P. et al. (2023). *Improving Diversity of Demographic Representation in LLMs via Collective-Critiques and Self-Voting.* (Introduces a technique where an LLM critiques its own outputs for diversity and self-corrects) ([openreview.net](https://openreview.net/forum?id=N58BZj5JB7#:~:text=datasets%20and%20propose%20metrics%20to,methods%20by%20a%20large%20margin))

