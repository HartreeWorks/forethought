# Research context: Returns to prompt engineering

## Background

This research is being conducted to inform a strategic decision for Forethought Research, a nonprofit focused on navigating the transition to superintelligent AI.

## About Forethought Research

Forethought Research works on conceptual and philosophical questions—not empirical science. Their research areas include:

- How to control increasingly powerful AI systems
- How to ensure abundance is shared fairly
- How AI can improve collective reasoning and coordination
- What rights to give digital minds
- Sketching paths to flourishing long-term futures

Their researchers use LLMs for tasks like:
- Generating critiques of philosophical papers
- Brainstorming arguments and counterarguments
- Synthesising positions across literature
- Stress-testing reasoning
- Drafting conceptual analyses

The outputs need to be **incisive and load-bearing**—generic or shallow responses have little value.

## The decision at stake

Forethought is deciding whether to invest in systematic prompt engineering expertise. The key question is:

**Should they hire a specialist to do systematic prompt iteration and chain development, or can researchers "wing it" with basic prompting and capture most of the available value?**

This depends on:
1. How large are the gains from prompt engineering?
2. What types of tasks benefit most?
3. Does chaining/orchestration add value beyond single prompts?
4. Are prompt engineering gains increasing or decreasing as models improve?

## What we need

Empirical evidence on returns to prompt engineering, with specific attention to:
- Philosophical reasoning and argumentation tasks
- Creative analytical work
- Quality of outputs (not just accuracy on benchmarks)
- Practical implications for investment decisions
