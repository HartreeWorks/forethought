# OpenAI Deep Research: query-a-do-prompts-matter

**Model:** o3-deep-research-2025-06-26
**Generated:** 2026-01-23T15:49:31.873Z
**Response ID:** resp_081c7f25f279ebec006973973aa350819f9ffe17027bf53629

---

### Q1: Magnitude of gains from prompt engineering

**Summary of key findings:** Empirical research shows that effective prompt engineering can **significantly boost performance** on many tasks, though the size of gains varies widely by technique and domain. For *reasoning-intensive problems*, prompt strategies like *Chain-of-Thought (CoT)* prompting have yielded especially large improvements. For example, combining CoT prompting with a large 540B-parameter model set a new state-of-the-art on the GSM8K math benchmark (58% accuracy vs. the prior 55% fine-tuned SOTA), and using self-consistency (majority voting over multiple reasoning paths) pushed this further to 74% ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)). Such **step-by-step prompting** has turned multi-step math or logic queries from near-failure to majority success in some cases. Few-shot exemplars (providing examples in the prompt) also consistently improve accuracy over zero-shot baselines; in the original GPT-3 paper, many tasks saw double-digit percentage point jumps with a few examples compared to a basic prompt (e.g. significant gains on QA and translation benchmarks). Outside of benchmarks, industry case studies find non-trivial productivity boosts: for coding tasks, well-crafted prompts and code-specific examples have been documented to raise developer productivity by roughly **20–55%** in real workflows ([gl0bal01.com](https://gl0bal01.com/blog/prompt-engineering-developers-research-evidence#:~:text=The%20field%20of%20prompt%20engineering,evidence%20and%20quantitative%20performance%20metrics)). In general, **moderate gains (20–50%)** are common with solid prompt techniques, and **occasionally very large gains (50%+)** are reported on narrowly focused tasks where baseline performance was low and prompt guidance unlocks a new capability ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)).

That said, **not all prompt tweaks yield big returns**. The benefits depend on the technique and context. Some prompting methods only give **small improvements (~5–15%)** or even no measurable gain on certain benchmarks. For instance, simply prepending *persona or role instructions* (“You are an expert in ___”) tends to **produce negligible accuracy improvement on factual tasks** ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D)). A rigorous 2025 study tested prompts like “Act as a physics expert” on hundreds of academic questions and found **no consistent boost in correctness** – the model didn’t become more accurate just from the expert persona ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D)). In some cases, superficial prompts can even **hurt performance** by introducing confusion or verbosity. Overall, the evidence indicates that **prompt engineering can yield substantial gains, but only with the right methods applied to the right problems**. Techniques that guide the model’s reasoning (structured steps, examples, etc.) tend to add far more value than style or persona changes for purely factual accuracy ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D)). Moreover, the magnitude of improvement often **plateaus with the strongest models**. Newer top-tier LLMs already perform complex reasoning internally, so advanced prompting gives diminishing returns there (sometimes only 0–5% gains, while adding more tokens and latency) ([slashpage.com](https://slashpage.com/haebom/7vgjr4m1nq9gj2dwpy86#:~:text=%EB%B3%B8%20%EB%85%BC%EB%AC%B8%EC%9D%80%20AI%20%ED%99%9C%EC%9A%A9%EC%97%90%20%EB%8C%80%ED%95%9C,%ED%95%84%EC%9A%94%ED%95%9C%20%EC%8B%9C%EA%B0%84%EA%B3%BC%20%ED%86%A0%ED%81%B0%EC%9D%80%20%ED%81%AC%EA%B2%8C%20%EC%A6%9D%EA%B0%80%ED%95%A9%EB%8B%88%EB%8B%A4)). In summary, **systematic prompt engineering has delivered moderate-to-large performance boosts in many evaluations**, but the payoff is highly variable – trivial prompts won’t magically fix hard tasks, and gains are more pronounced on some tasks (reasoning, coding, etc.) than others.

**Strength of evidence:** **Moderate.** There is **credible empirical support** for substantial prompt engineering benefits on certain tasks, derived from both peer-reviewed studies and documented benchmarks. Flagship papers (e.g. Wei et al. 2022 on CoT prompting) and follow-ups provide hard numbers for performance jumps ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)). Multiple studies and industry experiments concur that carefully designed prompts improve outcomes, which lends confidence. However, the evidence is **not uniform across all tasks** – some findings (like the persona prompt study) show null or negative results ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D)). Many reports focus on specific benchmarks (math problems, code completion, etc.), so **generalization is somewhat uncertain**. Overall, we have **strong evidence for certain techniques on certain tasks**, but only **anecdotal or weak evidence for others**, especially for more qualitative “insight” tasks. This means the consensus on prompt engineering’s ROI is still evolving and context-dependent.

**Key sources:** 

1. **Wei et al. (2022) – Chain-of-Thought prompting**: Demonstrated that adding step-by-step reasoning in prompts dramatically improves accuracy on math word problems and logical reasoning tasks ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)).  *Key result:* a few CoT examples enabled a 540B model to reach 58% on GSM8K (vs ~10-20% with standard prompting) and 74% with self-consistency voting, far exceeding prior approaches ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)).

2. **Wharton AI Lab (Mollick et al., 2025) – Persona prompts study**: Found that *“Act as an expert”*-style prompts **do not reliably improve factual question answering accuracy ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D))**. Testing six advanced models on hundreds of expert-level questions showed no significant difference in correctness when adding matched expert personas, debunking a common prompt tip ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D)).

3. **Industry reports on coding assistants (2023–2025)**: Internal evaluations by companies like Microsoft and GitHub have observed **20–50% productivity increases** for developers using optimized prompts with code-specific LLMs ([gl0bal01.com](https://gl0bal01.com/blog/prompt-engineering-developers-research-evidence#:~:text=The%20field%20of%20prompt%20engineering,evidence%20and%20quantitative%20performance%20metrics)). These gains include faster coding, fewer errors, and better code suggestions when prompts are iteratively refined and chained (as opposed to ad-hoc “winging it”).

4. **Luong et al. (2023) – Prompting vs model power in medicine**: Evaluation on medical QA tasks showed **model choice mattered more than prompt complexity** ([www.sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0010482525009655#:~:text=Model%20capabilities%20proved%20more%20crucial,mini%20leading%20performance)). Simple CoT prompts were as effective as complex prompt schemes, and a stronger model with basic prompting beat weaker models even with advanced prompting ([www.sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0010482525009655#:~:text=Model%20capabilities%20proved%20more%20crucial,mini%20leading%20performance)). *Implication:* prompt engineering can only leverage existing model capabilities – it can’t brute-force a poor model to outdo a much better model.

**Implications for the decision:** The evidence suggests **prompt engineering can unlock notable performance and quality gains**, but those gains are not automatic for every task – they **require skill and experimentation** to achieve. In areas relevant to Forethought (complex reasoning, analysis, synthesis), the upside from advanced prompting appears **meaningful**. When tasks resemble the reasoning benchmarks (multi-step problems, coding, logical argumentation), a specialist can potentially improve output quality substantially (possibly on the order of 20-50% better outcomes, or turning a failing answer into a solid one) by using techniques like CoT, structured prompts, or multi-step chains. This means there is real value on the table that an expert *could* capture. However, the ROI is uneven: for some tasks, especially those more about factual recall or well-trodden knowledge, fancy prompts won’t help much beyond a straightforward ask. Also, as top-tier models get more capable, they need less hand-holding – the **marginal benefit of prompt tinkering may shrink with cutting-edge models** ([slashpage.com](https://slashpage.com/haebom/7vgjr4m1nq9gj2dwpy86#:~:text=%EB%B3%B8%20%EB%85%BC%EB%AC%B8%EC%9D%80%20AI%20%ED%99%9C%EC%9A%A9%EC%97%90%20%EB%8C%80%ED%95%9C,%ED%95%84%EC%9A%94%ED%95%9C%20%EC%8B%9C%EA%B0%84%EA%B3%BC%20%ED%86%A0%ED%81%B0%EC%9D%80%20%ED%81%AC%EA%B2%8C%20%EC%A6%9D%EA%B0%80%ED%95%A9%EB%8B%88%EB%8B%A4)). In sum, **there are significant gains available through skillful prompt engineering**, but one must target the right scenarios. This points toward **hiring a prompt specialist being beneficial if Forethought’s core tasks are in the categories that see large boosts (complex reasoning, creative generation, etc.)**, while being mindful that for some simpler queries a basic prompt is already enough. The decision should weigh that **expertise can likely turn *difficult, high-value queries* into much better outputs**, which seems aligned with Forethought’s need for “incisive, load-bearing” results – a strong argument *for* investing in prompt engineering capability. Conversely, if Forethought expects to rely on cutting-edge models and mostly straightforward Q&A, the relative advantage of intensive prompt engineering might be more limited. Overall, the current evidence leans toward **prompt engineering expertise paying off for non-trivial analytic tasks**, but expectations should be nuanced by task type.

---

### Q2: Task characteristics that predict prompt sensitivity

**Summary of key findings:** Some tasks are **far more sensitive to prompt design** than others. Broadly, tasks that require **complex, multi-step reasoning or constrained creativity** tend to benefit most from advanced prompting, whereas tasks focused on simple retrieval or straightforward classification see less improvement. Empirical analyses support this: a large-scale review of 1,500+ NLP studies found that **“complex reasoning tasks benefit more from sophisticated prompting compared to contextual understanding tasks.”** ([gl0bal01.com](https://gl0bal01.com/blog/prompt-engineering-developers-research-evidence#:~:text=Recent%20academic%20research%20reveals%20that,development%20velocity%2C%20and%20developer%20satisfaction)) In practice, this means domains like math word problems, logical puzzles, coding challenges, and multi-hop question answering are highly prompt-sensitive – using chain-of-thought steps, explicit sub-questions, or few-shot examples can **dramatically improve accuracy** on these tasks. For example, we see huge gains from CoT on math/logic benchmarks, while something like basic sentiment classification was already easy for the model and changes little with prompt tweaks. Even within QA, prompt sensitivity varies: models struggle with reasoning-heavy questions unless guided. Notably, one analysis showed **CoT had its *largest* impact on a sports trivia task**, boosting accuracy from 84% to 95% by prompting the model to reason stepwise ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=match%20at%20L107%20performance%20improved,vs%2084)). This illustrates that tasks requiring reasoning over *implicit* knowledge (in this case, figuring out a sports question through logic or context) gained more from prompting than those involving direct fact recall. On the flip side, if a task mostly involves regurgitating a known fact or a short answer, fancy prompts (or adding steps) often don’t help much – the model either *knows* the answer or it doesn’t. In those cases, prompt engineering might mainly affect style, not core accuracy.

Beyond raw accuracy, tasks that demand **creative or analytical depth** (as opposed to one-off answers) also seem prompt-sensitive in terms of output *quality*. While systematic studies here are fewer, anecdotal evidence and related research indicate that **prompt structure can shape the depth and nuance of responses** for open-ended tasks. For instance, in **philosophical reasoning or argumentative writing**, prompting the model to **consider multiple perspectives or follow a structured outline** might yield a more thorough and well-reasoned response than a generic prompt. These “quality of thinking” tasks benefit from prompts that **break down the cognitive process**: e.g. first brainstorm pros and cons, then weigh them, then conclude. A close parallel is the academic domain: prompting GPT-4 to “think step by step and double-check assumptions” results in more coherent, less fallacious arguments in informal testing, whereas a default prompt might produce a shallow answer. However, it’s important to note the **evidence here is mostly extrapolative** – unlike math or coding tasks, we don’t have easy quantitative benchmarks for philosophical insight. Still, **theoretical frameworks and related studies** support the idea. Prompting methods that inject reasoning steps or role-play debates analogously improved performance on **Theory-of-Mind tests and moral reasoning** in some research ([www.emergentmind.com](https://www.emergentmind.com/topics/persona-prompting#:~:text=diversity%20and%20mitigate%20stereotype%20propagation,dependent)) ([www.emergentmind.com](https://www.emergentmind.com/topics/persona-prompting#:~:text=personalization%2C%20and%20more,of%20bias%20or%20performance%20variance)), suggesting that when the task is about complex inference or perspective-taking, a prompt that guides those cognitive moves has more effect. In contrast, tasks that are **highly knowledge-dependent (domain-specific)** but not reasoning-heavy can be *less* prompt-sensitive. A medical Q&A study found that using elaborate multi-prompt strategies did **no better than a basic prompt** – the limiting factor was the model’s medical knowledge, not the prompt format ([www.sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0010482525009655#:~:text=Model%20capabilities%20proved%20more%20crucial,mini%20leading%20performance)). Similarly, factual questions either have the answer in the model’s memory or not – no prompt magic will reliably retrieve an unknown fact. In summary, tasks that require the model to **organize thoughts, make inferences, or be creative under constraints** see the greatest benefit from prompt engineering, whereas tasks that are either trivial or hinge on pure knowledge retrieval show comparatively **low sensitivity** to prompt tinkering.

**Strength of evidence:** **Moderate (with some strong subsets).** There is **strong empirical evidence** distinguishing prompt-sensitive tasks in domains like math, logical reasoning, and coding – multiple papers clearly show much larger gains from advanced prompts in those areas ([gl0bal01.com](https://gl0bal01.com/blog/prompt-engineering-developers-research-evidence#:~:text=Recent%20academic%20research%20reveals%20that,development%20velocity%2C%20and%20developer%20satisfaction)) ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)). The pattern is consistent: where the chain-of-thought or examples help, they help a lot, indicating a true task difference. Evidence is **weaker or more anecdotal** for creative and philosophical tasks, since they’re harder to benchmark. We mostly infer from theory and analogous studies that reasoning-heavy creative work would behave like other complex tasks. Some user studies and case reports suggest improved essay quality or ideation with structured prompts, but these lack the quantitative rigor of benchmark studies. We also have **strong negative evidence** for certain tasks: e.g. the Wharton persona study strongly indicates factual Q&A is not prompt-sensitive to role prompts ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D)), and the medical AI study shows model capability dominates prompt method in that domain ([www.sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0010482525009655#:~:text=Model%20capabilities%20proved%20more%20crucial,mini%20leading%20performance)). Taken together, the evidence base supports clear trends (complex reasoning vs. simple recall), but when it comes to fine-grained task categories like “philosophical argumentation,” **we’re extrapolating from general reasoning evidence** rather than direct experimental proof. So the overall strength is moderate – well-grounded for broad classes of tasks, speculative for very specific use cases.

**Key sources:** 

1. **Glöckner & Schlag (2025) meta-review:** Analyzed ~1,500 papers on prompt techniques; concluded that **task complexity is a key moderator** – *“sophisticated prompts yield bigger improvements on complex reasoning tasks than on basic comprehension tasks.”* ([gl0bal01.com](https://gl0bal01.com/blog/prompt-engineering-developers-research-evidence#:~:text=Recent%20academic%20research%20reveals%20that,development%20velocity%2C%20and%20developer%20satisfaction)) This broad survey supports the view that multi-step reasoning problems are inherently more prompt-sensitive.

2. **Google Brain – CoT case studies:** Demonstrations by Wei et al. (2022) showed that tasks like arithmetic word problems, symbolic reasoning, and commonsense QA saw **dramatic accuracy jumps** with chain-of-thought prompting, whereas simpler tasks showed minimal change ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=On%20the%20GSM8K%20dataset%20of,accuracy%20on%20GSM8K)) ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=match%20at%20L107%20performance%20improved,vs%2084)). *Example:* Commonsense QA improved modestly, but a challenging sports reasoning set jumped by ~11 points with CoT (84→95% correct) ([research.google](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/#:~:text=match%20at%20L107%20performance%20improved,vs%2084)), indicating the latter’s greater sensitivity to prompting style.

3. **Wharton Generative AI Lab (2025) – Prompting Science Reports:** These reports examined when prompt tactics matter. Report #1 found *expert persona prompts* **did not improve factual question answering** – i.e. **knowledge recall tasks aren’t very prompt-sensitive to role framing ([medium.com](https://medium.com/%40zoominai/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff#:~:text=,%E2%80%9D))**. Report #2 found that for **“reasoning-optimized” LLMs, explicit CoT prompting had diminishing returns** because those models handle reasoning automatically ([slashpage.com](https://slashpage.com/haebom/7vgjr4m1nq9gj2dwpy86#:~:text=%EA%B8%B0%EB%B2%95%EC%9D%84%20%EC%A1%B0%EC%82%AC%ED%95%A9%EB%8B%88%EB%8B%A4,%ED%95%84%EC%9A%94%ED%95%9C%20%EC%8B%9C%EA%B0%84%EA%B3%BC%20%ED%86%A0%ED%81%B0%EC%9D%80%20%ED%81%AC%EA%B2%8C%20%EC%A6%9D%EA%B0%80%ED%95%A9%EB%8B%88%EB%8B%A4)). Both findings reinforce that **prompt engineering helps most when it compensates for a task’s difficulty, not when the model already has it covered**.

4. **Domain-specific studies:** In specialized domains (like medicine), researchers found **prompt complexity yielded little benefit**. Jin et al. (2025) evaluated medical diagnostics QA with various CoT-based prompt schemes and saw no gain from more complex prompts once a basic step-by-step format was in place ([www.sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0010482525009655#:~:text=Model%20capabilities%20proved%20more%20crucial,mini%20leading%20performance)). The model’s inherent medical knowledge was the bottleneck. This suggests tasks heavy on domain knowledge and less on reasoning steps are **less sensitive to prompt tricks** than tasks requiring logical deduction.

5. **Emergent observations on creative tasks:** While formal studies are sparse, some work indicates *prompt diversity aids creativity*. Zhou et al. (2021) and Kasahara et al. (2022) noted that giving a narrative **persona or scenario** can increase the diversity of story generation (more varied vocabulary, perspectives) ([www.emergentmind.com](https://www.emergentmind.com/topics/persona-prompting#:~:text=reasoning%2C%20linguistic%20style%2C%20or%20behaviors,of%20bias%20or%20performance%20variance)). However, these changes are about style/variety, not “accuracy,” highlighting that for open-ended creative tasks, prompt engineering alters the flavor and depth of outputs more than any measurable right/wrong outcome.

**Implications for the decision:** The task profile at Forethought should guide how valuable prompt engineering expertise will be. Given that their work involves **philosophical reasoning, ethical arguments, and conceptual analysis**, these tasks resemble the “complex reasoning” category where prompt engineering has the **biggest impact**. The researchers aren’t just asking factual questions with known answers – they need the AI to **produce insight, weigh arguments, and perhaps be creative but rigorous**. The evidence suggests such tasks *do* benefit from carefully engineered prompts: for instance, using a **chain-of-thought approach to break down a philosophical question into sub-questions** or adopting a structured debate format could yield a more incisive analysis than a one-shot answer. In short, Forethought’s work is likely **prompt-sensitive**, meaning a specialist could systematically draw out better reasoning and avoid shallow responses. This weighs in favor of investing in prompt engineering expertise because it aligns with the high-gain task types. 

That said, Forethought should also note which tasks are **less prompt-sensitive**. If some of their queries are straightforward (e.g. asking for definitions or summaries of known material), a basic prompt may do nearly as well as an engineered one – a prompt expert’s time is better spent on the hard, reasoning-heavy queries. Additionally, as more advanced models (with in-built reasoning abilities) become available, they might handle some complex tasks with less prompting effort ([slashpage.com](https://slashpage.com/haebom/7vgjr4m1nq9gj2dwpy86#:~:text=%EA%B8%B0%EB%B2%95%EC%9D%84%20%EC%A1%B0%EC%82%AC%ED%95%A9%EB%8B%88%EB%8B%A4,%ED%95%84%EC%9A%94%ED%95%9C%20%EC%8B%9C%EA%B0%84%EA%B3%BC%20%ED%86%A0%ED%81%B0%EC%9D%80%20%ED%81%AC%EA%B2%8C%20%EC%A6%9D%EA%B0%80%ED%95%A9%EB%8B%88%EB%8B%A4)). But even with top models, **forging excellent, “load-bearing” arguments likely requires orchestration** – e.g. prompting the model to critique and refine its own answer, or to generate multiple viewpoints. Those are nontrivial prompt skills. In practical terms, the characteristics of Forethought’s tasks (open-ended, high-reasoning, high stakes for quality) suggest that **“winging it” with basic prompts risks leaving a lot of quality on the table**. A prompt engineer can introduce structures (like step-by-step reasoning, role-played dialogues, or sequences of prompts) that systematically yield deeper analyses. Given the evidence, investing in that capability should meaningfully improve research outputs for the kind of complex, philosophical work Forethought does. The main caveat is to remain evidence-driven about *which* prompt techniques to use for *which* task – not all tricks work everywhere. But knowing that, having an expert who can navigate these nuances seems likely to pay off by elevating the quality of the organization’s AI-generated insights.