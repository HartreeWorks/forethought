# Query A: Do prompts matter?

## Questions

### Q1: Magnitude of gains from prompt engineering

What does empirical research show about performance improvements from prompt engineering techniques (chain-of-thought, few-shot examples, structured prompts, persona prompting, etc.)?

I'm looking for:
- Benchmark results quantifying improvements (e.g., "X% gain on Y benchmark")
- Meta-analyses or systematic reviews if they exist
- Case studies from industry or research labs
- Whether gains are typically small (5-15%), moderate (20-50%), or large (50%+)

### Q2: Task characteristics that predict prompt sensitivity

What makes some tasks benefit more from prompt engineering than others?

I'm looking for:
- Empirical findings on which task types show the largest gains
- Theoretical frameworks explaining why certain tasks are more prompt-sensitive
- **Specific relevance to:** philosophical reasoning, argumentation quality, conceptual analysis, critique generation, creative analytical work
- Any evidence on whether "quality of thinking" tasks (vs factual recall or pattern matching) are more or less prompt-sensitive

## Output format

For each question, please provide:
1. **Summary of key findings** (2-3 paragraphs)
2. **Strength of evidence** (strong/moderate/weak/speculative)
3. **Key sources** (papers, reports, documented case studies)
4. **Implications for the decision** (what this means for the invest-in-expertise vs wing-it question)

Prioritise empirical evidence over theoretical arguments. Flag clearly when evidence is limited and you're extrapolating.
