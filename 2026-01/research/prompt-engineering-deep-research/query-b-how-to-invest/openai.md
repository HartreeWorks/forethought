# OpenAI Deep Research: query-b-how-to-invest

**Model:** o3-deep-research-2025-06-26
**Generated:** 2026-01-23T15:55:28.424Z
**Response ID:** resp_02a0e894264da162006973973ad46881a2b0a1683a3ead7325

---

## Q3: Multi-Step Chains and Orchestration

**Summary of key findings:** Multi-step prompting techniques can yield substantial gains in performance and output quality for complex tasks. In **reasoning and problem-solving**, breaking a task into steps or calling the model multiple times often outperforms a single direct prompt. Research on *chain-of-thought (CoT) prompting* shows that prompting the model to reason step-by-step can significantly improve accuracy on math word problems and logical reasoning tasks ([your-ai-staff.com](https://your-ai-staff.com/self-consistency-improves-chain-of-thought/#:~:text=Our%20extensive%20empirical%20evaluation%20shows,challenge%20%28%2B3.9)). For example, using *self-consistency* (sampling multiple chains and voting on the answer) boosts performance on benchmarks like GSM8K (math) by 17.9% and other reasoning datasets by ~4–12% ([your-ai-staff.com](https://your-ai-staff.com/self-consistency-improves-chain-of-thought/#:~:text=Our%20extensive%20empirical%20evaluation%20shows,challenge%20%28%2B3.9)). Similarly, *tool use and agent frameworks* (where the LLM can act in a multi-step loop, e.g. querying a wiki, executing code, etc.) have demonstrated large benefits. The ReAct agent method, which interleaves reasoning and actions (like web searches), produces **more factual and step-by-step accurate answers** on knowledge-intensive questions than a single prompt answer ([www.kdjingpai.com](https://www.kdjingpai.com/en/react/#:~:text=interpretability%20and%20trustworthiness,one%20or%20two%20contextual%20examples)). In one study, ReAct avoided the hallucination errors of chain-of-thought alone by retrieving real evidence, and on interactive decision-making tasks it outperformed single-step baselines (even surpassing a trained reinforcement learning agent by 34% on success rate) ([www.kdjingpai.com](https://www.kdjingpai.com/en/react/#:~:text=interpretability%20and%20trustworthiness,one%20or%20two%20contextual%20examples)). *Iterative refinement* loops, where the model critiques and improves its own output over multiple turns, also show **meaningful gains**. A recent example is the Reflexion approach: by having GPT-4 examine its mistakes and try again, a coding task success rate rose from ~80% to 91% pass@1 (a state-of-the-art result) ([bbs.kanxue.com](https://bbs.kanxue.com/article-24510.htm#:~:text=%E5%9F%BA%E4%BA%8E%E4%B8%8A%E8%BF%B0%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E6%8C%87%E6%A0%87%E4%B8%8E%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95%EF%BC%8C%E4%B8%8B%E6%96%B9%E5%88%97%E5%87%BA%E4%BA%86%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%9C%80%E6%96%B0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D%EF%BC%9A%20,4%E7%9A%8480%25%E3%80%82)). On a multi-hop QA task (HotpotQA), allowing the model to repeatedly reflect and refine led to nearly a **30% accuracy improvement (up to 97% correct)**, whereas simply prompting the same question twice without reflection yielded no improvement ([blog.51cto.com](https://blog.51cto.com/xixiaoyao/6238456#:~:text=4%E8%BF%99%E7%B1%BBLLM%E9%83%BD%E8%83%BD%E5%A4%9F%E8%BF%9B%E8%A1%8C%E8%87%AA%E6%88%91%E5%8F%8D%E6%80%9D%E4%BA%86%EF%BC%8C%E9%82%A3%E5%85%B7%E4%BD%93%E6%95%88%E6%9E%9C%E7%A9%B6%E7%AB%9F%E5%A6%82%E4%BD%95%EF%BC%9F%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%E5%9C%A8ALFWorld%E5%92%8CHotpotQA%E5%9F%BA%E5%87%86%E4%B8%8A%E5%AF%B9%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E4%BA%86%E8%AF%84%E4%BC%B0%E3%80%82%E5%9C%A8HotpotQA%E7%9A%84100%E4%B8%AA%E9%97%AE%E7%AD%94%E5%AF%B9%20%E6%B5%8B%E8%AF%95%E4%B8%AD%EF%BC%8C%E4%BD%BF%E7%94%A8Reflexion%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84LLM%E6%98%BE%E7%A4%BA%E5%87%BA%E4%BA%86%E5%B7%A8%E5%A4%A7%E7%9A%84%E4%BC%98%E5%8A%BF%EF%BC%8C%E5%86%8D%E7%BB%8F%E8%BF%87%E5%A4%9A%E8%BD%AE%E5%8F%8D%E6%80%9D%E9%87%8D%E5%A4%8D%E6%8F%90%E9%97%AE%E4%B9%8B%E5%90%8E%EF%BC%8CLLM%E7%9A%84%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E4%BA%86%E6%8E%A5%E8%BF%9130)). These findings suggest that on challenging, multi-faceted problems, structured prompt chains (decomposition, self-critique, or using multiple models/instances in an ensemble) can produce **much better results** than a one-shot prompt.

However, the benefits depend on the **task and implementation**. Multi-step prompting is most useful when a task can be naturally decomposed or when a single-pass tends to make errors. Domains like math problem solving, code generation, evidence-based QA, or nuanced **analytical writing** all benefit from splitting into steps or iterative feedback. In contrast, for simpler queries or tasks the model can handle in one step, elaborate chaining may yield little to no benefit and can even introduce errors (“overthinking”). There is evidence of an *optimal chain length*: making the reasoning chain *too long or convoluted* can start to **hurt performance**, as the model may accumulate errors or drift off-track ([openreview.net](https://openreview.net/forum?id=6QDFsYxtI1#:~:text=performance%20rises%20initially%20but%20declines,mismatch%20with%20current%20practice%2C%20where)). One study found an *inverted U-shape* effect where adding some reasoning steps helped, but beyond an optimal point, extra steps caused accuracy to decline ([openreview.net](https://openreview.net/forum?id=6QDFsYxtI1#:~:text=performance%20rises%20initially%20but%20declines,mismatch%20with%20current%20practice%2C%20where)). This optimal chain length tends to be **shorter for more capable models** and longer for very hard tasks ([openreview.net](https://openreview.net/forum?id=6QDFsYxtI1#:~:text=performance%20rises%20initially%20but%20declines,mismatch%20with%20current%20practice%2C%20where)) – meaning powerful LLMs might not need as many explicit steps to get things right, whereas weaker models need more scaffolding. Overall, multi-step orchestration offers clear gains in many cases, but it must be applied thoughtfully. It also carries overhead: designing and tuning a chain is more complex and using multiple LLM calls or tools increases cost and latency. The *additional improvement* from these methods can be **very large on certain tasks** (double-digit percentage jumps in accuracy or quality), but not universally so.

**Strength of evidence:** **Strong** for complex reasoning and knowledge tasks. Multiple peer-reviewed studies and benchmarks demonstrate significant improvements from CoT prompting, voting ensembles, and tool-augmented chains ([your-ai-staff.com](https://your-ai-staff.com/self-consistency-improves-chain-of-thought/#:~:text=Our%20extensive%20empirical%20evaluation%20shows,challenge%20%28%2B3.9)) ([www.kdjingpai.com](https://www.kdjingpai.com/en/react/#:~:text=interpretability%20and%20trustworthiness,one%20or%20two%20contextual%20examples)). The evidence is especially robust in domains like math word problems, multi-hop question answering, and coding, where quantitative gains have been measured. For more open-ended creative or analytical tasks, direct empirical studies are fewer, so the case relies on analogies and anecdotes – still, the consistent pattern on reasoning tasks makes it plausible these methods help produce deeper, more *“load-bearing”* answers in fields like philosophy. Overall, the evidence that well-crafted multi-step approaches can outperform single prompts on challenging tasks is strong, though the exact benefit in a given domain may vary (and requires some trial and error).

**Key sources:** Recent research papers and reports provide the empirical comparisons. For example, Yao et al.’s *ReAct* showed that an LLM agent using reasoning + tool actions beat both standalone reasoning and standalone tool use on QA and decision-making tasks ([www.kdjingpai.com](https://www.kdjingpai.com/en/react/#:~:text=interpretability%20and%20trustworthiness,one%20or%20two%20contextual%20examples)). Wang et al. (2023) introduced *self-consistency decoding*, where the model’s multiple reasoning paths are aggregated – this **significantly improved accuracy** on benchmarks (e.g. +17.9% on GSM8K) ([your-ai-staff.com](https://your-ai-staff.com/self-consistency-improves-chain-of-thought/#:~:text=Our%20extensive%20empirical%20evaluation%20shows,challenge%20%28%2B3.9)). Reflexion (Shinn et al. 2023) demonstrated that letting GPT-4 iteratively self-correct yielded state-of-the-art code generation results (91% vs 80% pass@1) and dramatically higher QA performance ([bbs.kanxue.com](https://bbs.kanxue.com/article-24510.htm#:~:text=%E5%9F%BA%E4%BA%8E%E4%B8%8A%E8%BF%B0%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E6%8C%87%E6%A0%87%E4%B8%8E%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95%EF%BC%8C%E4%B8%8B%E6%96%B9%E5%88%97%E5%87%BA%E4%BA%86%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%9C%80%E6%96%B0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D%EF%BC%9A%20,4%E7%9A%8480%25%E3%80%82)) ([blog.51cto.com](https://blog.51cto.com/xixiaoyao/6238456#:~:text=4%E8%BF%99%E7%B1%BBLLM%E9%83%BD%E8%83%BD%E5%A4%9F%E8%BF%9B%E8%A1%8C%E8%87%AA%E6%88%91%E5%8F%8D%E6%80%9D%E4%BA%86%EF%BC%8C%E9%82%A3%E5%85%B7%E4%BD%93%E6%95%88%E6%9E%9C%E7%A9%B6%E7%AB%9F%E5%A6%82%E4%BD%95%EF%BC%9F%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%E5%9C%A8ALFWorld%E5%92%8CHotpotQA%E5%9F%BA%E5%87%86%E4%B8%8A%E5%AF%B9%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E4%BA%86%E8%AF%84%E4%BC%B0%E3%80%82%E5%9C%A8HotpotQA%E7%9A%84100%E4%B8%AA%E9%97%AE%E7%AD%94%E5%AF%B9%20%E6%B5%8B%E8%AF%95%E4%B8%AD%EF%BC%8C%E4%BD%BF%E7%94%A8Reflexion%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84LLM%E6%98%BE%E7%A4%BA%E5%87%BA%E4%BA%86%E5%B7%A8%E5%A4%A7%E7%9A%84%E4%BC%98%E5%8A%BF%EF%BC%8C%E5%86%8D%E7%BB%8F%E8%BF%87%E5%A4%9A%E8%BD%AE%E5%8F%8D%E6%80%9D%E9%87%8D%E5%A4%8D%E6%8F%90%E9%97%AE%E4%B9%8B%E5%90%8E%EF%BC%8CLLM%E7%9A%84%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E4%BA%86%E6%8E%A5%E8%BF%9130)). Additional studies have analyzed when decomposition helps vs hurts: Wu et al. (2025) found that extremely long reasoning chains lead to *diminishing returns or errors*, with the optimal chain length depending on task difficulty and model power ([openreview.net](https://openreview.net/forum?id=6QDFsYxtI1#:~:text=performance%20rises%20initially%20but%20declines,mismatch%20with%20current%20practice%2C%20where)). These sources reinforce that prompt chaining, used judiciously, can add significant value.

**Implications for the decision:** The evidence suggests that **systematic prompt engineering – especially building multi-step workflows – can unlock substantially better outputs** on the kind of complex, conceptual tasks Forethought tackles. If researchers are currently “winging it” with single prompts, they may be leaving performance on the table for tasks that involve multi-step reasoning, synthesis of evidence, or the generation of and reflection on arguments. For instance, in philosophical analysis, one could prompt an LLM to produce an argument, then separately prompt it to critique that argument or generate counterarguments, and finally synthesize a refined position. Such a chain is likely to yield more incisive results than a one-turn prompt asking for an all-in-one answer. The gains reported (often double-digit improvements in quality or accuracy) underline that a specialist who knows how to design these chains and use tools/ensembles could meaningfully boost the **depth and reliability** of the AI’s output. **That said,** the benefit comes with increased complexity and cost – not every task will justify a complicated pipeline. Forethought should consider the **nature of the research questions**: for high-stakes analyses or particularly hard problems, investing in prompt engineering expertise to systematically apply chaining, self-refinement, etc., is likely worth it. It can turn the AI from a one-shot respondent into a more rigorous analyst or “research assistant.” On more straightforward tasks, a basic prompt may do just fine. In summary, multi-step prompt strategies are a proven source of added value, especially for the challenging reasoning tasks central to Forethought’s mission – suggesting that hiring or training a specialist in these methods would pay off in higher-quality insights.

---

## Q4: Trajectory Over Time – Do Prompts Matter More or Less as Models Improve?

**Summary of key findings:** As AI models have become more capable (GPT-3 → GPT-4 and beyond), the *relative importance* of prompt engineering appears to be **shifting**. Early large language models were often highly sensitive to prompt phrasing and benefited greatly from tricks like carefully chosen few-shot examples or explicit reasoning prompts. With today’s top-tier models, those same prompt “hacks” tend to yield **smaller gains**. For example, the once-groundbreaking chain-of-thought prompting (giving step-by-step reasoning in the prompt) provided large accuracy boosts on GPT-3-class models, but with state-of-the-art models the gap has shrunk. A recent replication study found that GPT-4 variants and Claude 3.7 achieved ~95% accuracy on certain reasoning tasks **with or without** chain-of-thought prompting – in other words, these models solved the problems nearly perfectly even with a basic prompt, so CoT added no significant benefit ([bythyag.bearblog.dev](https://bythyag.bearblog.dev/chain-of-thought-in-large-language-models-what-has-changed-over-the-years/#:~:text=Though%20CoT%20prompting%20proved%20improvement,this%20advantage%20has%20largely%20disappeared)). The authors note that the advantage CoT had for earlier models has “largely disappeared” on the newest models ([bythyag.bearblog.dev](https://bythyag.bearblog.dev/chain-of-thought-in-large-language-models-what-has-changed-over-the-years/#:~:text=Though%20CoT%20prompting%20proved%20improvement,this%20advantage%20has%20largely%20disappeared)). This suggests that **better models now internally handle many complexities** that we used to explicitly prompt for. Anecdotally, advanced models interpret user intent much more readily: as one practitioner observed, GPT-4 “requires far less prompt engineering and generally interprets intent better” than its predecessors ([news.ycombinator.com](https://news.ycombinator.com/item?id=35483444#:~:text=For%20one%2C%20GPT,filenames%2C%20under%20a%20particular%20size)). In practical terms, GPT-4 often needs only a plain-English instruction to produce a good answer where GPT-3 might have needed a carefully structured prompt or examples.

That said, prompt engineering is **not obsolete** – it’s evolving. While higher-capability models can figure more out on their own, leveraging them fully still benefits from crafting the right context, especially for novel or abstract tasks. New models also tend to have different *instruction-following quirks*, meaning a prompt optimized for an older model might not work as well out-of-the-box on a new one. There’s evidence that **prompts don’t perfectly transfer across model versions**: one case showed that reusing a GPT-4 (2024) prompt on a newer model led to a drop in accuracy from 82.5% to 80%, until the prompt was **re-engineered** for the new model, after which performance jumped to 89% ([www.notdiamond.ai](https://www.notdiamond.ai/blog/better-models-worse-results-why-prompt-adaptation-matters-part-1#:~:text=With%20this%20prompt%2C%20GPT,dropped%20accuracy%20to%2080)) ([www.notdiamond.ai](https://www.notdiamond.ai/blog/better-models-worse-results-why-prompt-adaptation-matters-part-1#:~:text=performance%20degrades%20when%20they%20reuse,model%20version%20interprets%20instructions%20differently)). This indicates prompt tuning still matters when upgrading models, even if the model is “smarter.” The nature of prompt gains is also changing. With bigger models, the returns from simple prompt tweaks (like adding “Let’s think step by step”) may be smaller, but more **sophisticated prompting** – providing better context or constraints – can still notably improve reliability or steer the style/tone. Furthermore, as models get more capable, users attempt more complex tasks with them, essentially raising the bar of what we expect the model to do. In those frontier uses, careful prompting and chaining may still be critical to *reach the model’s peak performance*. The relationship between model power and optimal prompting is nuanced: one analysis found that **more capable models actually require fewer reasoning steps** for best results (they “overthink” sooner), whereas less capable models needed longer, more explicit reasoning prompts ([openreview.net](https://openreview.net/forum?id=6QDFsYxtI1#:~:text=performance%20rises%20initially%20but%20declines,mismatch%20with%20current%20practice%2C%20where)). In other words, as models get better, the prompt engineering focus shifts from brute-forcing lengthy reasoning toward *calibrating the right minimal structure* and context for the model to excel. Overall, the trend so far hints that prompt engineering is **becoming a bit easier or less finicky** for general use as models improve – but it remains important for pushing the limits and for tailoring the model’s output to specific needs.

**Strength of evidence:** **Moderate.** We have some empirical studies and credible reports, but the trajectory is not universally quantified. The strongest evidence comes from direct comparisons on reasoning benchmarks across model generations, which show diminishing returns for older prompt techniques (e.g. CoT) with today’s models ([bythyag.bearblog.dev](https://bythyag.bearblog.dev/chain-of-thought-in-large-language-models-what-has-changed-over-the-years/#:~:text=Though%20CoT%20prompting%20proved%20improvement,this%20advantage%20has%20largely%20disappeared)). There’s also consistent anecdotal agreement among early GPT-4 users that it’s more forgiving and capable with simple prompts ([news.ycombinator.com](https://news.ycombinator.com/item?id=35483444#:~:text=For%20one%2C%20GPT,filenames%2C%20under%20a%20particular%20size)). Additionally, engineering case studies (e.g. prompt transfer between models) give concrete examples of how a new model can render an old prompt suboptimal ([www.notdiamond.ai](https://www.notdiamond.ai/blog/better-models-worse-results-why-prompt-adaptation-matters-part-1#:~:text=performance%20degrades%20when%20they%20reuse,model%20version%20interprets%20instructions%20differently)). However, there’s not a large body of formal longitudinal studies yet – the landscape is inferred from a few key experiments and expert observations. The evidence thus leans in one direction (prompts matter **a bit less** in brute-force ways than before, though still matter in new ways), but we should acknowledge some speculation when extending this trend into the future. As models continue to improve (GPT-5 and beyond), it’s *expected* based on current understanding that they will handle complexity more autonomously, yet this expectation is partly theoretical. In summary, the evidence for prompt engineering’s changing role is solid in specific cases and logical in theory, but not exhaustive – we’re extrapolating recent patterns forward.

**Key sources:** A 2025 replication by Thyagarajan et al. compared classic prompt strategies on modern models, finding **no significant difference** between zero-shot and CoT prompting for GPT-4/Claude on tasks like GSM8K math and common-sense QA (93–96% accuracy either way) ([bythyag.bearblog.dev](https://bythyag.bearblog.dev/chain-of-thought-in-large-language-models-what-has-changed-over-the-years/#:~:text=Though%20CoT%20prompting%20proved%20improvement,this%20advantage%20has%20largely%20disappeared)). An HN discussion in early 2023 echoed that GPT-4 understood intents without fancy prompts ([news.ycombinator.com](https://news.ycombinator.com/item?id=35483444#:~:text=For%20one%2C%20GPT,filenames%2C%20under%20a%20particular%20size)). On the issue of cross-model prompt portability, a blog from Notion’s AI team detailed how upgrading to a newer model often **degraded performance until the prompt was adapted**, underlining that each model has its own “dialect” of instructions ([www.notdiamond.ai](https://www.notdiamond.ai/blog/better-models-worse-results-why-prompt-adaptation-matters-part-1#:~:text=performance%20degrades%20when%20they%20reuse,model%20version%20interprets%20instructions%20differently)). Research on CoT length by Wu et al. (2025) provides a theoretical backing: it showed that larger models have an *earlier peak* in the performance vs. chain length curve, implying they need less explicit reasoning structure ([openreview.net](https://openreview.net/forum?id=6QDFsYxtI1#:~:text=performance%20rises%20initially%20but%20declines,mismatch%20with%20current%20practice%2C%20where)). These sources collectively inform the trajectory: bigger models are more capable of figuring things out with minimal prompting, yet optimal results still require **thoughtful prompt design**, just tuned to the new generation’s strengths.

**Implications for the decision:** The trajectory suggests that **prompt engineering expertise might yield diminishing *marginal* returns as models get more powerful**, but it doesn’t become useless. If Forethought plans to leverage the latest top-tier models (GPT-4, Claude 4, etc.), those models will already do a lot with straightforward prompts – so researchers “winging it” will get better results today than they would have a few years ago. This means the **cost of not engineering prompts is lower than before** for many tasks. In fact, spending hours on prompt minutiae that a GPT-5 could handle implicitly might not be a good use of time. However, two caveats support still investing in prompt expertise. First, as models improve, organizations tend to tackle **more ambitious tasks** with them (harder queries, more complex analyses), and in those new regimes, having an expert to fine-tune prompts or break problems down can remain crucial to reach the best possible output. Second, the transition to each new model often *requires prompt tweaking*: a specialist can quickly adapt Forethought’s prompting style to maintain or improve performance on the new system, whereas non-experts might hit unexpected snags (misinterpreted instructions, etc.). Prompt engineering techniques also tend to evolve (what works best for GPT-4 may be different for GPT-5), so having someone who stays on top of these changes is valuable. In sum, **basic prompting is becoming more effective with advanced models**, reducing the urgency of heavy prompt tinkering for routine tasks. This leans toward the idea that researchers can get pretty far without a dedicated engineer for everyday use of GPT-4+. But if Forethought’s work involves pushing the boundaries – e.g. synthesizing nuanced philosophical arguments where you need every bit of coherence and depth – prompt engineering still offers a competitive edge. The investment in expertise may not “compound” as much as it once did (since the models compensate more), but it also doesn’t depreciate to zero; instead, it shifts toward *quality assurance and adaptation*. A reasonable conclusion is a **balanced approach**: empower researchers with enough prompt engineering knowledge to leverage advanced models (which are forgiving), and possibly have a specialist consultant who can step in for the most challenging projects or when migrating to a new model. This way, Forethought captures most of the available value – enjoying the improved base capabilities of new models, while still being able to squeeze out the last mile of performance when it really matters.