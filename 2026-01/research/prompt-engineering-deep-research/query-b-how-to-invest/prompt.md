# Query B: How to invest in prompt engineering?

## Questions

### Q3: Multi-step chains and orchestration

Beyond single-prompt optimisation, what's the evidence on:
- Multi-step prompt chains (decomposing tasks into sequential LLM calls)
- Agent architectures and tool use
- Model ensembles (using multiple models and aggregating)
- Iterative refinement (self-critique and revision loops)

Do these approaches produce meaningful additional gains beyond well-crafted single prompts? Under what conditions?

I'm looking for:
- Empirical comparisons of single-prompt vs multi-step approaches
- Studies on agent performance vs direct prompting
- Evidence on when decomposition helps vs hurts
- Cost-benefit analyses (complexity vs improvement)

### Q4: Trajectory over time—do prompts matter more or less with better models?

As frontier models have improved (GPT-3 → GPT-4 → GPT-5, Claude 2 → Claude 3 → Claude 4), have prompt engineering gains:
- Increased (better models can follow more sophisticated instructions)?
- Decreased (better models need less hand-holding)?
- Stayed roughly constant?

What does this imply for whether prompt engineering investments compound or depreciate over time?

I'm looking for:
- Longitudinal studies comparing prompt sensitivity across model generations
- Practitioner reports on whether old techniques still work
- Theoretical arguments about the trajectory
- Any evidence on prompt "portability" across models

## Output format

For each question, please provide:
1. **Summary of key findings** (2-3 paragraphs)
2. **Strength of evidence** (strong/moderate/weak/speculative)
3. **Key sources** (papers, reports, documented case studies)
4. **Implications for the decision** (what this means for the invest-in-expertise vs wing-it question)

Prioritise empirical evidence over theoretical arguments. Flag clearly when evidence is limited and you're extrapolating.
