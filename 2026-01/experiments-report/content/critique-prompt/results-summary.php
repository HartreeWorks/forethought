<h2 id="results">Results</h2>

<p>According to ACORN, there was a fairly clear split:</p>

<ul>
    <li><strong>Top performers:</strong> "Pivot-attack", "Unforgettable", "Personas".</li>
    <li><strong>Bottom performers:</strong> "Conversational", "November", and "Pre-mortem".</li>
</ul>

<p>But: do I agree with ACORN's better/worse judgements? Two ways to form a view on that:
    <ol>
    <li>Eyeball some of the critiques it considers best and worst.</li>
    <li>Do a blind pairwise comparison of best and worst critiques, then see how my judgements compare to ACORN's.</li>
    </ol>

<p>I did a bit of both. This turns out to be pretty hard work, but my key observations are: 
    
<ol>    
    <li>There aren't obviously massive differences in the quality of critiques generated by the different prompts. So I don't agree with the grader that some prompts are clearly outperforming.</li>
<li>Different prompts often surface the same ideas, but all of the prompts generated at least one idea that the others did not. The exclusive ideas were lower quality on average, but not all terrible (<a href="http://localhost:8080/critique-prompt-experiment-appendix-2.php">details</a>).</li>
<li>The ACORN grader does not rate the same ideas super consistently (<a href="/critique-prompt-experiment-appendix-2.php#cluster-no-easy-eutopia-1">worrying example</a>). </li>
</ol>
    
    <p><strong>Bottom line:</strong> The ACORN grader gives <em>some</em> signal. We'd need to rewrite it to make it better track what Forethought cares about. It'd take 1+ days of output grading by researchers to generate high confidence in a modified grader's judgements. It's unclear how reliable the grader would be even after that effort.</p>
    
<p>So, going back to the two questions I care about:</p>

<ol>
    <li><strong>Is the ACORN grader good for our purposes?</strong> No.</li>
    <li><strong>Do some prompts clearly outperform the conversational baseline?</strong> Yes. #todo</li>
</ol>


<p>Want to form your own view? In appendix 1, I'll share the ACORN grader ratings; appendix 2 gives example critiques to read and appendix 4 discusses uniqueness.</p>

<div style="display: none;">
<h2>Where we could go next</h2>
<ul>
    <li>Try other published papers</li>    
    <li>Try actual early drafts</li>
    <li>Try other prompts</li>
    <li>Get a devastating insight.</li>
    <li>Try other context engineering techniques. A "Forethought worldview / assumptions" context primer would filter a bunch of usless critiques (e.g. "you're assuming lockin is plausible" and "you're assuming aggregation views in pop ethics"). #todo - could do a quick experiment on this</li>
    <li>Try other use cases (e.g. crucial considerations).</li>
</ul>
</div>