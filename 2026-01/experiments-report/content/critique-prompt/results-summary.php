<h2 id="results">Results</h2>

<p>According to ACORN, there was a fairly clear split:</p>

<ul>
    <li><strong>Top performers:</strong> "Pivot-attack", "Unforgettable", "Personas".</li>
    <li><strong>Bottom performers:</strong> "Conversational", "November", and "Pre-mortem".</li>
</ul>

<p>But: do I agree with ACORN's better/worse judgements? Two ways to form a view on that:
    <ol>
    <li>Eyeball some of the critiques it considers best and worst.</li>
    <li>Do a blind pairwise comparison of best and worst critiques, then see how my judgements compare to ACORN's.</li>
    </ol>

<p>I did a bit of both. This turns out to be pretty hard work, but my key observations are: 
    
<ol>    
    <li>There aren't obviously massive differences in the quality of critiques generated by the different prompts. So I don't agree with the grader that some prompts are clearly outperforming.</li>
<li>Different prompts often surface the same ideas, but all of the prompts generated at least one idea that the others did not. The exclusive ideas were lower quality on average, but not all terrible (<a href="critique-prompt-experiment-appendix-2.php">details</a>).</li>
<li>The ACORN grader does not rate the same ideas super consistently (<a href="critique-prompt-experiment-appendix-2.php#cluster-no-easy-eutopia-1">worrying example</a>). </li>
</ol>
    
    <p><strong>Bottom line:</strong> The ACORN grader gives <em>some</em> signal. We'd need to rewrite it to make it better track what Forethought cares about. It'd take 1+ days of output grading by researchers to generate high confidence in a modified grader's judgements. It's unclear how reliable the grader would be even after that effort.</p>
    
<p>So, going back to the two questions I care about:</p>

<ol>
    <li><strong>Is the ACORN grader good for our purposes?</strong> No, it'd need at least a few days' work to get something useful.</li>
    <li><strong>Do some prompts clearly generate more insightful critiques than the conversational baseline?</strong> The ACORN grader certainly thinks so. I agree, but I'm not bowled over by the difference. The main thing I notice is that the best critique of "No Easy Eutopia"—according to me and ACORN—was not generated by the conversational prompt. I'd be interested to hear Fin's take on <a href="critique-prompt-experiment.php?paper=compute-bottlenecks&uniqueness=unique#appendix-5-all-critiques">the best critiques of the other papers</a>, and whether any were missed by the conversational prompt.</li>
        
</ol>


<p>Want to form your own view? Check the appendices below. If you'd like to do some blind pairwise comparisons, <a href="https://pjh.is/call">schedule a 25-min call</a> and I'll get you set up.</p>

<div style="display: none;">
<h2>Where we could go next</h2>
<ul>
    <li>Try other published papers</li>    
    <li>Try actual early drafts</li>
    <li>Try other prompts</li>
    <li>Get a devastating insight.</li>
    <li>Try other context engineering techniques. A "Forethought worldview / assumptions" context primer would filter a bunch of useless critiques (e.g. "you're assuming lockin is plausible" and "you're assuming aggregation views in pop ethics"). #todo - could do a quick experiment on this</li>
    <li>Try other use cases (e.g. crucial considerations).</li>
</ul>
</div>