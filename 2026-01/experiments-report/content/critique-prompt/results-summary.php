<h2 id="results">Results</h2>

<p>According to ACORN, there was a fairly clear split:</p>

<ul>
    <li><strong>Top performers:</strong> "Pivot-attack", "Unforgettable", "Personas".</li>
    <li><strong>Bottom performers:</strong> "Conversational", "November", and "Pre-mortem".</li>
</ul>

<p>But: do I agree with ACORN's better/worse judgements? Two ways to form a view on that:
    <ol>
    <li>Eyeball some of the critiques it considers best and worst.</li>
    <li>Do a blind pairwise comparison of best and worst critiques, then see how my judgements compare to ACORN's.</li>
    </ol>

<p>I did a bit of both. This turns out to be pretty hard work, but my key takeaways are: 
    
<ol>    
    <li>There aren't obviously massive differences in the quality of critiques generated by the different prompts</li>
<li>Different prompts often surface the same ideas, but 6/8 of the prompts generated at least one idea that the others did not.</li>
<li>The ACORN grader does not rate the same ideas super consistently (<a href="/critique-prompt-experiment-appendix-2.php#cluster-no-easy-eutopia-1">worrying example</a>). </li>
</ol>
    
    <p><strong>Bottom line:</strong> The ACORN grader gives useful signal for our purposes. We could use this grader—and probably some variations—to run many more experiments.</p>

<p>What is the range of critique quality, though? Are the "best" critiques actually a bunch better than the "worst"? Again, to answer this I'd ideally do a mix of "eyeballing" and blind pairwise comparisons. <strong>#todo:</strong> do more of this.</p>

<p>My current judgement: the quality range is significant.</p>

<p>Another question: are the prompts basically all surfacing <strong>the same critiques</strong>? In short: there's some dupliction, but not so much that it doesn't matter what prompt we use. See more detail in <a href="#appendix-2-how-many-unique-critiques">appendix 2</a>. #todo - do i stand behind this take?</p>

<p>So, going back to the two questions I care about:</p>

<ol>
    <li><strong>Is the ACORN grader good for our purposes?</strong> Yes.</li>
    <li><strong>Do some prompts clearly outperform the conversational baseline?</strong> Yes.</li>
</ol>


<p>Now, I want you to form your own view. Below, I'll share the ACORN grader ratings, and then give you some example critiques to read.</p>


<h2>Where we could go next</h2>
<ul>
    <li>Try other published papers</li>    
    <li>Try actual early drafts</li>
    <li>Try other prompts</li>
    <li>Get a devastating insight.</li>
    <li>Try other context engineering techniques. A "Forethought worldview / assumptions" context primer would filter a bunch of usless critiques (e.g. "you're assuming lockin is plausible" and "you're assuming aggregation views in pop ethics"). #todo - could do a quick experiment on this</li>
    <li>Try other use cases (e.g. crucial considerations).</li>
</ul>