<h2 id="results">Results</h2>

<p>According to ACORN, there was a fairly clear split:</p>

<ul>
    <li><strong>Top performers:</strong> "Pivot-attack", "Unforgettable", "Personas".</li>
    <li><strong>Bottom performers:</strong> "Conversational", "November", and "Pre-mortem".</li>
</ul>

<p>But: do I agree with ACORN's better/worse judgements? Two ways to form a view on that:
    <ol>
    <li>Eyeball some of the critiques it considers best and worst.</li>
    <li>Do a blind pairwise comparison of best and worst critiques, then see how my judgements compare to ACORN's.</li>
    </ol>

<p>I did a bit, but not enough, of both. <strong>#todo:</strong> do more pairwise comparisons, and enable others to do the same.</p>
    <p><strong>Bottom line:</strong> The ACORN grader gives useful signal for our purposes. We could use this grader—and probably some variations—to run many more experiments.</p>

<p>What is the range of critique quality, though? Are the "best" critiques actually a bunch better than the "worst"? Again, to answer this I'd ideally do a mix of "eyeballing" and blind pairwise comparisons. <strong>#todo:</strong> do more of this.</p>

<p>My current judgement: the quality range is significant.</p>

<p>Another question: are the prompts basically all surfacing <strong>the same critiques</strong>? In short: there's some dupliction, but not so much that it doesn't matter what prompt we use. #todo - do i stand behind this take?</p>

<p>So, going back to the two questions I care about:</p>

<ol>
    <li><strong>Is the ACORN grader good for our purposes?</strong> Yes.</li>
    <li><strong>Do some prompts clearly outperform the conversational baseline?</strong> Yes.</li>
</ol>


<p>Now, I want you to form your own view. Below, I'll share the ACORN grader ratings, and then give you some example critiques to read.</p>
