<h2 id="discussion">Discussion</h2>
<p>
  This experiment used a human-validated grader to compare a low-effort baseline prompt against three slightly more sophisticated prompts.</p>

<p>The results are consistent with general consensus that context engineering can improve LLM performace over simple "wing it" prompts. But—how much improvement is there to be had, either now, or later in 2026? This quick experiment doesn't tell us much.</p>

<p class="todo">#todo: What experiment actually <strong>would</strong>
tell us a bunch more? Or perhaps experiments aren't what we need right now—I should just talk to Casper and others?</p>

<p class="todo">#todo: What did I learn about the ACORN grader from these outputs? How trustworthy is it? How discerning? What did I learn about making graders more generally, aside from "yeah, if you want to really trust it, you need a lot of human time to validate...". Is it actually fine to just use pretty slapdash grading steps?</p>

<hr style="margin: 3rem 0;">
